<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="http://jekyllrb.com" version="3.4.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2018-12-28T14:31:51+00:00</updated><id>//</id><title type="html">ThoughtWorks</title><subtitle>DeepLearning.scala by ThougthWorks</subtitle><entry><title type="html">Announcing DeepLearning.scala 2.0.0</title><link href="/2017/07/26/Announcing-DeepLearning.scala-2.0.0.html" rel="alternate" type="text/html" title="Announcing DeepLearning.scala 2.0.0" /><published>2017-07-26T00:00:00+00:00</published><updated>2017-07-26T00:00:00+00:00</updated><id>/2017/07/26/Announcing-DeepLearning.scala-2.0.0</id><content type="html" xml:base="/2017/07/26/Announcing-DeepLearning.scala-2.0.0.html">&lt;p&gt;Today, we are happy to announce DeepLearning.scala 2.0.0, the new stable release of DeepLearning.scala, a simple library for creating complex neural networks from object-oriented and functional programming constructs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DeepLearning.scala runs on JVM, can be used either in standalone JVM applications or Jupyter Notebooks.&lt;/li&gt;
  &lt;li&gt;DeepLearning.scala is expressive. Various types of neural network layers can be created by composing &lt;code class=&quot;highlighter-rouge&quot;&gt;map&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;reduce&lt;/code&gt; or other higher order functions.&lt;/li&gt;
  &lt;li&gt;DeepLearning.scala supports plugins. You can share your own algorithms, models, hyperparameters as a plugin, as simple as creating a Github Gist.&lt;/li&gt;
  &lt;li&gt;All the above features are statically type checked.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;features-in-deeplearningscala-20&quot;&gt;Features in DeepLearning.scala 2.0&lt;/h2&gt;

&lt;p&gt;In DeepLearning.scala 2.0, we removed the special support for differentiable ADT and &lt;code class=&quot;highlighter-rouge&quot;&gt;Boolean&lt;/code&gt; types. Now differentiable computational graphs are ordinary Scala code, so all types including ADT and &lt;code class=&quot;highlighter-rouge&quot;&gt;Boolean&lt;/code&gt; are avialable in these graphs.&lt;/p&gt;

&lt;h3 id=&quot;dynamic-neural-networks&quot;&gt;Dynamic neural networks&lt;/h3&gt;

&lt;p&gt;Unlike some other deep learning frameworks, the structure of neural networks in DeepLearning.scala is dynamically determined during running. Our neural networks are programs. All Scala features, including functions and expressions, are available in neural networks.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ordinaryScalaFunction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;INDArray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Boolean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;signnum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sumT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myDynamicNeuralNetwork&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;INDArray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;INDArrayLayer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monadic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Do&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputOfLayer1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ordinaryScalaFunction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputOfLayer1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dynamicallySelectedLayer2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputOfLayer1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dynamicallySelectedLayer3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputOfLayer1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;each&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The above neural network will go into different subnetworks according to an ordinary Scala function.&lt;/p&gt;

&lt;p&gt;With the ability of creating dynamic neural networks, regular programmers are able to build complex neural networks from simple code. You write code almost as usual, the only difference being that code based on DeepLearning.scala is differentiable, which enables such code to evolve by modifying its parameters continuously.&lt;/p&gt;

&lt;h3 id=&quot;functional-programming&quot;&gt;Functional programming&lt;/h3&gt;

&lt;p&gt;DeepLearning.scala 2.0 is based on Monads, which are composable, thus a complex layer can be built from primitive operators. Along with the Monad, we provide an Applicative type class, to perform multiple calculations in parallel.&lt;/p&gt;

&lt;p&gt;For example, the previous example can be rewritten in higher-order function style as following:&lt;/p&gt;

&lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myDynamicNeuralNetwork&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;INDArray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;INDArrayLayer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputOfLayer1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ordinaryScalaFunction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputOfLayer1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;dynamicallySelectedLayer2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputOfLayer1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;dynamicallySelectedLayer3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputOfLayer1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The key construct in DeepLearning.scala 2.0 is the dependent type class &lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html&quot;&gt;DeepLearning&lt;/a&gt;, which witnesses a differentiable expression. In other words, given the &lt;code class=&quot;highlighter-rouge&quot;&gt;DeepLearning&lt;/code&gt; type class instance, you can activate the deep learning ability of any type.&lt;/p&gt;

&lt;h3 id=&quot;object-oriented-programming&quot;&gt;Object-oriented programming&lt;/h3&gt;

&lt;p&gt;The code base of DeepLearning.scala 2.0 is organized according to Dependent Object Type calculus (DOT). All features are provided as mixin-able plugins. A plugin is able to change APIs and behaviors of all DeepLearning.scala types. This approach not only resolves &lt;a href=&quot;https://en.wikipedia.org/wiki/Expression_problem&quot;&gt;expression problem&lt;/a&gt;, but also gives plugins the additional ability of &lt;strong&gt;virtually depending&lt;/strong&gt; on other plugins.&lt;/p&gt;

&lt;p&gt;For example, when a plugin author is creating the &lt;a href=&quot;https://gist.github.com/Atry/89ee1baa4c161b8ccc1b82cdd9c109fe#file-adagrad-sc&quot;&gt;Adagrad&lt;/a&gt; optimizer plugin, he does not have to explicitly call functions related to learning rate. However, once a plugin user enables both the &lt;code class=&quot;highlighter-rouge&quot;&gt;Adagrad&lt;/code&gt; plugin and the &lt;a href=&quot;https://gist.github.com/Atry/1fb0608c655e3233e68b27ba99515f16#file-readme-ipynb&quot;&gt;FixedLearningRate&lt;/a&gt; plugin, then computation in &lt;code class=&quot;highlighter-rouge&quot;&gt;FixedLearningRate&lt;/code&gt; will get called eventually when the &lt;code class=&quot;highlighter-rouge&quot;&gt;Adagrad&lt;/code&gt; optimization is executed.&lt;/p&gt;

&lt;h2 id=&quot;plugins-for-deeplearningscala-20&quot;&gt;Plugins for DeepLearning.scala 2.0&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;
Plugin Name
&lt;/th&gt;
&lt;th&gt;
Plugin Description
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;a href=&quot;https://www.javadoc.io/page/com.thoughtworks.deeplearning/plugins-builtins_2.11/latest/com/thoughtworks/deeplearning/plugins/Builtins.html&quot;&gt;Builtins&lt;/a&gt;
&lt;/th&gt;
&lt;td&gt;
All the built-in plugins.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;a href=&quot;https://gist.github.com/Atry/1fb0608c655e3233e68b27ba99515f16#file-readme-ipynb&quot;&gt;FixedLearningRate&lt;/a&gt;
&lt;/th&gt;
&lt;td&gt;
Setup fixed learning rate when training INDArray weights.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;a href=&quot;https://gist.github.com/Atry/89ee1baa4c161b8ccc1b82cdd9c109fe#file-adagrad-sc&quot;&gt;Adagrad&lt;/a&gt;
&lt;/th&gt;
&lt;td&gt;
An adaptive gradient algorithm with per-parameter learning rate for INDArray weights.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;a href=&quot;https://gist.github.com/TerrorJack/8154015cc0ac5cfba8e351b642ef12b3#file-readme-ipynb&quot;&gt;L1Regularization&lt;/a&gt;
&lt;/th&gt;
&lt;td&gt;
L1 Regularization.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;a href=&quot;https://gist.github.com/TerrorJack/a60ff752270c40a6485ee787837390aa#file-readme-ipynb&quot;&gt;L2Regularization&lt;/a&gt;
&lt;/th&gt;
&lt;td&gt;
L2 Regularization.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;a href=&quot;https://gist.github.com/TerrorJack/08454c71448b626b013ddabd74d06adf#file-readme-ipynb&quot;&gt;Momentum&lt;/a&gt;
&lt;/th&gt;
&lt;td&gt;
The Momentum and NesterovMomentum optimizer for SGD.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;a href=&quot;https://gist.github.com/TerrorJack/6b0640c76efc6788f13400ae91849e68#file-readme-ipynb&quot;&gt;RMSprop&lt;/a&gt;
&lt;/th&gt;
&lt;td&gt;
The RMSprop optimizer for SGD.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;a href=&quot;https://gist.github.com/TerrorJack/4a4dd1929963a34bf20340022b0f03d3#file-readme-ipynb&quot;&gt;Adam&lt;/a&gt;
&lt;/th&gt;
&lt;td&gt;
The Adam optimizer for SGD.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;a href=&quot;https://gist.github.com/TerrorJack/a7af811a0ee592d41ab57f2c5d49f08b#file-readme-ipynb&quot;&gt;INDArrayDumping&lt;/a&gt;
&lt;/th&gt;
&lt;td&gt;
A plugin to dump weight matrices during training.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
&lt;a href=&quot;https://gist.github.com/TerrorJack/cdd9cc5adc82fc86abf8b4c72cd26e76#file-readme-ipynb&quot;&gt;CNN&lt;/a&gt;
&lt;/th&gt;
&lt;td&gt;
A standalone CNN implementation.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;a href=&quot;http://deeplearning.thoughtworks.school/get-involved&quot;&gt;Add your own algorithms, models or any cool features here.&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;

&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://deeplearning.thoughtworks.school/&quot;&gt;DeepLearning.scala homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ThoughtWorksInc/DeepLearning.scala/&quot;&gt;DeepLearning.scala on Github&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://deeplearning.thoughtworks.school/demo/GettingStarted.html&quot;&gt;Getting Started for DeepLearning.scala 2.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/package.html&quot;&gt;API reference documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><summary type="html">Today, we are happy to announce DeepLearning.scala 2.0.0, the new stable release of DeepLearning.scala, a simple library for creating complex neural networks from object-oriented and functional programming constructs.</summary></entry><entry><title type="html">Announcing DeepLearning.scala 2.0.0-RC1</title><link href="/2017/07/03/Announcing-DeepLearning.scala-2.0.0-RC1.html" rel="alternate" type="text/html" title="Announcing DeepLearning.scala 2.0.0-RC1" /><published>2017-07-03T00:00:00+00:00</published><updated>2017-07-03T00:00:00+00:00</updated><id>/2017/07/03/Announcing-DeepLearning.scala-2.0.0-RC1</id><content type="html" xml:base="/2017/07/03/Announcing-DeepLearning.scala-2.0.0-RC1.html">&lt;p&gt;Today, we are happy to announce DeepLearning.scala 2.0.0-RC1, a release candidate of DeepLearning.scala 2.&lt;/p&gt;

&lt;p&gt;DeepLearning.scala 2.0 comes with two major features in addition to DeepLearning.scala 1.0: dynamic neural networks and &lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.feature/factory_2.11/latest/com/thoughtworks/feature/Factory.html&quot;&gt;Factory&lt;/a&gt;-based plugins.&lt;/p&gt;

&lt;p&gt;In DeepLearning.scala 2.0, a neural network is an ordinary Scala function that returns a &lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.deeplearning/plugins-builtins_2.11/latest/com/thoughtworks/deeplearning/plugins/Layers$Layer.html&quot;&gt;Layer&lt;/a&gt;, which represents the process that dynamically creates computational graph nodes, instead of static computational graphs in TensorFlow or some other deep learning frameworks. All Scala features, including functions and expressions, are available in DeepLearning.scala’s dynamic neural networks.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Factory&lt;/code&gt;-based plugins resolve &lt;a href=&quot;https://en.wikipedia.org/wiki/Expression_problem&quot;&gt;expression problem&lt;/a&gt;. Any hyperparameters, neural network optimization algorithms or special subnetworks are reusable in the simple &lt;code class=&quot;highlighter-rouge&quot;&gt;Factory[YourPlugin1 with YourPlugin2]&lt;/code&gt; mechanism.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/v1.0.0-doc/ipynbs/2.0.0-Preview/GettingStarted.ipynb&quot;&gt;Getting Started&lt;/a&gt; to have a try.&lt;/p&gt;

&lt;h3 id=&quot;links&quot;&gt;Links&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://deeplearning.thoughtworks.school/&quot;&gt;DeepLearning.scala homepage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ThoughtWorksInc/DeepLearning.scala/&quot;&gt;DeepLearning.scala on Github&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/v1.0.0-doc/ipynbs/2.0.0-Preview/GettingStarted.ipynb&quot;&gt;Getting Started for DeepLearning.scala 2.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/package.html&quot;&gt;Scaladoc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.feature/factory_2.11/latest/com/thoughtworks/feature/Factory.html&quot;&gt;Factory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><summary type="html">Today, we are happy to announce DeepLearning.scala 2.0.0-RC1, a release candidate of DeepLearning.scala 2.</summary></entry><entry><title type="html">Announcing DeepLearning.scala 1.0.0</title><link href="/2017/03/28/Announcing-DeepLearning.scala-1.0.0.html" rel="alternate" type="text/html" title="Announcing DeepLearning.scala 1.0.0" /><published>2017-03-28T00:00:00+00:00</published><updated>2017-03-28T00:00:00+00:00</updated><id>/2017/03/28/Announcing-DeepLearning.scala-1.0.0</id><content type="html" xml:base="/2017/03/28/Announcing-DeepLearning.scala-1.0.0.html">&lt;p&gt;Version 1.0.0 is the first stable release of &lt;a href=&quot;http://deeplearning.thoughtworks.school/&quot;&gt;DeepLearning.scala&lt;/a&gt;, a simple language for creating complex neural networks.&lt;/p&gt;

&lt;p&gt;Along with the library, we created &lt;a href=&quot;http://deeplearning.thoughtworks.school/doc/&quot;&gt;a series of tutorials&lt;/a&gt; for developers who want to learn deep learning algorithms.&lt;/p&gt;

&lt;h2 id=&quot;features-in-100&quot;&gt;Features in 1.0.0&lt;/h2&gt;

&lt;h3 id=&quot;differentiable-basic-types&quot;&gt;Differentiable basic types&lt;/h3&gt;

&lt;p&gt;Like &lt;a href=&quot;http://deeplearning.net/software/theano/&quot;&gt;Theano&lt;/a&gt; and other deep learning toolkits, DeepLearning.scala allows you to build neural networks from mathematical formulas. It supports &lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.deeplearning/unidoc_2.11/1.0.0/com/thoughtworks/deeplearning/DifferentiableFloat$.html&quot;&gt;float&lt;/a&gt;s, &lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.deeplearning/unidoc_2.11/1.0.0/com/thoughtworks/deeplearning/DifferentiableDouble$.html&quot;&gt;double&lt;/a&gt;s, &lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.deeplearning/unidoc_2.11/1.0.0/com/thoughtworks/deeplearning/DifferentiableINDArray$.html&quot;&gt;GPU-accelerated N-dimensional array&lt;/a&gt;s, and calculates derivatives of the weights in the formulas.&lt;/p&gt;

&lt;h3 id=&quot;differentiable-adts&quot;&gt;Differentiable ADTs&lt;/h3&gt;

&lt;p&gt;Neural networks created by DeepLearning.scala support &lt;a href=&quot;https://en.wikipedia.org/wiki/Algebraic_data_type&quot;&gt;ADT&lt;/a&gt; data structures (e.g. &lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.deeplearning/unidoc_2.11/1.0.0/com/thoughtworks/deeplearning/DifferentiableHList$.html&quot;&gt;HList&lt;/a&gt; and &lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.deeplearning/unidoc_2.11/1.0.0/com/thoughtworks/deeplearning/DifferentiableCoproduct$.html&quot;&gt;Coproduct&lt;/a&gt;), and calculate derivatives through these data structures.&lt;/p&gt;

&lt;h3 id=&quot;differentiable-control-flow&quot;&gt;Differentiable control flow&lt;/h3&gt;

&lt;p&gt;Neural networks created by DeepLearning.scala may contains control flows like &lt;code class=&quot;highlighter-rouge&quot;&gt;if&lt;/code&gt;/&lt;code class=&quot;highlighter-rouge&quot;&gt;else&lt;/code&gt;/&lt;code class=&quot;highlighter-rouge&quot;&gt;match&lt;/code&gt;/&lt;code class=&quot;highlighter-rouge&quot;&gt;case&lt;/code&gt; in a regular language. Combined with ADT data structures, you can implement arbitary algorithms inside neural networks, and still keep some of the variables used in the algorithms differentiable and trainable.&lt;/p&gt;

&lt;h3 id=&quot;composability&quot;&gt;Composability&lt;/h3&gt;

&lt;p&gt;Neural networks created by DeepLearning.scala are composable. You can create large networks by combining smaller networks. If two larger networks share some sub-networks, the weights in shared sub-networks trained with one network affect the other network.&lt;/p&gt;

&lt;h3 id=&quot;static-type-system&quot;&gt;Static type system&lt;/h3&gt;

&lt;p&gt;All of the above features are statically type checked.&lt;/p&gt;

&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://thoughtworksinc.github.io/DeepLearning.scala/doc/&quot;&gt;Tutorials&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://javadoc.io/page/com.thoughtworks.deeplearning/unidoc_2.11/1.0.0/com/thoughtworks/deeplearning/package.html&quot;&gt;Scaladoc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gitter.im/ThoughtWorksInc/DeepLearning.scala&quot;&gt;Chat room&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledges&quot;&gt;Acknowledges&lt;/h2&gt;

&lt;p&gt;DeepLearning.scala is heavily inspired by &lt;a href=&quot;https://github.com/MarisaKirisame&quot;&gt;@MarisaKirisame&lt;/a&gt;. Originally, we worked together for a prototype of deep learning framework, then we split our work aprt to this project and &lt;a href=&quot;https://github.com/ThoughtWorksInc/DeepDarkFantasy&quot;&gt;DeepDarkFantasy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/milessabin&quot;&gt;@milessabin&lt;/a&gt;’s &lt;a href=&quot;https://github.com/milessabin/shapeless&quot;&gt;shapeless&lt;/a&gt; provides a solid foundation for type-level programming as used in DeepLearning.scala.&lt;/p&gt;</content><summary type="html">Version 1.0.0 is the first stable release of DeepLearning.scala, a simple language for creating complex neural networks.</summary></entry></feed>
