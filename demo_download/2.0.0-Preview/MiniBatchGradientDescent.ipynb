{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "During large-scale date training, the order of magnitude of data can reach millions. If a parameter is acquired via the computation of the whole training set, the update speed will be too slow. To solve this problem, a common used method is [Mini-Batch Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) which computes mini-batche data in the training set, resulting faster training of parameters in a neural network.\n",
    "\n",
    "In this article, we will first define a softmax classifier, then use the training set of [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) to train this neural network, and finally use the test set to verify the accuracy of the neural network. The difference is that we will use Mini-Batch Gradient Descent, thus the accuracy of the neural network can reach 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies & build your own neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the [previous course](http://deeplearning.thoughtworks.school/demo/2.0.0-Preview/SoftmaxLinearClassifier.html), we need to introduce each class of DeepLearning.scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                            \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$url.$                                                                                                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$url.$                                                                                                               \n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.math._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable.Any._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable.INDArray.{\n",
       "  Optimizer => INDArrayOptimizer\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mINDArrayOptimizer.LearningRate\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.each.Monadic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.raii.asynchronous.Do\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable.Double._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Tape\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.jupyter.differentiable\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.ExecutionContext.Implicits.global\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscalaz.concurrent.Task\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscalaz.{-\\/, \\/, \\/-}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscalaz.std.vector._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m\n",
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mNumberOfPixels\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3072\u001b[39m\n",
       "\u001b[36mweight\u001b[39m: \u001b[32mcom\u001b[39m.\u001b[32mthoughtworks\u001b[39m.\u001b[32mdeeplearning\u001b[39m.\u001b[32mjupyter\u001b[39m.\u001b[32mdifferentiable\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mINDArray\u001b[39m = Suspend(<function0>)\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmyNeuralNetwork\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mpolyLoss\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`org.scalamacros:paradise_2.11.11:2.1.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::jupyter-differentiable:2.0.0-M1`\n",
    "import $ivy.`org.nd4j:nd4j-native-platform:0.7.2`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.2`\n",
    "import $url.{`https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/master/ipynbs/ReadCIFAR10ToNDArray.sc` => ReadCIFAR10ToNDArray}\n",
    "import $url.{`https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/master/ipynbs/Utils.sc` => Utils}\n",
    "\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "import com.thoughtworks.deeplearning.math._\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable.Any._\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.{\n",
    "  Optimizer => INDArrayOptimizer\n",
    "}\n",
    "import INDArrayOptimizer.LearningRate\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable.INDArray.implicits._\n",
    "import com.thoughtworks.each.Monadic._\n",
    "import com.thoughtworks.raii.asynchronous.Do\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable.Double._\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable.Double.implicits._\n",
    "import com.thoughtworks.deeplearning.Tape\n",
    "import com.thoughtworks.deeplearning.jupyter.differentiable\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4s.Implicits._\n",
    "import scala.concurrent.ExecutionContext.Implicits.global\n",
    "import scalaz.concurrent.Task\n",
    "import scalaz.{-\\/, \\/, \\/-}\n",
    "import scalaz.std.vector._\n",
    "import shapeless._\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq\n",
    "\n",
    "pprintConfig() = pprintConfig().copy(height = 2)\n",
    "\n",
    "implicit def optimizer: INDArrayOptimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.00001\n",
    "}\n",
    "\n",
    "def softmax(scores: differentiable.INDArray): differentiable.INDArray = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / sum(expScores, 1)\n",
    "}\n",
    "\n",
    "//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "val NumberOfPixels: Int = 3072\n",
    "\n",
    "val weight: differentiable.INDArray =\n",
    "  (Nd4j.randn(NumberOfPixels, NumberOfClasses) * 0.001).toWeight\n",
    "\n",
    "def myNeuralNetwork(input: INDArray): differentiable.INDArray = {\n",
    "  softmax(dot(input, weight))\n",
    "}\n",
    "\n",
    "def lossFunction(input: INDArray,\n",
    "                 expectOutput: INDArray): differentiable.Double = {\n",
    "  val probabilities = myNeuralNetwork(input)\n",
    "  -mean(log(probabilities) * expectOutput)\n",
    "}\n",
    "             \n",
    "plotly.JupyterScala.init()\n",
    "def polyLoss(lossSeq: IndexedSeq[Double]): Unit = {\n",
    "  plotly.JupyterScala.init()\n",
    "\n",
    "  val plot = Seq(\n",
    "    Scatter(lossSeq.indices, lossSeq)\n",
    "  )\n",
    "\n",
    "  plot.plot(\n",
    "    title = \"loss by time\"\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disrupt the order of a sequence once for each [epoch](http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks), and generate the random arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainTask\u001b[39m: \u001b[32mTask\u001b[39m[\u001b[32mUnit\u001b[39m] = scalaz.concurrent.Task@396c2d6b"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@monadic[Task]\n",
    "val trainTask: Task[Unit] = {\n",
    "\n",
    "  val random = new scala.util.Random\n",
    "\n",
    "  val MiniBatchSize = 256\n",
    "\n",
    "  val lossSeq =\n",
    "    (\n",
    "      for (iteration <- (0 to 50).toVector) yield {\n",
    "        val randomIndex = random\n",
    "          .shuffle[Int, IndexedSeq](0 until 10000) //https://issues.scala-lang.org/browse/SI-6948\n",
    "          .toArray\n",
    "        for (times <- (0 until 10000 / MiniBatchSize).toVector) yield {\n",
    "          val randomIndexArray =\n",
    "            randomIndex.slice(times * MiniBatchSize,\n",
    "                              (times + 1) * MiniBatchSize)\n",
    "          val trainNDArray :: expectLabel :: shapeless.HNil =\n",
    "            ReadCIFAR10ToNDArray.getSGDTrainNDArray(randomIndexArray)\n",
    "          val input =\n",
    "            trainNDArray.reshape(MiniBatchSize, 3072)\n",
    "\n",
    "          val expectLabelVectorized =\n",
    "            Utils.makeVectorized(expectLabel, NumberOfClasses)\n",
    "          val loss = train(lossFunction(input, expectLabelVectorized)).each\n",
    "          if(times == 3 & iteration % 5 == 4){\n",
    "            println(\"at epoch \" + (iteration / 5 + 1) + \" loss is :\" + loss)\n",
    "          }\n",
    "          loss\n",
    "        }\n",
    "      }\n",
    "    ).flatten\n",
    "\n",
    "  polyLoss(lossSeq)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and process the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like [the previous article](http://deeplearning.thoughtworks.school/demo/2.0.0-Preview/SoftmaxLinearClassifier.html), we read the images and corresponding label information for test data from CIFAR10 database and process them. However, here we only read the test set, and the training set is randomly read during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testNDArray =\n",
    "   ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)\n",
    "\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Predict your Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredictResult\u001b[39m: \u001b[32mTask\u001b[39m[\u001b[32mTape\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mData\u001b[39m] = scalaz.concurrent.Task@4c5663bd"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictResult = throwableMonadic[Task] {\n",
    "  trainTask.each\n",
    "  predict(myNeuralNetwork(testData)).each\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the last article, we use the test data to verify the prediction result of the neural network and compute the accuracy. This time, the accuracy may increase to about 41%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1 loss is :0.2140218734741211\n",
      "at epoch 2 loss is :0.19828615188598633\n",
      "at epoch 3 loss is :0.1983615279197693\n",
      "at epoch 4 loss is :0.19120612144470214\n",
      "at epoch 5 loss is :0.19014278650283814\n",
      "at epoch 6 loss is :0.19041664600372316\n",
      "at epoch 7 loss is :0.17840851545333863\n",
      "at epoch 8 loss is :0.18262848854064942\n",
      "at epoch 9 loss is :0.18263672590255736\n",
      "at epoch 10 loss is :0.189388644695282\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1915618569\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0],\"y\":[0.2300715446472168,0.22974910736083984,0.22962684631347657,0.22991640567779542,0.2293985366821289,0.2290342092514038,0.22930574417114258,0.22903108596801758,0.22836718559265137,0.22857089042663575,0.2287527561187744,0.22787694931030272,0.22724299430847167,0.22818727493286134,0.2262643337249756,0.22722480297088624,0.22741217613220216,0.2266237497329712,0.22569715976715088,0.22725212574005127,0.22453532218933106,0.2264793634414673,0.22601211071014404,0.22473835945129395,0.2250814199447632,0.22585339546203614,0.22619452476501464,0.223974347114563,0.22497334480285644,0.22531638145446778,0.2242985725402832,0.22473726272583008,0.22336525917053224,0.22368457317352294,0.22359302043914794,0.22381997108459473,0.2250349760055542,0.2237466096878052,0.22329435348510743,0.2215416431427002,0.22360692024230958,0.22063772678375243,0.22308011054992677,0.22252559661865234,0.22259035110473632,0.22229409217834473,0.223671293258667,0.2226329565048218,0.22244844436645508,0.22336301803588868,0.22162246704101562,0.22302722930908203,0.22135422229766846,0.221152925491333,0.22121694087982177,0.22085094451904297,0.22198078632354737,0.2201235294342041,0.22245521545410157,0.2182159900665283,0.22043452262878419,0.21910312175750732,0.22217869758605957,0.218798828125,0.21951525211334227,0.21999764442443848,0.21959424018859863,0.21787831783294678,0.21851656436920167,0.22005267143249513,0.21871933937072754,0.2178213119506836,0.21864080429077148,0.2187519073486328,0.2153780460357666,0.21988353729248047,0.21850996017456054,0.22007064819335936,0.21969983577728272,0.2182913064956665,0.2179234504699707,0.220192813873291,0.21690676212310792,0.21429529190063476,0.21628327369689943,0.21885721683502196,0.21640849113464355,0.21589107513427735,0.2151495933532715,0.21761908531188964,0.21447396278381348,0.2185309648513794,0.21657848358154297,0.21358613967895507,0.21588053703308105,0.21549053192138673,0.21780722141265868,0.21592917442321777,0.21509599685668945,0.21278877258300782,0.21682844161987305,0.21352238655090333,0.2198000431060791,0.21614158153533936,0.21409907341003417,0.21297926902770997,0.2182009696960449,0.21213040351867676,0.21496107578277587,0.2122546672821045,0.21939327716827392,0.21634247303009033,0.21526782512664794,0.21202688217163085,0.2152923583984375,0.21680874824523927,0.2123029947280884,0.21198458671569825,0.2176053047180176,0.21620793342590333,0.21530575752258302,0.21287174224853517,0.2128905773162842,0.21268763542175292,0.21167774200439454,0.2138397216796875,0.21289267539978027,0.21223044395446777,0.21494781970977783,0.21198792457580568,0.21520681381225587,0.21161289215087892,0.21488046646118164,0.20858113765716552,0.21647944450378417,0.21278860569000244,0.2108428955078125,0.2099531650543213,0.21174936294555663,0.21097123622894287,0.21328504085540773,0.21365270614624024,0.21291344165802,0.2087337017059326,0.2147317886352539,0.21131467819213867,0.2144176959991455,0.2133336067199707,0.21125340461730957,0.21068935394287108,0.21030652523040771,0.21135330200195312,0.21030635833740235,0.21272354125976561,0.2111952543258667,0.20776019096374512,0.21257514953613282,0.21081185340881348,0.20775692462921141,0.2140218734741211,0.20991430282592774,0.21170442104339598,0.2136309862136841,0.20783686637878418,0.21129837036132812,0.2132906436920166,0.20918216705322265,0.21068854331970216,0.20849852561950682,0.20882532596588135,0.20822455883026122,0.212471866607666,0.21144695281982423,0.2069375991821289,0.20931413173675537,0.21129682064056396,0.20989348888397216,0.20972211360931398,0.21032025814056396,0.2043825387954712,0.2097320795059204,0.21003503799438478,0.21104164123535157,0.20688986778259277,0.21354939937591552,0.20669982433319092,0.2109067440032959,0.2105936050415039,0.21156203746795654,0.20721466541290284,0.21037678718566893,0.20879528522491456,0.20660109519958497,0.20679407119750975,0.2075120449066162,0.2043677568435669,0.20690679550170898,0.2096999168395996,0.20800716876983644,0.21172606945037842,0.2058267593383789,0.20644221305847169,0.20650925636291503,0.20715851783752443,0.20757503509521485,0.20653033256530762,0.21055662631988525,0.20784327983856202,0.2099748134613037,0.20578718185424805,0.20918588638305663,0.21090283393859863,0.20779075622558593,0.2079761505126953,0.20749180316925048,0.2064741373062134,0.20890257358551026,0.2105933904647827,0.20777010917663574,0.21145944595336913,0.20624876022338867,0.20617716312408446,0.20539407730102538,0.2079171657562256,0.20641605854034423,0.21059095859527588,0.20718934535980224,0.21131932735443115,0.20766282081604004,0.20574264526367186,0.21042296886444092,0.20762720108032226,0.2086054801940918,0.2083409070968628,0.20827114582061768,0.20942239761352538,0.20643553733825684,0.20917716026306152,0.20637135505676268,0.20778450965881348,0.207381534576416,0.20493860244750978,0.20475413799285888,0.2081087350845337,0.20842378139495848,0.20183048248291016,0.2018674850463867,0.21123437881469725,0.20779633522033691,0.20548603534698487,0.20756514072418214,0.2073678493499756,0.20315804481506347,0.20720152854919432,0.2090679883956909,0.203064227104187,0.2023077964782715,0.2052597999572754,0.20668327808380127,0.20396647453308106,0.207741641998291,0.20529279708862305,0.20603570938110352,0.20593559741973877,0.2054598808288574,0.20767683982849122,0.2077383041381836,0.21036686897277831,0.2055518627166748,0.19999207258224488,0.2047590970993042,0.20440409183502198,0.20252602100372313,0.20515875816345214,0.2091146945953369,0.20495131015777587,0.20496048927307128,0.20174217224121094,0.1983409643173218,0.20111162662506105,0.2007617473602295,0.20622758865356444,0.2042081356048584,0.2103114604949951,0.20310633182525634,0.20025577545166015,0.2035912036895752,0.20547609329223632,0.20900530815124513,0.20351896286010743,0.2054394006729126,0.2000713348388672,0.20088646411895753,0.20567498207092286,0.2020465612411499,0.20632390975952147,0.2073850154876709,0.2059776782989502,0.2077712297439575,0.20224528312683104,0.20578665733337403,0.2005776882171631,0.20566074848175048,0.20487699508666993,0.20678577423095704,0.2060706377029419,0.20680580139160157,0.20796604156494142,0.20697598457336425,0.20302987098693848,0.20274677276611328,0.19957486391067505,0.20795822143554688,0.20141592025756835,0.20249249935150146,0.20118849277496337,0.20132913589477539,0.20637156963348388,0.19827125072479249,0.20328865051269532,0.20177826881408692,0.2000171184539795,0.20068607330322266,0.2034374713897705,0.19995131492614746,0.20325057506561278,0.20393164157867433,0.19967796802520751,0.2042644739151001,0.20940790176391602,0.20201926231384276,0.20070643424987794,0.20700762271881104,0.2106424331665039,0.20317952632904052,0.20283064842224122,0.20295567512512208,0.20216050148010253,0.1991642713546753,0.20426712036132813,0.20179336071014403,0.2042987823486328,0.20318331718444824,0.19918603897094728,0.20493438243865966,0.2038012981414795,0.20241622924804686,0.20604300498962402,0.20163040161132811,0.2082606554031372,0.20322782993316652,0.19819989204406738,0.19969496726989747,0.20479993820190429,0.19828615188598633,0.20081894397735595,0.2014101505279541,0.19951652288436889,0.20270051956176757,0.20255966186523439,0.19914498329162597,0.20201196670532226,0.2037278175354004,0.20022106170654297,0.20186161994934082,0.19728643894195558,0.19712038040161134,0.199621319770813,0.20051305294036864,0.2003793716430664,0.2085104465484619,0.2010108470916748,0.20031917095184326,0.19789695739746094,0.196482253074646,0.19945664405822755,0.20296745300292968,0.20228946208953857,0.2016446828842163,0.20305333137512208,0.2012399196624756,0.2029980182647705,0.20221354961395263,0.19927079677581788,0.19822070598602295,0.20005016326904296,0.20600695610046388,0.2036379337310791,0.19992762804031372,0.20137314796447753,0.19676663875579833,0.2035996437072754,0.19838457107543944,0.19751213788986205,0.20056095123291015,0.2066265106201172,0.20495877265930176,0.201090669631958,0.20436592102050782,0.20327887535095215,0.19794365167617797,0.2028512716293335,0.19977765083312987,0.20088725090026854,0.20515837669372558,0.19963645935058594,0.20029959678649903,0.1974647045135498,0.19929399490356445,0.20258057117462158,0.1980404496192932,0.20201447010040283,0.20557503700256347,0.1978912830352783,0.2010343313217163,0.2006401538848877,0.19654276371002197,0.1974376320838928,0.19990096092224122,0.20051343441009523,0.19762766361236572,0.19801955223083495,0.19754812717437745,0.20071799755096437,0.19695568084716797,0.20030856132507324,0.1980691909790039,0.20099778175354005,0.19820754528045653,0.19671850204467772,0.1939978241920471,0.19990040063858033,0.201324462890625,0.19835915565490722,0.2007061004638672,0.19664623737335205,0.20304570198059083,0.1956164598464966,0.20092270374298096,0.1980313777923584,0.19788445234298707,0.2007509231567383,0.2024974822998047,0.19580096006393433,0.2039482593536377,0.19396004676818848,0.19907636642456056,0.20746026039123536,0.19617573022842408,0.19735852479934693,0.1987444519996643,0.2017911911010742,0.20076360702514648,0.20164735317230226,0.206697416305542,0.19958057403564453,0.19517821073532104,0.19789319038391112,0.20485033988952636,0.19837844371795654,0.2066699504852295,0.2020942211151123,0.19771642684936525,0.1956185817718506,0.1964979887008667,0.19400473833084106,0.1914237141609192,0.19988001585006715,0.2030113458633423,0.20276122093200682,0.19895565509796143,0.19632117748260497,0.19734245538711548,0.20470285415649414,0.1978179097175598,0.20103821754455567,0.19632771015167236,0.19765535593032837,0.2012186050415039,0.19585078954696655,0.19364721775054933,0.19825904369354247,0.1990360736846924,0.19848482608795165,0.20110321044921875,0.19728924036026002,0.1997831106185913,0.19955991506576537,0.19685810804367065,0.19534575939178467,0.19496642351150512,0.1935938596725464,0.20096993446350098,0.19911539554595947,0.1973336458206177,0.20100102424621583,0.19560914039611815,0.19489824771881104,0.19880380630493164,0.19585399627685546,0.19771867990493774,0.19281296730041503,0.1944914698600769,0.20048186779022217,0.19665337800979615,0.19587682485580443,0.1961282730102539,0.19752519130706786,0.2037343978881836,0.20153000354766845,0.19574649333953859,0.20079994201660156,0.20215086936950682,0.19815162420272828,0.20195789337158204,0.1945479154586792,0.19949334859848022,0.194929039478302,0.20466241836547852,0.1994682788848877,0.2007430076599121,0.20259582996368408,0.20086638927459716,0.199030601978302,0.1974153995513916,0.1936793565750122,0.19764890670776367,0.20022802352905272,0.19243829250335692,0.19634606838226318,0.19727034568786622,0.1916082262992859,0.1972744345664978,0.1961879014968872,0.1969621777534485,0.1923682928085327,0.19711663722991943,0.1952430486679077,0.19310357570648193,0.19397029876708985,0.19728842973709107,0.20754244327545165,0.19870022535324097,0.20247290134429932,0.19935420751571656,0.19988722801208497,0.1986431360244751,0.19986212253570557,0.19484119415283202,0.1983615279197693,0.19610047340393066,0.1916250228881836,0.19577149152755738,0.19404276609420776,0.19519623517990112,0.19907323122024537,0.1972040891647339,0.19705214500427246,0.1930802583694458,0.19581956863403321,0.20158216953277588,0.1965059757232666,0.19983725547790526,0.19697856903076172,0.20110783576965333,0.19798572063446046,0.18974945545196534,0.1985707998275757,0.19426414966583253,0.19882779121398925,0.19572880268096923,0.19196443557739257,0.19688767194747925,0.19858224391937257,0.1978306293487549,0.19447230100631713,0.19845478534698485,0.1962105393409729,0.1969773530960083,0.1970372200012207,0.19581842422485352,0.19895546436309813,0.1963498115539551,0.1997887372970581,0.1967022657394409,0.19489961862564087,0.19409056901931762,0.19279255867004394,0.1943673610687256,0.19989670515060426,0.20063431262969972,0.19981184005737304,0.19645462036132813,0.1914312481880188,0.19116822481155396,0.19327412843704223,0.19399189949035645,0.19984033107757568,0.1976354718208313,0.2021190643310547,0.19739949703216553,0.18889069557189941,0.19436998367309571,0.18815965652465821,0.19122483730316162,0.20218498706817628,0.19593807458877563,0.19083700180053711,0.19440817832946777,0.19105193614959717,0.1930363416671753,0.19345898628234864,0.19912033081054686,0.1930980682373047,0.19784078598022461,0.19376157522201537,0.20285332202911377,0.19540170431137086,0.19685835838317872,0.19687358140945435,0.18929920196533204,0.19379692077636718,0.18905867338180543,0.19509296417236327,0.19957646131515502,0.20124130249023436,0.200028133392334,0.19526319503784179,0.19605196714401246,0.19300737380981445,0.19396238327026366,0.1968587875366211,0.19336433410644532,0.19540274143218994,0.1989771842956543,0.19241042137145997,0.19090999364852906,0.1960237741470337,0.1948567271232605,0.19727754592895508,0.1897389531135559,0.19560956954956055,0.1961157202720642,0.18995800018310546,0.1923382043838501,0.19762330055236815,0.18856077194213866,0.1968323826789856,0.19996485710144044,0.1915809392929077,0.19719878435134888,0.19510014057159425,0.19473379850387573,0.1914563298225403,0.19576728343963623,0.19242632389068604,0.1892325758934021,0.19783177375793456,0.20309715270996093,0.20045454502105714,0.19173049926757812,0.19630157947540283,0.19615933895111085,0.19551749229431153,0.1905044436454773,0.1960163712501526,0.19240214824676513,0.19631333351135255,0.19448016881942748,0.1996880888938904,0.1959836006164551,0.19632242918014525,0.19477790594100952,0.19229716062545776,0.1958696126937866,0.19634108543395995,0.19402928352355958,0.1951427936553955,0.19861972332000732,0.1949515700340271,0.19473891258239745,0.1909637451171875,0.19742302894592284,0.20079660415649414,0.19172580242156984,0.19936269521713257,0.20148420333862305,0.19800209999084473,0.1938995122909546,0.19400124549865722,0.1941571831703186,0.19504722356796264,0.19441742897033693,0.19167020320892333,0.1935657501220703,0.19837827682495118,0.19607930183410643,0.19376821517944337,0.1922650694847107,0.1947110652923584,0.19048227071762086,0.193693208694458,0.18942773342132568,0.19788578748703003,0.19857864379882811,0.1980919599533081,0.19478565454483032,0.19398659467697144,0.19617443084716796,0.1928034782409668,0.18337844610214232,0.202886962890625,0.19086791276931764,0.19224473237991332,0.19494287967681884,0.19499013423919678,0.19098989963531493,0.19674015045166016,0.19607640504837037,0.19282763004302977,0.1973675847053528,0.19217088222503662,0.19931178092956542,0.186879563331604,0.20194652080535888,0.18761072158813477,0.1945359468460083,0.19452743530273436,0.1926392436027527,0.19105385541915892,0.19711816310882568,0.19593405723571777,0.19763792753219606,0.19903759956359862,0.1981420636177063,0.18902047872543334,0.19695589542388917,0.19488991498947145,0.19466925859451295,0.1889951705932617,0.18894728422164916,0.19635080099105834,0.19117882251739501,0.20331864356994628,0.19120612144470214,0.19655725955963135,0.19022884368896484,0.18910932540893555,0.19610666036605834,0.2001175880432129,0.19475786685943602,0.19456708431243896,0.18837069272994994,0.19583917856216432,0.19475328922271729,0.19836385250091554,0.19356672763824462,0.19158053398132324,0.18992276191711427,0.19715421199798583,0.19512557983398438,0.19353235960006715,0.193152117729187,0.1974405288696289,0.19434913396835327,0.1937305212020874,0.19563595056533814,0.19710586071014405,0.1994672656059265,0.19851081371307372,0.1940707802772522,0.1953045129776001,0.19074971675872804,0.19032458066940308,0.1886146068572998,0.19528114795684814,0.1950908660888672,0.19408044815063477,0.19075820446014405,0.19112194776535035,0.19314876794815064,0.19544553756713867,0.19814703464508057,0.1951266646385193,0.19527788162231446,0.18986352682113647,0.19352309703826903,0.19710071086883546,0.19435977935791016,0.18604037761688233,0.198423171043396,0.18826236724853515,0.1897595167160034,0.18761153221130372,0.19612796306610109,0.18638348579406738,0.19588409662246703,0.1933137893676758,0.19411466121673585,0.1911036729812622,0.19453327655792235,0.19397823810577391,0.18840762376785278,0.1965220093727112,0.1856871008872986,0.18490395545959473,0.1929114818572998,0.19100066423416137,0.19346117973327637,0.19090923070907592,0.19708733558654784,0.19836962223052979,0.19771169424057006,0.19658081531524657,0.19105305671691894,0.1866983652114868,0.1911684274673462,0.19650094509124755,0.19368468523025512,0.193528151512146,0.1970961570739746,0.19821065664291382,0.19633642435073853,0.1881986141204834,0.187498939037323,0.19622888565063476,0.18874194622039794,0.19216470718383788,0.19849483966827391,0.19600315093994142,0.19440774917602538,0.1940719962120056,0.1916489601135254,0.19027529954910277,0.18952362537384032,0.20005531311035157,0.19182331562042237,0.19978756904602052,0.193223237991333,0.19489378929138185,0.1934342861175537,0.18897807598114014,0.19319969415664673,0.19673290252685546,0.18931431770324708,0.18951187133789063,0.19244948625564576,0.18986259698867797,0.1889573335647583,0.19875626564025878,0.18383642435073852,0.18905724287033082,0.18768762350082396,0.187833833694458,0.19697773456573486,0.18474192619323732,0.1905425786972046,0.19162708520889282,0.19364341497421264,0.191499924659729,0.19926350116729735,0.19933726787567138,0.19144591093063354,0.19238311052322388,0.19624168872833253,0.18669782876968383,0.18806049823760987,0.18732563257217408,0.19177713394165039,0.19140963554382323,0.1913989782333374,0.19200756549835205,0.19670259952545166,0.18978490829467773,0.18837546110153197,0.186192524433136,0.1921160936355591,0.19191781282424927,0.1864067554473877,0.19048237800598145,0.19392653703689575,0.19725611209869384,0.19759669303894042,0.18590747117996215,0.19314701557159425,0.18848536014556885,0.19496326446533202,0.1899126648902893,0.18632819652557372,0.19077107906341553,0.19462279081344605,0.18898658752441405,0.18760762214660645,0.19140310287475587,0.19265322685241698,0.19908065795898439,0.19318301677703859,0.193461012840271,0.1948906421661377,0.18996000289916992,0.19182176589965821,0.18807640075683593,0.19051551818847656,0.19534611701965332,0.18924829959869385,0.19175410270690918,0.1956761121749878,0.19482940435409546,0.1872843861579895,0.18992286920547485,0.19121687412261962,0.19744123220443727,0.19560095071792602,0.19189963340759278,0.19701845645904542,0.19954190254211426,0.19151010513305664,0.19190341234207153,0.19512442350387574,0.19435570240020753,0.1876556396484375,0.18977725505828857,0.1870429754257202,0.1891481399536133,0.18732688426971436,0.1928340196609497,0.19750072956085205,0.19433486461639404,0.1889542818069458,0.19424681663513182,0.192240571975708,0.18881011009216309,0.18885500431060792,0.1893290877342224,0.19321508407592775,0.2107818603515625,0.19280648231506348,0.1961057662963867,0.19027965068817138,0.19014278650283814,0.18803725242614747,0.19252510070800782,0.19359588623046875,0.19042272567749025,0.19142669439315796,0.19403963088989257,0.19092442989349365,0.19923421144485473,0.18699731826782226,0.18748444318771362,0.19751135110855103,0.18527294397354127,0.19247310161590575,0.18540788888931276,0.19033952951431274,0.1901885986328125,0.1905538558959961,0.18743114471435546,0.19724100828170776,0.18912365436553955,0.1895226001739502,0.19617533683776855,0.192130708694458,0.18511710166931153,0.19055182933807374,0.19376327991485595,0.19012328386306762,0.19049980640411376,0.186474609375,0.19011378288269043,0.19188301563262938,0.20305285453796387,0.19047946929931642,0.1925666093826294,0.1869558334350586,0.18972713947296144,0.19093735218048097,0.18821710348129272,0.18746129274368287,0.18654762506484984,0.1960749387741089,0.1848301410675049,0.19477931261062623,0.19400105476379395,0.19374678134918213,0.1886260986328125,0.1918300747871399,0.18497169017791748,0.1905156135559082,0.19549590349197388,0.19145489931106568,0.196943998336792,0.19130046367645265,0.1861910939216614,0.1929474353790283,0.18237360715866088,0.19864245653152465,0.18493611812591554,0.19344699382781982,0.19782919883728028,0.19189584255218506,0.19187555313110352,0.1917797803878784,0.18910061120986937,0.19683276414871215,0.19216612577438355,0.18759459257125854,0.19815220832824706,0.1908401370048523,0.20274879932403564,0.1866111636161804,0.19284477233886718,0.18629196882247925,0.1959204912185669,0.181020188331604,0.1868902325630188,0.19047535657882692,0.19688307046890258,0.19029006958007813,0.192139732837677,0.18864946365356444,0.1892363667488098,0.19470558166503907,0.19562101364135742,0.1996188759803772,0.1951403260231018,0.18855006694793702,0.19408864974975587,0.187045156955719,0.19572644233703612,0.1902460813522339,0.19627783298492432,0.19290190935134888,0.1910369873046875,0.19384628534317017,0.19455018043518066,0.19355591535568237,0.1932801127433777,0.1909063220024109,0.188620924949646,0.1890727162361145,0.19039597511291503,0.1842146635055542,0.19756097793579103,0.18753571510314943,0.1926479697227478,0.19094185829162597,0.18859831094741822,0.19975180625915528,0.19321409463882447,0.19119361639022828,0.19501372575759887,0.19229602813720703,0.19010252952575685,0.1860712766647339,0.19243732690811158,0.18876903057098388,0.18736393451690675,0.1831509590148926,0.19114294052124023,0.18773359060287476,0.19093308448791504,0.19568963050842286,0.19086132049560547,0.1890718698501587,0.19525326490402223,0.19217513799667357,0.18935770988464357,0.18641210794448854,0.19422827959060668,0.1948331117630005,0.19653401374816895,0.18661398887634278,0.19440919160842896,0.19056522846221924,0.18693937063217164,0.18703479766845704,0.1913419246673584,0.19331530332565308,0.18961867094039916,0.1828679323196411,0.1888163924217224,0.19239132404327391,0.19319878816604613,0.19114986658096314,0.18783221244812012,0.18826133012771606,0.19326882362365722,0.1881479501724243,0.19579366445541382,0.19078164100646972,0.19250669479370117,0.19396203756332397,0.193625009059906,0.18606412410736084,0.1911618232727051,0.19117747545242308,0.19137537479400635,0.1896953582763672,0.1931161403656006,0.19148683547973633,0.19237239360809327,0.17859210968017578,0.1883041501045227,0.18385441303253175,0.19479087591171265,0.1966903805732727,0.19189506769180298,0.1926991581916809,0.19235763549804688,0.19160767793655395,0.19391335248947145,0.18623260259628296,0.18765461444854736,0.1939820647239685,0.18816120624542237,0.18999173641204833,0.1898794412612915,0.19013116359710694,0.19211676120758056,0.1908287525177002,0.1908551573753357,0.1904591679573059,0.19104433059692383,0.19164574146270752,0.1930872917175293,0.1909133553504944,0.1900578260421753,0.19044978618621827,0.19162834882736207,0.1957160711288452,0.1843319058418274,0.19367679357528686,0.18575314283370972,0.19041664600372316,0.19961432218551636,0.1959398627281189,0.1851353406906128,0.1911402463912964,0.18452270030975343,0.1861637592315674,0.18903563022613526,0.18925042152404786,0.19485756158828735,0.18700888156890869,0.19674631357192993,0.18927310705184935,0.19178653955459596,0.1828504681587219,0.1935439109802246,0.18914334774017333,0.18698145151138307,0.18642197847366332,0.1882317304611206,0.19204680919647216,0.19192947149276735,0.1917118549346924,0.18480262756347657,0.18656508922576903,0.19878199100494384,0.19523322582244873,0.19020457267761232,0.19974814653396605,0.18651285171508789,0.19077163934707642,0.1931499123573303,0.1900359034538269,0.19416685104370118,0.19373297691345215,0.19589030742645264,0.1838115334510803,0.19455380439758302,0.1923760175704956,0.18945786952972413,0.18798599243164063,0.19293121099472046,0.18537395000457763,0.18697447776794435,0.19149178266525269,0.18987762928009033,0.1936277389526367,0.19215332269668578,0.18248108625411988,0.18918302059173583,0.19199447631835936,0.18518922328948975,0.18333567380905152,0.17881572246551514,0.18950290679931642,0.18897457122802735,0.1939988613128662,0.18903239965438842,0.18572421073913575,0.1918264150619507,0.1826311707496643,0.1939762830734253,0.19526456594467162,0.18276678323745726,0.18419125080108642,0.19214508533477784,0.19542778730392457,0.19485357999801636,0.18111566305160523,0.18695077896118165,0.19248138666152953,0.1980891704559326,0.18576598167419434,0.19203567504882812,0.19195523262023925,0.18812066316604614,0.18877074718475342,0.18832483291625976,0.18967318534851074,0.1948625087738037,0.1877284049987793,0.1938166379928589,0.19119490385055543,0.18261337280273438,0.1901031732559204,0.18265073299407958,0.1970759153366089,0.19177987575531005,0.18803880214691163,0.19069727659225463,0.18405430316925048,0.19621629714965821,0.1828635573387146,0.18654017448425292,0.19124480485916137,0.18921422958374023,0.19030311107635497,0.19167202711105347,0.19466670751571655,0.18587710857391357,0.18839671611785888,0.18937708139419557,0.185044002532959,0.18699464797973633,0.18981869220733644,0.19403040409088135,0.1971163868904114,0.19168561697006226,0.19071354866027831,0.18446611166000365,0.1831258773803711,0.18740744590759278,0.17899515628814697,0.1898226857185364,0.19470996856689454,0.180134117603302,0.18885130882263185,0.19262430667877198,0.19153523445129395,0.18949649333953858,0.19004523754119873,0.1850944995880127,0.18744610548019408,0.18955148458480836,0.1904157042503357,0.19107102155685424,0.18624731302261352,0.18800946474075317,0.1916213035583496,0.18505518436431884,0.1935424327850342,0.19561995267868043,0.18542635440826416,0.19675912857055664,0.18458476066589355,0.1931331515312195,0.190176522731781,0.1824120283126831,0.18698477745056152,0.18948405981063843,0.18350434303283691,0.18623629808425904,0.19486366510391234,0.19524223804473878,0.18569658994674682,0.19105665683746337,0.1821288824081421,0.18775471448898315,0.18827733993530274,0.1919501543045044,0.19091837406158446,0.19594779014587402,0.1954747200012207,0.19088995456695557,0.19079203605651857,0.1972133755683899,0.18342015743255616,0.18788435459136962,0.18236083984375,0.19614176750183104,0.1878216028213501,0.18294310569763184,0.19160009622573854,0.19430327415466309,0.19354751110076904,0.18830587863922119,0.19086141586303712,0.19498214721679688,0.20057437419891358,0.19818755388259887,0.1941293239593506,0.1837711215019226,0.19704656600952147,0.1877846598625183,0.18326094150543212,0.18684579133987428,0.19228265285491944,0.18751517534255982,0.1876363754272461,0.19157177209854126,0.18975266218185424,0.184650719165802,0.18419355154037476,0.18525584936141967,0.1919865369796753,0.1892560601234436,0.1864023208618164,0.1872548818588257,0.18366410732269287,0.18651440143585205,0.18594456911087037,0.19280529022216797,0.18982548713684083,0.18850840330123902,0.1887340784072876,0.17840851545333863,0.19110989570617676,0.18968591690063477,0.1852973461151123,0.18493447303771973,0.1855319619178772,0.18622710704803466,0.19248450994491578,0.18913646936416625,0.18709194660186768,0.19750072956085205,0.18249634504318238,0.19033621549606322,0.19383058547973633,0.1865147113800049,0.18900082111358643,0.18890037536621093,0.18550348281860352,0.19515998363494874,0.19241305589675903,0.1846078634262085,0.19308624267578126,0.1916326880455017,0.1846708297729492,0.187951397895813,0.1874943971633911,0.18705353736877442,0.19167468547821045,0.18922312259674073,0.18749254941940308,0.18555049896240233,0.19185945987701417,0.18853704929351806,0.19178813695907593,0.1869104266166687,0.18791069984436035,0.18966012001037597,0.1794463038444519,0.19053503274917602,0.18596856594085692,0.18921887874603271,0.18702093362808228,0.18743107318878174,0.1887684464454651,0.1840407967567444,0.19179248809814453,0.18809797763824462,0.19136512279510498,0.19075063467025757,0.1882038116455078,0.18848978281021117,0.18958243131637573,0.1923130750656128,0.18793716430664062,0.18691513538360596,0.18674514293670655,0.1866842031478882,0.19454816579818726,0.1866887927055359,0.1874564528465271,0.18626418113708496,0.19684451818466187,0.19212768077850342,0.18794654607772826,0.18078383207321166,0.18924190998077392,0.18216460943222046,0.1926555037498474,0.19377127885818482,0.19398505687713624,0.18395522832870484,0.19747387170791625,0.190595805644989,0.18940894603729247,0.18825981616973878,0.18949713706970214,0.18657801151275635,0.1917734384536743,0.17855145931243896,0.18728262186050415,0.18657946586608887,0.1936877489089966,0.18770145177841185,0.189650297164917,0.18165009021759032,0.1861378073692322,0.18698763847351074,0.1851770043373108,0.1855539083480835,0.19001121520996095,0.19596067667007447,0.1857028603553772,0.18219555616378785,0.18866403102874757,0.1878186583518982,0.18937308788299562,0.18639073371887208,0.18527460098266602,0.1885823130607605,0.1879873275756836,0.18835823535919188,0.18587920665740967,0.1922709822654724,0.18830320835113526,0.18288326263427734,0.19404138326644899,0.18255763053894042,0.19572780132293702,0.18650386333465577,0.19068846702575684,0.19349191188812256,0.18159788846969604,0.18314404487609864,0.18626248836517334,0.18133130073547363,0.17993682622909546,0.18323943614959717,0.18680475950241088,0.190335214138031,0.1913634181022644,0.1919248580932617,0.18576184511184693,0.19154874086380005,0.1835999846458435,0.1916811466217041,0.18928685188293456,0.19238533973693847,0.1903978705406189,0.19321694374084472,0.1874847412109375,0.187072229385376,0.18613553047180176,0.1842414140701294,0.19198496341705323,0.19682488441467286,0.18464717864990235,0.19293012619018554,0.18965933322906495,0.19283418655395507,0.18890870809555055,0.1840693712234497,0.1910482883453369,0.18526642322540282,0.18518385887145997,0.1888486385345459,0.1999263048171997,0.18305832147598267,0.18904112577438353,0.1907423734664917,0.1850066900253296,0.19221291542053223,0.18492624759674073,0.18773694038391114,0.19159127473831178,0.1870542049407959,0.18640336990356446,0.19059103727340698,0.18732364177703859,0.17785193920135497,0.1852344512939453,0.18207099437713622,0.18672139644622804,0.18805150985717772,0.18155333995819092,0.1824584722518921,0.18751788139343262,0.1899914860725403,0.1901382565498352,0.1893749475479126,0.18112140893936157,0.19117798805236816,0.18946938514709472,0.19186872243881226,0.18709027767181396,0.1949455976486206,0.1906861186027527,0.197057044506073,0.19561362266540527,0.1935080647468567,0.19208451509475707,0.19327933788299562,0.18832193613052367,0.19273757934570312,0.19097254276275635,0.18781137466430664,0.19232850074768065,0.1833409547805786,0.19366855621337892,0.18545540571212768,0.191670823097229,0.19828057289123535,0.18546364307403565,0.1852238655090332,0.18755239248275757,0.1940720796585083,0.18262848854064942,0.19269222021102905,0.18292468786239624,0.18576903343200685,0.18807305097579957,0.1863077163696289,0.18667979240417482,0.18838609457015992,0.1823594093322754,0.18081928491592408,0.18335654735565185,0.18560636043548584,0.1844485282897949,0.1731666922569275,0.18252856731414796,0.18575562238693238,0.1955513834953308,0.18618905544281006,0.18713150024414063,0.18845465183258056,0.19366384744644166,0.18577196598052978,0.18809067010879515,0.18191059827804565,0.18840013742446898,0.18704593181610107,0.19199752807617188,0.18607856035232545,0.1847764492034912,0.1841585397720337,0.18433471918106079,0.18633770942687988,0.18526415824890136,0.19126797914505006,0.19372618198394775,0.1870396375656128,0.1868033766746521,0.18757680654525757,0.18351309299468993,0.1890251398086548,0.1894575834274292,0.18455076217651367,0.18210954666137696,0.18859806060791015,0.18369213342666627,0.17937248945236206,0.19099518060684204,0.18770413398742675,0.19157860279083253,0.19109508991241456,0.1790350317955017,0.1852356195449829,0.19036871194839478,0.18517670631408692,0.19615558385849,0.1879021406173706,0.18777554035186766,0.1869547963142395,0.1900683879852295,0.1877583622932434,0.18720335960388185,0.1882184386253357,0.195500648021698,0.1863226056098938,0.18685390949249267,0.1791716694831848,0.194863760471344,0.19827618598937988,0.19404199123382568,0.18741737604141234,0.19433799982070923,0.19289138317108154,0.19229592084884645,0.18573176860809326,0.18462884426116943,0.1956777334213257,0.1819985628128052,0.18041772842407228,0.1824256658554077,0.18933141231536865,0.18487107753753662,0.1959443211555481,0.19237878322601318,0.1848085880279541,0.18433624505996704,0.194826340675354,0.19525456428527832,0.19764877557754518,0.18944356441497803,0.19039957523345946,0.18244196176528932,0.1843029260635376,0.18572120666503905,0.18382996320724487,0.1871003031730652,0.1857091188430786,0.18674415349960327,0.18725844621658325,0.18698996305465698,0.1874879002571106,0.17821874618530273,0.18811894655227662,0.19667016267776488,0.19440509080886842,0.1917941927909851,0.18912005424499512,0.19463913440704345,0.19363219738006593,0.1937311053276062,0.19142836332321167,0.18022838830947877,0.19158625602722168,0.18311280012130737,0.1797553777694702,0.18683799505233764,0.1767444133758545,0.18535771369934081,0.19126315116882325,0.1822238087654114,0.18603569269180298,0.18558521270751954,0.18497463464736938,0.18446459770202636,0.18897961378097533,0.18662126064300538,0.1880239486694336,0.19160280227661133,0.1912304162979126,0.18597607612609862,0.1848289966583252,0.18358774185180665,0.1869118928909302,0.18146467208862305,0.18189654350280762,0.1762758255004883,0.1874366283416748,0.19157222509384156,0.19001373052597045,0.1970095753669739,0.18665807247161864,0.18666276931762696,0.19440393447875975,0.1880800485610962,0.18328561782836914,0.1932775378227234,0.1852056384086609,0.19322125911712645,0.1910884737968445,0.18553422689437865,0.19224107265472412,0.17804735898971558,0.18657989501953126,0.19145612716674804,0.18585110902786256,0.1811484932899475,0.18606081008911132,0.1796025037765503,0.1925894021987915,0.18440918922424315,0.19001421928405762,0.18885265588760375,0.1945134401321411,0.18638935089111328,0.19155184030532837,0.18748691082000732,0.18893046379089357,0.18845876455307006,0.18115544319152832,0.18940587043762208,0.18794379234313965,0.1888332724571228,0.18045215606689452,0.1894906997680664,0.18367528915405273,0.1930704355239868,0.1887161135673523,0.1866304397583008,0.1855273127555847,0.17998871803283692,0.1833803415298462,0.18261573314666749,0.18433840274810792,0.1845582127571106,0.19312487840652465,0.19282073974609376,0.18329181671142578,0.1834214448928833,0.18004406690597535,0.19208364486694335,0.19019485712051393,0.1861208438873291,0.1925356864929199,0.1899272918701172,0.1881101369857788,0.17922699451446533,0.18263672590255736,0.18306396007537842,0.18082453012466432,0.18724178075790404,0.18004913330078126,0.18234605789184571,0.18771448135375976,0.19542531967163085,0.18163059949874877,0.18681211471557618,0.17928287982940674,0.182546865940094,0.19234172105789185,0.18724353313446046,0.19144721031188966,0.19218666553497316,0.18249940872192383,0.19109835624694824,0.1847182035446167,0.17932029962539672,0.19028868675231933,0.18500056266784667,0.18047564029693602,0.18586366176605223,0.18851151466369628,0.18225641250610353,0.17903378009796142,0.18553198575973512,0.18127647638320923,0.18396848440170288,0.1911829710006714,0.19384909868240358,0.1826666235923767,0.1805223345756531,0.18851699829101562,0.17874343395233155,0.18752520084381102,0.1830212116241455,0.18481113910675048,0.18269094228744506,0.19096779823303223,0.1891412615776062,0.17779183387756348,0.1892082691192627,0.1859383463859558,0.17897932529449462,0.18814122676849365,0.1833552360534668,0.18129609823226928,0.19074633121490478,0.18742889165878296,0.1856417179107666,0.18704792261123657,0.19439780712127686,0.18495326042175292,0.19864953756332399,0.1961613655090332,0.18991986513137818,0.18263144493103028,0.18796219825744628,0.18084228038787842,0.18200209140777587,0.1921204924583435,0.19748069047927858,0.17904791831970215,0.18547961711883545,0.19548381567001344,0.18392823934555053,0.1871715784072876,0.1815093755722046,0.18907009363174437,0.1865551471710205,0.1847382068634033,0.18665852546691894,0.18160171508789064,0.19141614437103271,0.18232563734054566,0.17580156326293944,0.1903796076774597,0.1894822597503662,0.1878448486328125,0.19393755197525026,0.1888329029083252,0.18468976020812988,0.17809094190597535,0.18426792621612548,0.18870854377746582,0.19133970737457276,0.18861281871795654,0.19349576234817506,0.18480066061019898,0.1896551012992859,0.19637022018432618,0.1877496838569641,0.1930107593536377,0.18786282539367677,0.18517012596130372,0.1847091794013977,0.18409086465835572,0.18481099605560303,0.18621302843093873,0.18408478498458863,0.1891712188720703,0.19515492916107177,0.1908641576766968,0.19204949140548705,0.18646284341812133,0.19295569658279418,0.18477988243103027,0.18817706108093263,0.18348941802978516,0.18645341396331788,0.19321168661117555,0.18706369400024414,0.18677160739898682,0.1873821496963501,0.18261549472808838,0.1903247833251953,0.19867157936096191,0.18785345554351807,0.1851351261138916,0.18154579401016235,0.18096141815185546,0.18583229780197144,0.18612470626831054,0.19075727462768555,0.18553295135498046,0.1915760040283203,0.18186688423156738,0.1881159543991089,0.19291243553161622,0.19378528594970704,0.19334195852279662,0.1872033953666687,0.19268633127212526,0.18204922676086427,0.18460384607315064,0.18764839172363282,0.19191331863403321,0.18357315063476562,0.1852329730987549,0.18492900133132933,0.1876429796218872,0.18022977113723754,0.19055140018463135,0.18129210472106932,0.18251068592071534,0.18558740615844727,0.18409205675125123,0.18726322650909424,0.1843062996864319,0.1847271203994751,0.18648693561553956,0.18233754634857177,0.18865106105804444,0.18157414197921753,0.18618645668029785,0.18289147615432738,0.18839746713638306,0.18401753902435303,0.18480386734008789,0.18954917192459106,0.18227131366729737,0.1860125780105591,0.19440174102783203,0.1853433609008789,0.19809975624084472,0.18742952346801758,0.19091858863830566,0.1868281841278076,0.1828702926635742,0.18334743976593018,0.18310167789459228,0.18761229515075684,0.18711053133010863,0.1860891342163086,0.18570610284805297,0.20030369758605956,0.1806489944458008,0.1850113868713379,0.18661463260650635,0.1831912398338318,0.18593988418579102,0.18533389568328856,0.18018953800201415,0.18675320148468016,0.18814637660980224,0.18309224843978883,0.18914501667022704,0.17307753562927247,0.18845973014831544,0.18555495738983155,0.18007256984710693,0.1830942749977112,0.18984777927398683,0.189388644695282,0.18541816473007203,0.1819467306137085,0.1835179328918457,0.1816352128982544,0.19451507329940795,0.18146278858184814,0.17902793884277343,0.18433911800384523,0.18679803609848022,0.1839076519012451,0.1904909610748291,0.18394442796707153,0.1820732593536377,0.18006540536880494,0.18464868068695067,0.1847563624382019,0.17979238033294678,0.19112465381622315,0.18440158367156984,0.19187890291213988,0.1847402811050415,0.18341355323791503,0.17854316234588624,0.18559021949768068,0.18415913581848145,0.18509920835494995,0.18253446817398072,0.19067952632904053,0.177125883102417,0.19094748497009278,0.18838627338409425,0.18384681940078734,0.18138872385025023,0.1902800679206848,0.18503384590148925,0.18862125873565674,0.18546451330184938,0.18739602565765381,0.18537987470626832,0.18732360601425171,0.1891002655029297,0.1852677583694458,0.17690937519073485,0.17547273635864258,0.18699407577514648,0.1903218984603882,0.18623459339141846,0.17397855520248412,0.18437793254852294,0.1771925687789917,0.18392786979675294,0.18025445938110352,0.17990598678588868,0.19807653427124022,0.18118789196014404,0.18032431602478027,0.19079614877700807,0.19293107986450195,0.1832227110862732,0.18405299186706542,0.18780198097229003,0.18397458791732788,0.17984321117401122,0.18578484058380126,0.18058629035949708,0.18177815675735473,0.1728561282157898,0.18645333051681517,0.18490052223205566,0.18532674312591552,0.18114712238311767,0.1797979712486267,0.18020522594451904,0.17706433534622193]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-1915618569', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 37.0%\n"
     ]
    }
   ],
   "source": [
    "predictResult.unsafePerformSyncAttempt match {\n",
    "  case -\\/(e) => {\n",
    "    throw e\n",
    "  }\n",
    "  case \\/-(result) =>\n",
    "    println(\"The accuracy is \" + Utils.getAccuracy(result,testExpectResult) + \"%\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have learned the follows in this article:\n",
    "\n",
    "* Mini-Batch Gradient Descent\n",
    "* epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source code](https://github.com/izhangzhihao/deeplearning-tutorial/blob/2.0.x/src/main/scala/com/github/izhangzhihao/MiniBatchGradientDescent.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
