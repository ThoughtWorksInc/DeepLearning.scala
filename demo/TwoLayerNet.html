---
layout: demo
title: TwoLayerNet
download_path: demo_download/.
filename: TwoLayerNet.ipynb
---
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Background">Background<a class="anchor-link" href="#Background">&#182;</a></h2><p>To improve the prediction accuracy, we need to use multi-layer neural network, because generally, the more the layers of a network, the higher the capability of the network. That is because the more the parameters, the more the state information represented, and the stronger the expression capability will be. Multi-layer neural network can tackle with more complex problems.</p>
<p>In this article, we will first define a simple neural network, and then use the training set of <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a> to train this neural network. Finally, we will use a test set to verify the accuracy of the neural network, and the final accuracy can reach 51%.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Import-dependencies">Import dependencies<a class="anchor-link" href="#Import-dependencies">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$plugin.$ivy.</span><span class="n">`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableany:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablenothing:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableseq:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiabledouble:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablefloat:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablehlist:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableindarray:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.nd4j:nd4j-native-platform:0.7.2`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.rauschig:jarchivelib:0.5.0`</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.plotly-scala::plotly-jupyter-scala:0.3.0`</span>

<span class="k">import</span> <span class="nn">java.io.</span><span class="o">{</span><span class="nc">FileInputStream</span><span class="o">,</span> <span class="nc">InputStream</span><span class="o">}</span>


<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableHList._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableDouble._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableINDArray._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableAny._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableINDArray.Layers.Weight</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.</span><span class="o">{</span>
  <span class="nc">DifferentiableHList</span><span class="o">,</span>
  <span class="nc">DifferentiableINDArray</span><span class="o">,</span>
  <span class="nc">Layer</span><span class="o">,</span>
  <span class="nc">Symbolic</span>
<span class="o">}</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Layer.Tape</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Symbolic.Layers.Identity</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Symbolic._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathFunctions._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathMethods.</span><span class="o">/</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathOps</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.factory.Nd4j</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.indexing.</span><span class="o">{</span><span class="nc">INDArrayIndex</span><span class="o">,</span> <span class="nc">NDArrayIndex</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.ops.transforms.Transforms</span>
<span class="k">import</span> <span class="nn">org.nd4s.Implicits._</span>
<span class="k">import</span> <span class="nn">shapeless._</span>

<span class="k">import</span> <span class="nn">plotly._</span>
<span class="k">import</span> <span class="nn">plotly.element._</span>
<span class="k">import</span> <span class="nn">plotly.layout._</span>
<span class="k">import</span> <span class="nn">plotly.JupyterScala._</span>

<span class="k">import</span> <span class="nn">scala.collection.immutable.IndexedSeq</span>
<span class="k">import</span> <span class="nn">scala.util.Random</span>

<span class="n">pprintConfig</span><span class="o">()</span> <span class="k">=</span> <span class="n">pprintConfig</span><span class="o">().</span><span class="n">copy</span><span class="o">(</span><span class="n">height</span> <span class="k">=</span> <span class="mi">2</span><span class="o">)</span>

<span class="k">import</span> <span class="nn">$file.ReadCIFAR10ToNDArray</span>
<span class="k">import</span> <span class="nn">$file.Utils</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Compiling ReadCIFAR10ToNDArray.sc
Compiling Utils.sc
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[5]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$plugin.$                                                                             

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                       
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                           
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                       
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                          
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                         
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                         
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                             
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                            
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                    
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                               

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                             

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">java.io.{FileInputStream, InputStream}


</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableHList._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableDouble._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableINDArray._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableAny._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableINDArray.Layers.Weight
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.{
  DifferentiableHList,
  DifferentiableINDArray,
  Layer,
  Symbolic
}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Layer.Tape
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Symbolic.Layers.Identity
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Symbolic._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathFunctions._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathMethods./
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathOps
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.factory.Nd4j
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.ops.transforms.Transforms
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4s.Implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">shapeless._

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.element._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.layout._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.JupyterScala._

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.collection.immutable.IndexedSeq
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.util.Random

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$file.$                   
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$file.$    </span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Build-two-layers-of-neural-network">Build two layers of neural network<a class="anchor-link" href="#Build-two-layers-of-neural-network">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Parameter-tuning">Parameter tuning<a class="anchor-link" href="#Parameter-tuning">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This article is different with the previous article, in this article, we will adopt some means for parameter tuning, set learning rate and use <a href="http://neuralnetworksanddeeplearning.com/chap3.html">L2Regularization</a>,L2Regularization can be used to avoid <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>. We also solved the problem of too-slow decrease or no decrease of <code>loss</code> due to relatively high <code>learningRate</code> during extended training time, by decreasing each iteration <code>learningRate</code> to that 0.9995 time of each of its original value.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">implicit</span> <span class="k">val</span> <span class="n">optimizerFactory</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DifferentiableINDArray</span><span class="o">.</span><span class="nc">OptimizerFactory</span> <span class="o">{</span>
  <span class="k">override</span> <span class="k">def</span> <span class="n">ndArrayOptimizer</span><span class="o">(</span><span class="n">weight</span><span class="k">:</span> <span class="kt">Weight</span><span class="o">)</span><span class="k">:</span> <span class="kt">Optimizer</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">new</span> <span class="nc">LearningRate</span> <span class="k">with</span> <span class="n">L2Regularization</span> <span class="o">{</span>

      <span class="k">var</span> <span class="n">learningRate</span> <span class="k">=</span> <span class="mf">0.001</span>

      <span class="k">override</span> <span class="k">protected</span> <span class="k">def</span> <span class="n">currentLearningRate</span><span class="o">()</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">learningRate</span> <span class="o">*=</span> <span class="mf">0.9995</span>
        <span class="n">learningRate</span>
      <span class="o">}</span>

      <span class="k">override</span> <span class="k">protected</span> <span class="k">def</span> <span class="n">l2Regularization</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="mf">0.03</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[6]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">optimizerFactory</span>: <span class="ansi-green-fg">AnyRef</span> with <span class="ansi-green-fg">OptimizerFactory</span> = $sess.cmd5Wrapper$Helper$$anon$2@5e698c2</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Write-the-first-layer-of-the-neural-network">Write the first layer of the neural network<a class="anchor-link" href="#Write-the-first-layer-of-the-neural-network">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is the neural network consisted of full connection and <a href="http://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-network">relu</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">fullyConnectedThenRelu</span><span class="o">(</span><span class="n">inputSize</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">outputSize</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)(</span>
    <span class="k">implicit</span> <span class="n">row</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">w</span> <span class="k">=</span> <span class="o">(</span><span class="nc">Nd4j</span><span class="o">.</span><span class="n">randn</span><span class="o">(</span><span class="n">inputSize</span><span class="o">,</span> <span class="n">outputSize</span><span class="o">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">outputSize</span> <span class="o">/</span> <span class="mf">2.0</span><span class="o">)).</span><span class="n">toWeight</span> <span class="o">*</span> <span class="mf">0.1</span>
  <span class="k">val</span> <span class="n">b</span> <span class="k">=</span> <span class="nc">Nd4j</span><span class="o">.</span><span class="n">zeros</span><span class="o">(</span><span class="n">outputSize</span><span class="o">).</span><span class="n">toWeight</span>
  <span class="n">max</span><span class="o">((</span><span class="n">row</span> <span class="n">dot</span> <span class="n">w</span><span class="o">)</span> <span class="o">+</span> <span class="n">b</span><span class="o">,</span> <span class="mf">0.0</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[7]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">fullyConnectedThenRelu</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Write-the-second-layer-of-the-neural-network">Write the second layer of the neural network<a class="anchor-link" href="#Write-the-second-layer-of-the-neural-network">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Like the last article, we use <code>softmax</code> as the classifier.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">softmax</span><span class="o">(</span><span class="k">implicit</span> <span class="n">scores</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">expScores</span> <span class="k">=</span> <span class="n">exp</span><span class="o">(</span><span class="n">scores</span><span class="o">)</span>
  <span class="n">expScores</span> <span class="o">/</span> <span class="n">expScores</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[8]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">softmax</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Write the second neural network of the two layers of the neural network. This is a neural network consisted of a layer of full connection and a layer of <code>softmax</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">fullyConnectedThenSoftmax</span><span class="o">(</span><span class="n">inputSize</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">outputSize</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)(</span>
    <span class="k">implicit</span> <span class="n">row</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">w</span> <span class="k">=</span> <span class="o">(</span><span class="nc">Nd4j</span><span class="o">.</span><span class="n">randn</span><span class="o">(</span><span class="n">inputSize</span><span class="o">,</span> <span class="n">outputSize</span><span class="o">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="o">(</span><span class="n">outputSize</span><span class="o">)).</span><span class="n">toWeight</span>
  <span class="k">val</span> <span class="n">b</span> <span class="k">=</span> <span class="nc">Nd4j</span><span class="o">.</span><span class="n">zeros</span><span class="o">(</span><span class="n">outputSize</span><span class="o">).</span><span class="n">toWeight</span>
  <span class="n">softmax</span><span class="o">.</span><span class="n">compose</span><span class="o">((</span><span class="n">row</span> <span class="n">dot</span> <span class="n">w</span><span class="o">)</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[9]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">fullyConnectedThenSoftmax</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Combine-two-layers-of-the-neural-network">Combine two layers of the neural network<a class="anchor-link" href="#Combine-two-layers-of-the-neural-network">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To implement two layers of neural network, we use <code>compose</code> to combine the above two layers of neural networks into one tow-layer neural network. <code>a.compose(b)</code> can input the output of <code>b</code> as <code>a</code>, so as to combine the two layers of neural network.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="nc">NumberOfPixels</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">3072</span>
<span class="k">def</span> <span class="n">hiddenLayer</span><span class="o">(</span><span class="k">implicit</span> <span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">layer0</span> <span class="k">=</span> <span class="n">fullyConnectedThenRelu</span><span class="o">(</span><span class="nc">NumberOfPixels</span><span class="o">,</span> <span class="mi">500</span><span class="o">).</span><span class="n">compose</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>
  <span class="n">fullyConnectedThenSoftmax</span><span class="o">(</span><span class="mi">500</span><span class="o">,</span> <span class="mi">10</span><span class="o">).</span><span class="n">compose</span><span class="o">(</span><span class="n">layer0</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">predictor</span> <span class="k">=</span> <span class="n">hiddenLayer</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stderr output_text">
<pre>SLF4J: Failed to load class &#34;org.slf4j.impl.StaticLoggerBinder&#34;.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[10]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">NumberOfPixels</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">3072</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">hiddenLayer</span>
<span class="ansi-cyan-fg">predictor</span>: (<span class="ansi-green-fg">Symbolic</span>.<span class="ansi-green-fg">To</span>[<span class="ansi-green-fg">INDArray</span>]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#<span class="ansi-green-fg">@</span> = Compose(Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),PlusINDArray(Dot(Identity(),Weight([[-0.03, 0.16, -0.52, 0.09<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Write-network-and-combine-the-input-layer-and-hidden-layer">Write <code>network</code> and combine the input layer and <a href="http://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute">hidden layer</a><a class="anchor-link" href="#Write-network-and-combine-the-input-layer-and-hidden-layer">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">crossEntropy</span><span class="o">(</span>
    <span class="k">implicit</span> <span class="n">pair</span><span class="k">:</span> <span class="o">(</span><span class="kt">INDArray</span> <span class="kt">::</span> <span class="kt">INDArray</span> <span class="kt">::</span> <span class="kt">HNil</span><span class="o">)</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">score</span> <span class="k">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">head</span>
  <span class="k">val</span> <span class="n">label</span> <span class="k">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>
  <span class="o">-(</span><span class="n">label</span> <span class="o">*</span> <span class="n">log</span><span class="o">(</span><span class="n">score</span> <span class="o">*</span> <span class="mf">0.9</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">)</span> <span class="o">+</span> <span class="o">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">label</span><span class="o">)</span> <span class="o">*</span> <span class="n">log</span><span class="o">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">score</span> <span class="o">*</span> <span class="mf">0.9</span><span class="o">)).</span><span class="n">mean</span>
<span class="o">}</span>

<span class="k">def</span> <span class="n">network</span><span class="o">(</span>
   <span class="k">implicit</span> <span class="n">pair</span><span class="k">:</span> <span class="o">(</span><span class="kt">INDArray</span> <span class="kt">::</span> <span class="kt">INDArray</span> <span class="kt">::</span> <span class="kt">HNil</span><span class="o">)</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">input</span> <span class="k">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">head</span>
  <span class="k">val</span> <span class="n">label</span> <span class="k">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>
  <span class="k">val</span> <span class="n">score</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">compose</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>
  <span class="k">val</span> <span class="n">hnilLayer</span><span class="k">:</span> <span class="kt">HNil</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="nc">HNil</span>
  <span class="n">crossEntropy</span><span class="o">.</span><span class="n">compose</span><span class="o">(</span><span class="n">score</span> <span class="o">::</span> <span class="n">label</span> <span class="o">::</span> <span class="n">hnilLayer</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">trainer</span> <span class="k">=</span> <span class="n">network</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[11]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">crossEntropy</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">network</span>
<span class="ansi-cyan-fg">trainer</span>: (<span class="ansi-green-fg">Symbolic</span>.<span class="ansi-green-fg">To</span>[<span class="ansi-green-fg">Double</span>]{type OutputData = Double;type OutputDelta = Double;type InputData = shapeless.::[org.nd4j.linalg.api.ndarray.INDArray,shapeless.::[org.nd4j.linalg.api.ndarray.INDArray,shapeless.HNil]];type InputDelta = shapeless.:+:[org.nd4j.linalg.api.ndarray.INDArray,shapeless.:+:[org.nd4j.linalg.api.ndarray.INDArray,shapeless.CNil]]})#<span class="ansi-green-fg">@</span> = Compose(Negative(ReduceMean(PlusINDArray(MultiplyINDArray(Head(Tail(Identity())),Log(PlusDouble(MultiplyDouble(Head(Identity()),Literal(0.9)),Literal(0.1)))),Mu<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-the-neural-network">Train the neural network<a class="anchor-link" href="#Train-the-neural-network">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Like the last article, train the neural network and observe the change of <code>loss</code> in each training.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">random</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Random</span>

<span class="k">val</span> <span class="nc">MiniBatchSize</span> <span class="k">=</span> <span class="mi">256</span>

<span class="c1">//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)</span>
<span class="k">val</span> <span class="nc">NumberOfClasses</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">def</span> <span class="n">trainData</span><span class="o">(</span><span class="n">randomIndexArray</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">trainNDArray</span> <span class="o">::</span> <span class="n">expectLabel</span> <span class="o">::</span> <span class="n">shapeless</span><span class="o">.</span><span class="nc">HNil</span> <span class="k">=</span>
    <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="n">getSGDTrainNDArray</span><span class="o">(</span><span class="n">randomIndexArray</span><span class="o">)</span>
  <span class="k">val</span> <span class="n">input</span> <span class="k">=</span>
    <span class="n">trainNDArray</span><span class="o">.</span><span class="n">reshape</span><span class="o">(</span><span class="nc">MiniBatchSize</span><span class="o">,</span> <span class="nc">NumberOfPixels</span><span class="o">)</span>

  <span class="k">val</span> <span class="n">expectLabelVectorized</span> <span class="k">=</span>
    <span class="nc">Utils</span><span class="o">.</span><span class="n">makeVectorized</span><span class="o">(</span><span class="n">expectLabel</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span>
  <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="o">(</span><span class="n">input</span> <span class="o">::</span> <span class="n">expectLabelVectorized</span> <span class="o">::</span> <span class="nc">HNil</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">lossSeq</span> <span class="k">=</span>
  <span class="o">(</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">iteration</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">to</span> <span class="mi">50</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">randomIndex</span> <span class="k">=</span> <span class="n">random</span>
        <span class="o">.</span><span class="n">shuffle</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">IndexedSeq</span><span class="o">](</span><span class="mi">0</span> <span class="n">until</span> <span class="mi">10000</span><span class="o">)</span> <span class="c1">//https://issues.scala-lang.org/browse/SI-6948</span>
        <span class="o">.</span><span class="n">toArray</span>
      <span class="k">for</span> <span class="o">(</span><span class="n">times</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="mi">10000</span> <span class="o">/</span> <span class="nc">MiniBatchSize</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
        <span class="k">val</span> <span class="n">randomIndexArray</span> <span class="k">=</span>
          <span class="n">randomIndex</span><span class="o">.</span><span class="n">slice</span><span class="o">(</span><span class="n">times</span> <span class="o">*</span> <span class="nc">MiniBatchSize</span><span class="o">,</span>
                            <span class="o">(</span><span class="n">times</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="o">*</span> <span class="nc">MiniBatchSize</span><span class="o">)</span>
          <span class="k">val</span> <span class="n">loss</span> <span class="k">=</span> <span class="n">trainData</span><span class="o">(</span><span class="n">randomIndexArray</span><span class="o">)</span>
          <span class="k">if</span><span class="o">(</span><span class="n">times</span> <span class="o">==</span> <span class="mi">3</span> <span class="o">&amp;</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">4</span><span class="o">){</span>
            <span class="n">println</span><span class="o">(</span><span class="s">&quot;at epoch &quot;</span> <span class="o">+</span> <span class="o">(</span><span class="n">iteration</span> <span class="o">/</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; loss is :&quot;</span> <span class="o">+</span> <span class="n">loss</span><span class="o">)</span>
          <span class="o">}</span>
          <span class="n">loss</span>
      <span class="o">}</span>
    <span class="o">}</span>
  <span class="o">).</span><span class="n">flatten</span>

<span class="n">plotly</span><span class="o">.</span><span class="nc">JupyterScala</span><span class="o">.</span><span class="n">init</span><span class="o">()</span>

<span class="k">val</span> <span class="n">plot</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
  <span class="nc">Scatter</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">.</span><span class="n">indices</span><span class="o">,</span> <span class="n">lossSeq</span><span class="o">)</span>
<span class="o">)</span>

<span class="n">plot</span><span class="o">.</span><span class="n">plot</span><span class="o">(</span>
  <span class="n">title</span> <span class="k">=</span> <span class="s">&quot;loss by time&quot;</span>
<span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>at epoch 1 loss is :0.2202770948410034
at epoch 2 loss is :0.21146070957183838
at epoch 3 loss is :0.20522694587707518
at epoch 4 loss is :0.1946331739425659
at epoch 5 loss is :0.18703371286392212
at epoch 6 loss is :0.19631543159484863
at epoch 7 loss is :0.18934404850006104
at epoch 8 loss is :0.18849481344223024
at epoch 9 loss is :0.18192483186721803
at epoch 10 loss is :0.17548918724060059
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">

      <script type="text/javascript">
        require.config({
  paths: {
    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',
    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'
  },

  shim: {
    plotly: {
      deps: ['d3', 'jquery'],
      exports: 'plotly'
    }
  }
});
        

        require(['plotly'], function(Plotly) {
          window.Plotly = Plotly;
        });
      </script>
    
</div>

</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div class="chart" id="plot-1350734249"></div>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>




<div id="3714608d-9868-417f-884c-2f419952dfec"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#3714608d-9868-417f-884c-2f419952dfec');
requirejs(["plotly"], function(Plotly) {
  (function () {
  var data0 = {"type":"scatter","x":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0],"y":[0.25606164932250974,0.25506224632263186,0.2535944223403931,0.2502109527587891,0.2539495944976807,0.25488479137420655,0.2661862850189209,0.2693119764328003,0.2811598539352417,0.28043057918548586,0.28128795623779296,0.31333212852478026,0.2948655128479004,0.3939645528793335,0.3664750099182129,0.42386565208435056,0.4257476806640625,0.42403512001037597,0.409814453125,0.40780277252197267,0.4287348747253418,0.41240806579589845,0.42710151672363283,0.4182870388031006,0.4127157211303711,0.40867862701416013,0.40715646743774414,0.42292060852050783,0.41391763687133787,0.39282405376434326,0.4035985469818115,0.391954231262207,0.40638008117675783,0.33773341178894045,0.404130744934082,0.4062495708465576,0.4152361392974854,0.40601334571838377,0.420411491394043,0.43180322647094727,0.4053515911102295,0.39059650897979736,0.3992602348327637,0.29303817749023436,0.27087626457214353,0.30969820022583006,0.2787556409835815,0.2771109342575073,0.29126272201538084,0.31919939517974855,0.37400021553039553,0.27619969844818115,0.2676316022872925,0.258559513092041,0.2583328723907471,0.2555328607559204,0.25408072471618653,0.25745835304260256,0.25954740047454833,0.2634570121765137,0.25910518169403074,0.26260838508605955,0.25611166954040526,0.25798568725585935,0.25040359497070314,0.25217955112457274,0.2502096891403198,0.24632205963134765,0.24855775833129884,0.25124034881591795,0.24805045127868652,0.24584536552429198,0.24222412109375,0.2424482822418213,0.2442718744277954,0.24238295555114747,0.2494065284729004,0.24522628784179687,0.242482328414917,0.24103951454162598,0.2407756805419922,0.24032866954803467,0.23763501644134521,0.2389930248260498,0.2373893976211548,0.2355576515197754,0.2304818868637085,0.2390322208404541,0.23650798797607422,0.23040566444396973,0.2313764810562134,0.23083944320678712,0.22927372455596923,0.230364990234375,0.22969160079956055,0.2280714988708496,0.22830576896667482,0.22635502815246583,0.2316075563430786,0.23168387413024902,0.22559525966644287,0.23251423835754395,0.22806873321533203,0.227610445022583,0.2300246238708496,0.22879619598388673,0.22817449569702147,0.23413410186767578,0.22842779159545898,0.2287362575531006,0.22911474704742432,0.2296881914138794,0.22475748062133788,0.2231759548187256,0.2268745183944702,0.22119011878967285,0.22838106155395507,0.22722315788269043,0.23157191276550293,0.22212367057800292,0.22178926467895507,0.22977471351623535,0.2262038469314575,0.21779003143310546,0.2234290599822998,0.22241146564483644,0.21974515914916992,0.22595715522766113,0.22809348106384278,0.2213360071182251,0.22515788078308105,0.22400212287902832,0.22533872127532958,0.2235123872756958,0.2247699975967407,0.21641454696655274,0.2241593360900879,0.22091305255889893,0.2167044162750244,0.21966977119445802,0.2217503786087036,0.2287152051925659,0.22653627395629883,0.2147273302078247,0.2180556297302246,0.21503138542175293,0.22591547966003417,0.22333226203918458,0.22502241134643555,0.21879901885986328,0.21846609115600585,0.2158588409423828,0.2183436632156372,0.21645536422729492,0.21786484718322754,0.218996262550354,0.22774133682250977,0.21715645790100097,0.2238619327545166,0.2202770948410034,0.2147066593170166,0.21516139507293702,0.2207864046096802,0.2209246873855591,0.22091262340545653,0.21833105087280275,0.21978721618652344,0.22339122295379638,0.22372221946716309,0.22263479232788086,0.22530503273010255,0.22629122734069823,0.2123964548110962,0.22076520919799805,0.21701126098632811,0.2188345670700073,0.2079521656036377,0.2196505546569824,0.21894326210021972,0.21861250400543214,0.20899577140808107,0.22111945152282714,0.21805887222290038,0.2190110206604004,0.2119285821914673,0.2146683931350708,0.21238279342651367,0.22059991359710693,0.21718454360961914,0.21425445079803468,0.21597027778625488,0.20825800895690919,0.2102724552154541,0.21733779907226564,0.2163318157196045,0.21627910137176515,0.22310466766357423,0.21932780742645264,0.22145180702209472,0.21319432258605958,0.21991519927978515,0.21068389415740968,0.21678755283355713,0.2158756971359253,0.21393229961395263,0.21217846870422363,0.22906789779663086,0.21560730934143066,0.22276320457458496,0.20623087882995605,0.2177673578262329,0.21333153247833253,0.20498552322387695,0.208853816986084,0.21193723678588866,0.2104243278503418,0.2175349235534668,0.2140103816986084,0.21340389251708985,0.21525092124938966,0.20638744831085204,0.21889686584472656,0.20783576965332032,0.21067650318145753,0.21061453819274903,0.21281538009643555,0.20364718437194823,0.2140738248825073,0.20534584522247315,0.20629611015319824,0.21336822509765624,0.20829229354858397,0.21159570217132567,0.2076268196105957,0.2099837303161621,0.20304064750671386,0.21382393836975097,0.20877535343170167,0.20400533676147461,0.211568021774292,0.2166391372680664,0.21514739990234374,0.2106006145477295,0.2142261028289795,0.2170923948287964,0.21696534156799316,0.2177405834197998,0.2090909957885742,0.2104933738708496,0.21453118324279785,0.21189804077148439,0.2074263572692871,0.2074275016784668,0.20599727630615233,0.21329329013824463,0.21321656703948974,0.2132688045501709,0.21454176902770997,0.2055062770843506,0.2162010908126831,0.224603271484375,0.20521669387817382,0.21098880767822265,0.19749848842620848,0.20255322456359864,0.21421442031860352,0.21242170333862304,0.21263625621795654,0.21295623779296874,0.21141390800476073,0.21617438793182372,0.20928571224212647,0.1939399480819702,0.20643978118896483,0.20391554832458497,0.21365771293640137,0.20481209754943847,0.20449552536010743,0.21467161178588867,0.2004056453704834,0.2119502067565918,0.20410628318786622,0.20206964015960693,0.2147233486175537,0.21072072982788087,0.20313558578491211,0.21548840999603272,0.2042222499847412,0.21998021602630616,0.21652498245239257,0.2157524585723877,0.20722558498382568,0.21018950939178466,0.2087472915649414,0.2091740608215332,0.20618231296539308,0.20692830085754393,0.20669107437133788,0.20424134731292726,0.21895294189453124,0.20674071311950684,0.20738017559051514,0.2112133026123047,0.20726888179779052,0.21391716003417968,0.19632381200790405,0.211456298828125,0.2036160945892334,0.21266846656799315,0.2048410654067993,0.20669198036193848,0.21219654083251954,0.2041930675506592,0.203482985496521,0.2060627222061157,0.20643463134765624,0.2102285861968994,0.20696737766265869,0.19749670028686522,0.20902154445648194,0.20164875984191893,0.20369749069213866,0.21045494079589844,0.20717475414276124,0.19592416286468506,0.20782942771911622,0.21458420753479004,0.20731120109558104,0.21294894218444824,0.22528674602508544,0.21166720390319824,0.21035175323486327,0.20117106437683105,0.20459821224212646,0.19941717386245728,0.2051236391067505,0.19873019456863403,0.21180219650268556,0.21033236980438233,0.20302746295928956,0.2055217742919922,0.20726513862609863,0.20710530281066894,0.2160555124282837,0.20646195411682128,0.20219933986663818,0.20540056228637696,0.2034660816192627,0.20373635292053222,0.19334311485290528,0.21548221111297608,0.2069408416748047,0.19744921922683717,0.2016420841217041,0.21146070957183838,0.1989421248435974,0.20125818252563477,0.19854705333709716,0.20048489570617675,0.2147054672241211,0.2073061466217041,0.20490579605102538,0.20145761966705322,0.2076892852783203,0.19257326126098634,0.2167490005493164,0.20409631729125977,0.1997942566871643,0.21100120544433593,0.20445361137390136,0.19939782619476318,0.19834139347076415,0.20442800521850585,0.20195908546447755,0.2012300968170166,0.19936788082122803,0.2047201871871948,0.20204343795776367,0.21119217872619628,0.19618030786514282,0.2148287773132324,0.20359158515930176,0.19565234184265137,0.21102981567382811,0.2052760124206543,0.20495007038116456,0.2100666046142578,0.2039353609085083,0.20102534294128419,0.19678959846496583,0.201537561416626,0.20517916679382325,0.19961283206939698,0.19851499795913696,0.1957174062728882,0.2024698257446289,0.20585532188415528,0.20830750465393066,0.2080239772796631,0.20572233200073242,0.1983806610107422,0.20442404747009277,0.2029803991317749,0.20557839870452882,0.20818397998809815,0.19659626483917236,0.2045051336288452,0.20818541049957276,0.1993393898010254,0.21277542114257814,0.20622119903564454,0.21184260845184327,0.20424776077270507,0.20313329696655275,0.19993678331375123,0.20569775104522706,0.19183988571166993,0.1998342514038086,0.2089857816696167,0.20460133552551268,0.19419362545013427,0.19997856616973878,0.20715487003326416,0.2045151710510254,0.19515392780303956,0.20610175132751465,0.19309478998184204,0.19932308197021484,0.1999261498451233,0.20331261157989503,0.1936908483505249,0.1964107036590576,0.20918004512786864,0.19479902982711791,0.20475318431854247,0.21161623001098634,0.20186128616333007,0.1995617628097534,0.196844482421875,0.21012182235717775,0.2033473014831543,0.19296154975891114,0.2000990629196167,0.19317976236343384,0.20239274501800536,0.211379075050354,0.20850710868835448,0.20302331447601318,0.20290498733520507,0.19751262664794922,0.2021103858947754,0.20067949295043946,0.19131239652633666,0.20277223587036133,0.20152652263641357,0.20040631294250488,0.1933719038963318,0.2043851137161255,0.20441081523895263,0.20484764575958253,0.2104422330856323,0.20058858394622803,0.1999380350112915,0.20048530101776124,0.20636143684387206,0.2119212865829468,0.20277805328369142,0.20546083450317382,0.18667304515838623,0.1847979784011841,0.20763823986053467,0.205635929107666,0.21230788230895997,0.19081079959869385,0.19730511903762818,0.1976517081260681,0.20596506595611572,0.1949393033981323,0.19927210807800294,0.20697460174560547,0.2029432773590088,0.19670392274856568,0.20051951408386232,0.19345468282699585,0.20016469955444335,0.20305325984954833,0.2004401922225952,0.20245850086212158,0.20129671096801757,0.20481672286987304,0.2003694534301758,0.2041708469390869,0.20712287425994874,0.19099571704864501,0.2074512004852295,0.18445470333099365,0.195737886428833,0.19049856662750245,0.2034991979598999,0.20324535369873048,0.19761619567871094,0.20418918132781982,0.20253369808197022,0.20089998245239257,0.2016366243362427,0.20087060928344727,0.18918535709381104,0.21155781745910646,0.19407644271850585,0.20449819564819335,0.1999928593635559,0.19956167936325073,0.19934868812561035,0.20468506813049317,0.20059807300567628,0.21210980415344238,0.192214035987854,0.19885220527648925,0.19563124179840088,0.20088210105895996,0.19157356023788452,0.2076786518096924,0.19610750675201416,0.1931530475616455,0.1974818468093872,0.20161380767822265,0.19921988248825073,0.20060274600982667,0.20600008964538574,0.1960594654083252,0.2027642011642456,0.2072906017303467,0.20216338634490966,0.20210888385772705,0.21246657371520997,0.20504932403564452,0.21036210060119628,0.20445723533630372,0.19416791200637817,0.2048746109008789,0.19642086029052735,0.19944050312042236,0.1963056206703186,0.1943004846572876,0.19785635471343993,0.1929189920425415,0.1914217233657837,0.20069525241851807,0.19514528512954712,0.20522694587707518,0.202475643157959,0.1931448459625244,0.19981637001037597,0.19685046672821044,0.20325396060943604,0.19798195362091064,0.197275710105896,0.20745766162872314,0.20438055992126464,0.2019324541091919,0.19823297262191772,0.20563712120056152,0.19508254528045654,0.1923806071281433,0.19786876440048218,0.20516047477722169,0.20401308536529542,0.20029640197753906,0.19158895015716554,0.20081515312194825,0.20423226356506347,0.20360984802246093,0.21031718254089354,0.1927367329597473,0.1930437922477722,0.19785008430480958,0.1924727439880371,0.21000065803527831,0.19847962856292725,0.20534439086914064,0.1965547561645508,0.20736310482025147,0.2060615062713623,0.19716354608535766,0.1977844715118408,0.19903850555419922,0.20616798400878905,0.1925374984741211,0.20544874668121338,0.19131360054016114,0.20101747512817383,0.194101881980896,0.2062678337097168,0.211967134475708,0.19779715538024903,0.201326322555542,0.1921508193016052,0.19827853441238402,0.19675638675689697,0.19720933437347413,0.20322871208190918,0.19257848262786864,0.19770220518112183,0.19353264570236206,0.19831290245056152,0.1844748854637146,0.1998615860939026,0.19175217151641846,0.1958450436592102,0.1998591899871826,0.2039180278778076,0.19757909774780275,0.1989431381225586,0.19861148595809935,0.19510438442230224,0.20079946517944336,0.19644347429275513,0.19312515258789062,0.20498278141021728,0.20149035453796388,0.2011181354522705,0.1975322723388672,0.19575469493865966,0.19514405727386475,0.1992654323577881,0.19542815685272216,0.19276493787765503,0.19903154373168946,0.19070690870285034,0.18603959083557128,0.19458006620407103,0.19301581382751465,0.20773673057556152,0.19818161725997924,0.19055254459381105,0.19537192583084106,0.1994530439376831,0.18959949016571045,0.1933220148086548,0.19930803775787354,0.1948290228843689,0.19325047731399536,0.19429221153259277,0.18614537715911866,0.1935559630393982,0.20053772926330565,0.19487417936325074,0.19999769926071168,0.1998846411705017,0.19416508674621583,0.1981911540031433,0.20278005599975585,0.20180039405822753,0.198353374004364,0.18928725719451905,0.20983619689941407,0.1956994652748108,0.19965349435806273,0.19433168172836304,0.20404787063598634,0.19624556303024293,0.19443663358688354,0.1902600884437561,0.20798883438110352,0.19345251321792603,0.1928173542022705,0.19334378242492675,0.19304938316345216,0.19489285945892335,0.1947220206260681,0.19868576526641846,0.1974566698074341,0.19150543212890625,0.19554588794708253,0.18816863298416137,0.1886945366859436,0.2008517026901245,0.19209891557693481,0.19001989364624022,0.1900429368019104,0.19321005344390868,0.20000863075256348,0.1983358383178711,0.20886874198913574,0.19213979244232177,0.19071415662765503,0.19991066455841064,0.19441559314727783,0.2031914234161377,0.19072195291519164,0.1948647379875183,0.18608520030975342,0.18840384483337402,0.18963632583618165,0.1935965299606323,0.19641218185424805,0.1894511580467224,0.19254815578460693,0.1929314136505127,0.19599997997283936,0.20646188259124756,0.20160889625549316,0.19836084842681884,0.19629297256469727,0.2036578893661499,0.19437594413757325,0.19569272994995118,0.20016274452209473,0.19035085439682006,0.19895687103271484,0.203515625,0.19363436698913575,0.20090980529785157,0.18309404850006103,0.1855332612991333,0.19701074361801146,0.19085590839385985,0.1886514186859131,0.19973074197769164,0.2024394989013672,0.1957775354385376,0.20317497253417968,0.19115447998046875,0.18591482639312745,0.1957536220550537,0.2041919469833374,0.20529236793518066,0.19364097118377685,0.18593182563781738,0.20163383483886718,0.18447749614715575,0.1981884241104126,0.1991422653198242,0.1999862313270569,0.1914340615272522,0.1946328639984131,0.20204625129699708,0.19413019418716432,0.19885964393615724,0.19414674043655394,0.17304999828338624,0.2015134334564209,0.19172247648239135,0.1954284429550171,0.1946331739425659,0.19186315536499024,0.18873188495635987,0.1919800877571106,0.19226192235946654,0.19814761877059936,0.19752602577209472,0.1914414644241333,0.20848169326782226,0.20504581928253174,0.19615434408187865,0.19639134407043457,0.1905620813369751,0.19653208255767823,0.19527652263641357,0.19107601642608643,0.19448734521865846,0.194197416305542,0.20407321453094482,0.18789350986480713,0.19862515926361085,0.2018826961517334,0.18816812038421632,0.19523943662643434,0.18551304340362548,0.1839683771133423,0.20027694702148438,0.19439260959625243,0.18380941152572633,0.19769054651260376,0.1981992244720459,0.1949591279029846,0.1946107864379883,0.19908437728881836,0.19187253713607788,0.187959623336792,0.1958545446395874,0.18760511875152588,0.19289711713790894,0.20014028549194335,0.19286850690841675,0.18886117935180663,0.19140000343322755,0.20253190994262696,0.21126937866210938,0.20129632949829102,0.1921631932258606,0.19202765226364135,0.18909574747085572,0.19797213077545167,0.18401433229446412,0.19264354705810546,0.19823384284973145,0.1947126030921936,0.18744693994522094,0.19572372436523439,0.19913787841796876,0.18712892532348632,0.19958531856536865,0.19787405729293822,0.1902519702911377,0.19545284509658814,0.17644435167312622,0.19906708002090454,0.1980334758758545,0.1960880994796753,0.19493527412414552,0.20040466785430908,0.19091237783432008,0.19023325443267822,0.1901930809020996,0.18881441354751588,0.19358291625976562,0.19507105350494386,0.1960777997970581,0.20109095573425292,0.19853606224060058,0.1905590057373047,0.19792659282684327,0.18365054130554198,0.19011125564575196,0.19003535509109498,0.20450091361999512,0.19346753358840943,0.1941441774368286,0.19264867305755615,0.2000293254852295,0.1893429636955261,0.2027735471725464,0.19327367544174195,0.19515597820281982,0.195579195022583,0.1832182765007019,0.20088026523590088,0.18992680311203003,0.19963433742523193,0.19670498371124268,0.1905229926109314,0.19335901737213135,0.1940913677215576,0.186016845703125,0.18902961015701295,0.19282932281494142,0.1885256290435791,0.19417515993118287,0.19428679943084717,0.19241654872894287,0.1861474633216858,0.1937701940536499,0.20112459659576415,0.19349656105041504,0.18587516546249389,0.20143816471099854,0.20165295600891114,0.1940784454345703,0.19740380048751832,0.19866539239883424,0.18576200008392335,0.195076060295105,0.1981372356414795,0.1873654007911682,0.19489105939865112,0.20068588256835937,0.19585856199264526,0.20022571086883545,0.19115058183670045,0.19703948497772217,0.18984715938568114,0.19235328435897828,0.19127154350280762,0.18517638444900514,0.19068411588668824,0.19119720458984374,0.193342125415802,0.17505781650543212,0.1879286527633667,0.20677525997161866,0.18242194652557372,0.1847759008407593,0.19195505380630493,0.19215734004974366,0.19433223009109496,0.1923051118850708,0.19415977001190185,0.19091731309890747,0.19457993507385254,0.19075489044189453,0.19407085180282593,0.2056811809539795,0.19122121334075928,0.1915663480758667,0.19497549533843994,0.19193823337554933,0.19187089204788207,0.18844070434570312,0.19068510532379152,0.18747713565826415,0.18804759979248048,0.20368647575378418,0.19080605506896972,0.19481399059295654,0.19544998407363892,0.18627945184707642,0.18803749084472657,0.18959760665893555,0.18765225410461425,0.19643431901931763,0.18422676324844361,0.19047471284866332,0.1958600878715515,0.19343278408050538,0.1879420042037964,0.1990064024925232,0.1964555025100708,0.19592711925506592,0.1904914617538452,0.1906428575515747,0.20674171447753906,0.1907862424850464,0.1855902075767517,0.19398908615112304,0.18679691553115846,0.1859784483909607,0.18098688125610352,0.19149576425552367,0.19862241744995118,0.18518412113189697,0.18812849521636962,0.18040745258331298,0.18661856651306152,0.17977899312973022,0.19104622602462767,0.19315361976623535,0.1935688853263855,0.2004091262817383,0.18703371286392212,0.18744964599609376,0.1948569655418396,0.18674297332763673,0.20312693119049072,0.18006365299224852,0.18854713439941406,0.18565409183502196,0.19302897453308104,0.18890421390533446,0.19584094285964965,0.19474368095397948,0.18785183429718016,0.19328052997589112,0.19102001190185547,0.1991182804107666,0.19469823837280273,0.19186174869537354,0.1970914602279663,0.18907530307769777,0.2071887731552124,0.19618698358535766,0.18361055850982666,0.18822580575942993,0.1945866346359253,0.1887156128883362,0.19037895202636718,0.18688950538635254,0.20141181945800782,0.1923201322555542,0.19777448177337648,0.1868907928466797,0.1950671434402466,0.18509337902069092,0.1839916229248047,0.1990511655807495,0.18682878017425536,0.18504016399383544,0.18567280769348143,0.1937696099281311,0.19198336601257324,0.1860060691833496,0.2035001039505005,0.1885662078857422,0.19480605125427247,0.18062344789505005,0.18240222930908204,0.190171217918396,0.18876926898956298,0.18970715999603271,0.19879404306411744,0.18784782886505128,0.18313708305358886,0.18097314834594727,0.19220058917999266,0.20518794059753417,0.19219391345977782,0.187156343460083,0.19419305324554442,0.1848728060722351,0.19762864112854003,0.18447487354278563,0.18822081089019777,0.1926006555557251,0.18635498285293578,0.18374725580215454,0.1898963212966919,0.19050629138946534,0.19419437646865845,0.1811991810798645,0.19238853454589844,0.1902194619178772,0.18212153911590576,0.19259450435638428,0.1855006694793701,0.1892040491104126,0.1876811385154724,0.18222999572753906,0.19586579799652098,0.184627902507782,0.19295623302459716,0.1889609694480896,0.19168410301208497,0.189089035987854,0.1877972364425659,0.1902660012245178,0.19972983598709107,0.1910731315612793,0.17984046936035156,0.19462575912475585,0.18495407104492187,0.19660626649856566,0.18262327909469606,0.19759193658828736,0.1959136724472046,0.18963449001312255,0.18932205438613892,0.18437954187393188,0.1853501796722412,0.18724555969238282,0.19463005065917968,0.18604848384857178,0.19199683666229247,0.19064488410949706,0.1944440245628357,0.19606803655624389,0.1933140277862549,0.18645153045654297,0.19133336544036866,0.18312137126922606,0.18329141139984131,0.19038708209991456,0.19813172817230223,0.18902876377105712,0.19384317398071288,0.1896143674850464,0.19061810970306398,0.19703505039215088,0.1978748321533203,0.20220706462860108,0.18230630159378053,0.189117431640625,0.1865769863128662,0.2000974655151367,0.1838059663772583,0.1874657988548279,0.19536036252975464,0.1889534115791321,0.19813326597213746,0.18575360774993896,0.196859610080719,0.18108618259429932,0.2080465316772461,0.1876198649406433,0.19251861572265624,0.19095443487167357,0.19614675045013427,0.18444905281066895,0.1864572286605835,0.18827877044677735,0.18853600025177003,0.19793355464935303,0.1844814419746399,0.18785613775253296,0.19954195022583007,0.19082536697387695,0.19190027713775634,0.18271448612213134,0.18973991870880128,0.1988466501235962,0.1985432982444763,0.1817670226097107,0.18684561252593995,0.18690958023071289,0.19931623935699463,0.19186863899230958,0.18976060152053834,0.19056590795516967,0.1904480814933777,0.19080634117126466,0.18834395408630372,0.1913995623588562,0.18819746971130372,0.1894243597984314,0.20184526443481446,0.18681734800338745,0.18280134201049805,0.18416855335235596,0.211956787109375,0.19372140169143676,0.18215572834014893,0.18917289972305298,0.19327731132507325,0.18943904638290404,0.19816583395004272,0.1988346815109253,0.1870051622390747,0.19570283889770507,0.18998229503631592,0.19167324304580688,0.19689228534698486,0.19705283641815186,0.18828614950180053,0.1902853488922119,0.18294684886932372,0.1969913125038147,0.17866581678390503,0.18761100769042968,0.18820916414260863,0.1925899028778076,0.1986583948135376,0.19587627649307252,0.18876965045928956,0.1892542839050293,0.19338278770446776,0.19631543159484863,0.18064924478530883,0.19043323993682862,0.18569177389144897,0.18213093280792236,0.18904467821121215,0.20210683345794678,0.19606537818908693,0.1902734637260437,0.18194108009338378,0.19575493335723876,0.19160716533660888,0.1865708351135254,0.20013906955718994,0.1960087776184082,0.19183365106582642,0.18324017524719238,0.18214380741119385,0.18863085508346558,0.18900775909423828,0.19859509468078612,0.1868217706680298,0.18596243858337402,0.18659743070602416,0.19057066440582277,0.20031442642211914,0.19122933149337767,0.19483957290649415,0.18425990343093873,0.18814637660980224,0.1780741333961487,0.18903330564498902,0.18417301177978515,0.19060643911361694,0.18822195529937744,0.19705471992492676,0.19612449407577515,0.18797714710235597,0.20006799697875977,0.18090896606445311,0.18902581930160522,0.18431476354599,0.18912917375564575,0.1857316017150879,0.18883233070373534,0.18411074876785277,0.18935678005218506,0.18432457447052003,0.1843705415725708,0.17646371126174926,0.1878219723701477,0.19384130239486694,0.18808059692382811,0.1871943712234497,0.187711501121521,0.18061373233795167,0.1869894027709961,0.18992589712142943,0.18601670265197753,0.18038733005523683,0.1948242664337158,0.1828610897064209,0.18277947902679442,0.19231213331222535,0.18973088264465332,0.1849766969680786,0.20318374633789063,0.1953378438949585,0.18879477977752684,0.18854973316192628,0.19135843515396117,0.18887269496917725,0.18770380020141603,0.1789594769477844,0.20160622596740724,0.18373603820800782,0.19087443351745606,0.19163334369659424,0.192450213432312,0.1725314736366272,0.19912681579589844,0.19283268451690674,0.1887439489364624,0.19075446128845214,0.1841362714767456,0.19239667654037476,0.19573805332183838,0.18899900913238527,0.19801836013793944,0.1865408182144165,0.18641934394836426,0.19474010467529296,0.19626777172088622,0.18088235855102539,0.18199973106384276,0.1859795331954956,0.19078330993652343,0.18551827669143678,0.18476898670196534,0.1900165557861328,0.18863011598587037,0.18256494998931885,0.19173967838287354,0.1994718551635742,0.19709923267364501,0.19441792964935303,0.19863897562026978,0.18315501213073732,0.1901706337928772,0.1831955909729004,0.1856669306755066,0.19097284078598023,0.1996564030647278,0.1902230978012085,0.183960485458374,0.18866961002349852,0.18546221256256104,0.1795608162879944,0.18765697479248047,0.18278126716613768,0.18902511596679689,0.1847292184829712,0.1932140588760376,0.1779460072517395,0.18646117448806762,0.1911670446395874,0.1871255397796631,0.18933844566345215,0.18933228254318238,0.18504657745361328,0.18959124088287355,0.19258558750152588,0.19577831029891968,0.19261033535003663,0.19241673946380616,0.20038769245147706,0.18708174228668212,0.19550203084945678,0.1877748966217041,0.17821799516677855,0.1962919592857361,0.20103914737701417,0.18658113479614258,0.18453571796417237,0.18111846446990967,0.19315617084503173,0.17388219833374025,0.18822563886642457,0.19045472145080566,0.19008225202560425,0.18745315074920654,0.19636270999908448,0.18674168586730958,0.1885419726371765,0.1728874921798706,0.19395675659179687,0.1949772834777832,0.19052212238311766,0.18773754835128784,0.18925182819366454,0.1857440233230591,0.18655312061309814,0.1905975580215454,0.19264190196990966,0.19101295471191407,0.1901383399963379,0.18163321018218995,0.18110487461090088,0.1837836980819702,0.19438450336456298,0.18093103170394897,0.18509681224823,0.19099340438842774,0.18844760656356813,0.18859864473342897,0.1896498680114746,0.18638129234313966,0.18856561183929443,0.18840723037719725,0.1752310037612915,0.18233286142349242,0.18750237226486205,0.1972159266471863,0.17177882194519042,0.18653473854064942,0.18772604465484619,0.19208252429962158,0.18971476554870606,0.18641749620437623,0.17620428800582885,0.1713247060775757,0.19296342134475708,0.18489229679107666,0.1831640124320984,0.1790199637413025,0.18934404850006104,0.1808112382888794,0.18814326524734498,0.1883924961090088,0.17868270874023437,0.20205130577087402,0.1898528218269348,0.18108770847320557,0.19005331993103028,0.1918126344680786,0.19349452257156372,0.18871452808380126,0.19184093475341796,0.1903048872947693,0.19187791347503663,0.1908268451690674,0.19343054294586182,0.1922778844833374,0.1923628568649292,0.19914600849151612,0.18611865043640136,0.19330284595489503,0.18977776765823365,0.18419713973999025,0.18411495685577392,0.19056981801986694,0.191732656955719,0.17555079460144044,0.19461880922317504,0.18858790397644043,0.19571399688720703,0.18424475193023682,0.1897961139678955,0.1913512945175171,0.17562215328216552,0.1853451609611511,0.1895308256149292,0.19193747043609619,0.18915504217147827,0.18567944765090943,0.19186022281646728,0.1858907461166382,0.1800938844680786,0.18939621448516847,0.18503018617630004,0.179851496219635,0.18613411188125611,0.19171537160873414,0.1814652919769287,0.18504366874694825,0.18887494802474974,0.1796482563018799,0.18484644889831542,0.18259172439575194,0.19184412956237792,0.19556050300598143,0.1893485426902771,0.17816792726516723,0.19134376049041749,0.19007580280303954,0.19284688234329223,0.18516161441802978,0.19526896476745606,0.18937368392944337,0.19518510103225709,0.18399782180786134,0.1924704909324646,0.1900960922241211,0.18995784521102904,0.17632031440734863,0.19690303802490233,0.1958639144897461,0.1883082389831543,0.18488757610321044,0.203428316116333,0.17944808006286622,0.19850484132766724,0.182437002658844,0.19544787406921388,0.19482287168502807,0.18289588689804076,0.1736685037612915,0.1867527723312378,0.17573175430297852,0.191566801071167,0.18749287128448486,0.18135735988616944,0.19208018779754638,0.17919517755508424,0.17637377977371216,0.19043498039245604,0.19086108207702637,0.18291594982147216,0.18762121200561524,0.17222247123718262,0.18165161609649658,0.18245817422866822,0.18063424825668334,0.18592606782913207,0.19746366739273072,0.1874117374420166,0.1780034065246582,0.17785660028457642,0.184067702293396,0.19127880334854125,0.19496084451675416,0.18197357654571533,0.18266901969909669,0.19343862533569336,0.18643548488616943,0.19520004987716674,0.18550053834915162,0.19303996562957765,0.19443747997283936,0.1840900182723999,0.19282596111297606,0.18545421361923217,0.19004007577896118,0.18276968002319335,0.18675369024276733,0.18764989376068114,0.1917865037918091,0.1849888324737549,0.18660980463027954,0.18154006004333495,0.18534526824951172,0.19324506521224977,0.1896619439125061,0.1925769567489624,0.1797875165939331,0.17948540449142455,0.18316012620925903,0.1939818501472473,0.18426952362060547,0.1731462597846985,0.18466176986694335,0.18823630809783937,0.17724475860595704,0.18145420551300048,0.18740519285202026,0.18127559423446654,0.1758375883102417,0.1900106906890869,0.18583062887191773,0.17640745639801025,0.18784611225128173,0.19005494117736815,0.17676750421524048,0.19008357524871827,0.19448858499526978,0.1877837896347046,0.18673715591430665,0.20215771198272706,0.19493669271469116,0.1863858699798584,0.19394668340682983,0.18046720027923585,0.18351422548294066,0.18440301418304444,0.18421350717544555,0.1875375747680664,0.18658549785614015,0.1806769847869873,0.18529057502746582,0.17174134254455567,0.17995078563690187,0.18252483606338502,0.17664096355438233,0.1879543900489807,0.19004613161087036,0.1854802131652832,0.19233380556106566,0.19196261167526246,0.17831428050994874,0.1852929711341858,0.1771453619003296,0.18728232383728027,0.18936045169830323,0.1859904408454895,0.18881402015686036,0.18106043338775635,0.17697978019714355,0.18815107345581056,0.1805996775627136,0.18012409210205077,0.19105169773101807,0.18107752799987792,0.1795165419578552,0.1824905276298523,0.18681750297546387,0.18959711790084838,0.1802138090133667,0.1821271538734436,0.1900946617126465,0.18181223869323732,0.18849481344223024,0.18394224643707274,0.1925663709640503,0.19105349779129027,0.18617722988128663,0.1960350513458252,0.1894338011741638,0.18425614833831788,0.18743486404418946,0.18432848453521727,0.18144100904464722,0.1934720754623413,0.17680450677871704,0.19236431121826172,0.18416304588317872,0.18227753639221192,0.18774261474609374,0.18144034147262572,0.17928026914596557,0.18247613906860352,0.1852115273475647,0.1880277395248413,0.18318818807601928,0.1804811716079712,0.1750023603439331,0.19993773698806763,0.18780549764633178,0.18256621360778807,0.1883608102798462,0.1856680393218994,0.18594152927398683,0.2005544662475586,0.18127535581588744,0.19084577560424804,0.18821771144866944,0.18375611305236816,0.1773830771446228,0.1923828601837158,0.18103479146957396,0.1908484935760498,0.18278827667236328,0.1920325994491577,0.185824716091156,0.18335328102111817,0.1920370101928711,0.19163818359375,0.1886705279350281,0.1802193522453308,0.1779545783996582,0.18895175457000732,0.18172574043273926,0.1744930624961853,0.18162596225738525,0.1818521499633789,0.1798145055770874,0.19066915512084961,0.19064933061599731,0.18818154335021972,0.17717547416687013,0.19000422954559326,0.18719942569732667,0.18200864791870117,0.1835850715637207,0.18138413429260253,0.18881986141204835,0.19192228317260743,0.19086155891418458,0.1893524169921875,0.1826571822166443,0.17462102174758912,0.1816248655319214,0.1777702808380127,0.17099466323852539,0.18532047271728516,0.1912978172302246,0.19678153991699218,0.18434460163116456,0.1844605565071106,0.19277786016464232,0.18376128673553466,0.18374619483947754,0.18716953992843627,0.17954642772674562,0.1886378526687622,0.19169423580169678,0.18858641386032104,0.17548208236694335,0.17961647510528564,0.1832528829574585,0.19177223443984986,0.18011050224304198,0.18146998882293702,0.18656250238418579,0.18041133880615234,0.19293540716171265,0.18905301094055177,0.19721503257751466,0.17994604110717774,0.18331122398376465,0.1857462167739868,0.18869733810424805,0.17457571029663085,0.18363094329833984,0.19028197526931762,0.18032658100128174,0.17887344360351562,0.18475897312164308,0.19258267879486085,0.18489856719970704,0.18549211025238038,0.17300357818603515,0.17338709831237792,0.1709247350692749,0.18481839895248414,0.18461425304412843,0.184143328666687,0.18208184242248535,0.18793079853057862,0.17495543956756593,0.19182039499282838,0.18704440593719482,0.19249637126922609,0.17713184356689454,0.18351035118103026,0.19537837505340577,0.190167236328125,0.1848214864730835,0.18911818265914918,0.1793039321899414,0.18344277143478394,0.19056324958801268,0.18725521564483644,0.17917174100875854,0.18519725799560546,0.18786426782608032,0.17475652694702148,0.18124920129776,0.19170689582824707,0.17506619691848754,0.18850271701812743,0.19122947454452516,0.18282883167266845,0.18205748796463012,0.18582011461257936,0.18292127847671508,0.18587223291397095,0.19393154382705688,0.19322999715805053,0.172284996509552,0.18554402589797975,0.18542890548706054,0.18374831676483155,0.18365199565887452,0.17836902141571045,0.18791836500167847,0.18216297626495362,0.1730739712715149,0.1845123291015625,0.18501584529876708,0.1831119418144226,0.19063137769699096,0.17682697772979736,0.18667590618133545,0.19304187297821046,0.1882842779159546,0.18160314559936525,0.1812973737716675,0.1870335817337036,0.18177762031555175,0.19076573848724365,0.19008591175079345,0.18430068492889404,0.18325822353363036,0.18897591829299926,0.17720974683761598,0.17952170372009277,0.18418891429901124,0.1949160575866699,0.18034088611602783,0.1837223529815674,0.17924555540084838,0.1798431396484375,0.18757661581039428,0.1872208833694458,0.17089307308197021,0.18589913845062256,0.17842502593994142,0.1823885440826416,0.1805631399154663,0.1846035122871399,0.19196438789367676,0.18856565952301024,0.1889070987701416,0.16856787204742432,0.1841019868850708,0.18192483186721803,0.19040557146072387,0.17956342697143554,0.17732641696929932,0.1884310483932495,0.18238451480865478,0.18670885562896727,0.19065710306167602,0.1888371706008911,0.17937836647033692,0.17514073848724365,0.18206546306610108,0.18363274335861207,0.19146389961242677,0.18439915180206298,0.18703920841217042,0.18599849939346313,0.1702549934387207,0.1859745502471924,0.18817732334136963,0.1870880126953125,0.1789824366569519,0.18331139087677,0.18637566566467284,0.17745707035064698,0.18543605804443358,0.18756706714630128,0.19019612073898315,0.18358525037765502,0.18708199262619019,0.17946656942367553,0.19685890674591064,0.17843146324157716,0.17808175086975098,0.1848926305770874,0.18754911422729492,0.1726696014404297,0.1781598687171936,0.185870361328125,0.19067437648773194,0.18552641868591307,0.18308253288269044,0.1953666925430298,0.18136193752288818,0.18516509532928466,0.19007520675659179,0.1709052085876465,0.17805598974227904,0.17947400808334352,0.17238202095031738,0.18335577249526977,0.1771608591079712,0.18080215454101561,0.18304342031478882,0.1859333872795105,0.17545217275619507,0.18737919330596925,0.18827998638153076,0.18558104038238527,0.17824841737747193,0.17048416137695313,0.18510940074920654,0.18237700462341308,0.19055111408233644,0.1936400055885315,0.19834380149841307,0.17601560354232787,0.18033019304275513,0.1909247875213623,0.1886054277420044,0.18644435405731202,0.17830591201782225,0.18398510217666625,0.1882023572921753,0.1835003137588501,0.18116202354431152,0.18158409595489503,0.17751083374023438,0.18381068706512452,0.17680692672729492,0.1870686411857605,0.18373783826828002,0.1960826635360718,0.18550539016723633,0.1876484751701355,0.18074848651885986,0.1856091260910034,0.1888718843460083,0.18998771905899048,0.18933649063110353,0.18123412132263184,0.18325672149658204,0.19378950595855712,0.17823792695999147,0.17096357345581054,0.18605368137359618,0.18477630615234375,0.18047114610671997,0.1790841817855835,0.19717836380004883,0.19626612663269044,0.182697069644928,0.18578245639801025,0.1854069471359253,0.17452189922332764,0.18520405292510986,0.18243610858917236,0.19754631519317628,0.1763124942779541,0.1933006763458252,0.18420621156692504,0.18059735298156737,0.18039950132369995,0.18059961795806884,0.18248434066772462,0.18368737697601317,0.18409881591796876,0.1858273983001709,0.1859089493751526,0.17942320108413695,0.1734817147254944,0.182240629196167,0.18171290159225464,0.1734939455986023,0.18022146224975585,0.17110660076141357,0.1912163496017456,0.1864238500595093,0.18858706951141357,0.18471364974975585,0.17819600105285643,0.18782689571380615,0.1807623863220215,0.18035004138946534,0.17957693338394165,0.181612765789032,0.19124783277511598,0.1799294114112854,0.18480614423751832,0.18938593864440917,0.17611286640167237,0.17582077980041505,0.1759633779525757,0.1852179765701294,0.17573695182800292,0.18567516803741455,0.18805878162384032,0.18275398015975952,0.17509028911590577,0.18231499195098877,0.17723429203033447,0.17942579984664916,0.18335872888565063,0.18316609859466554,0.18840270042419432,0.1836969017982483,0.18665752410888672,0.18298509120941162,0.18114475011825562,0.18339216709136963,0.18513692617416383,0.18562458753585814,0.18391455411911012,0.1898320198059082,0.1831254243850708,0.17296630144119263,0.18160910606384278,0.18043482303619385,0.17355570793151856,0.17344247102737426,0.18750250339508057,0.17323684692382812,0.18899213075637816,0.18673830032348632,0.18287949562072753,0.18869987726211548,0.1886082410812378,0.18339918851852416,0.17963309288024903,0.17746754884719848,0.19807995557785035,0.17887879610061647,0.19738521575927734,0.17853256464004516,0.18478708267211913,0.18038440942764283,0.18278372287750244,0.18266459703445434,0.18372949361801147,0.1825825810432434,0.18304455280303955,0.1873852252960205,0.18690105676651,0.18813495635986327,0.18847320079803467,0.17548918724060059,0.1974676012992859,0.18151018619537354,0.1904898166656494,0.17790403366088867,0.19219491481781006,0.17423036098480224,0.1878686785697937,0.188607120513916,0.18550362586975097,0.18424333333969117,0.1839332938194275,0.1840045213699341,0.18750884532928466,0.18039032220840454,0.17494728565216064,0.18478983640670776,0.19639776945114135,0.17959458827972413,0.19515109062194824,0.1857674837112427,0.1731201410293579,0.18034851551055908,0.1851269483566284,0.1840265154838562,0.1849273443222046,0.18202309608459472,0.19720816612243652,0.1824164628982544,0.17725199460983276,0.18071850538253784,0.1723224401473999,0.18493763208389283,0.1817629098892212,0.18157312870025635,0.18975436687469482,0.1942375898361206,0.1866755485534668,0.18073787689208984,0.17112151384353638,0.1897801160812378,0.17399702072143555,0.18357393741607667,0.18632479906082153,0.19358503818511963,0.19027104377746581,0.17545716762542723,0.1831714153289795,0.1913552165031433,0.17595163583755494,0.18167877197265625,0.17757548093795777,0.17518584728240966,0.17890090942382814,0.18257250785827636,0.16530705690383912,0.18075187206268312,0.17259812355041504,0.18696272373199463,0.18055087327957153,0.1738749384880066,0.18120732307434081,0.19025135040283203,0.179009747505188,0.1783603310585022,0.19321935176849364,0.17777066230773925,0.18691608905792237,0.18553041219711303,0.1819726824760437,0.17740957736968993,0.17631062269210815,0.1813734769821167,0.1834062933921814,0.18713243007659913]};

  var data = [data0];
  var layout = {"title":"loss by time"};

  Plotly.plot('plot-1350734249', data, layout);
})();
});
      
</script>
</div>

</div>

<div class="output_area">
<div class="prompt output_prompt">Out[12]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">random</span>: <span class="ansi-green-fg">Random</span> = scala.util.Random@52f81f44
<span class="ansi-cyan-fg">MiniBatchSize</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">256</span>
<span class="ansi-cyan-fg">NumberOfClasses</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">10</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">trainData</span>
<span class="ansi-cyan-fg">lossSeq</span>: <span class="ansi-green-fg">IndexedSeq</span>[<span class="ansi-green-fg">Double</span>] = <span class="ansi-yellow-fg">Vector</span>(
  <span class="ansi-green-fg">0.25606164932250974</span>,
<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">plot</span>: <span class="ansi-green-fg">Seq</span>[<span class="ansi-green-fg">Scatter</span>] = <span class="ansi-yellow-fg">List</span>(
  <span class="ansi-yellow-fg">Scatter</span>(
<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">res11_7</span>: <span class="ansi-green-fg">String</span> = <span class="ansi-green-fg">&#34;plot-1350734249&#34;</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Read-and-process-the-test-set">Read and process the test set<a class="anchor-link" href="#Read-and-process-the-test-set">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Like <a href="https://thoughtworksinc.github.io/DeepLearning.scala/demo/MiniBatchGradientDescent.html">the previous article</a> read and process images and label information of the test set from CIFAR10 database</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">testNDArray</span> <span class="k">=</span>
  <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="n">readFromResource</span><span class="o">(</span><span class="s">&quot;/cifar-10-batches-bin/test_batch.bin&quot;</span><span class="o">,</span> <span class="mi">100</span><span class="o">)</span>

<span class="k">val</span> <span class="n">testData</span> <span class="k">=</span> <span class="n">testNDArray</span><span class="o">.</span><span class="n">head</span>

<span class="k">val</span> <span class="n">testExpectResult</span> <span class="k">=</span> <span class="n">testNDArray</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>

<span class="k">val</span> <span class="n">vectorizedTestExpectResult</span> <span class="k">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">makeVectorized</span><span class="o">(</span><span class="n">testExpectResult</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[13]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">testNDArray</span>: <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">HNil</span> = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">testData</span>: <span class="ansi-green-fg">INDArray</span> = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">testExpectResult</span>: <span class="ansi-green-fg">INDArray</span> = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">vectorizedTestExpectResult</span>: <span class="ansi-green-fg">INDArray</span> = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],
<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Verify-the-prediction-accuracy-of-the-neural-network">Verify the prediction accuracy of the neural network<a class="anchor-link" href="#Verify-the-prediction-accuracy-of-the-neural-network">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Like the last article, we use the test data to verify the prediction result of the neural network and compute the accuracy. This accuracy shall increase to about 51%.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">getAccuracy</span><span class="o">(</span><span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">testData</span><span class="o">),</span> <span class="n">testExpectResult</span><span class="o">)</span>
<span class="n">println</span><span class="o">(</span><span class="s">s&quot;the result is </span><span class="si">$right</span><span class="s"> %&quot;</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>the result is 51.0 %
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[14]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">right</span>: <span class="ansi-green-fg">Double</span> = <span class="ansi-green-fg">51.0</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summery">Summery<a class="anchor-link" href="#Summery">&#182;</a></h2><p>In this article, we have learned the follows:</p>
<ul>
<li>Parameter tuning</li>
<li>L2Regularization</li>
<li>Relu</li>
<li>Build a two-layer neural network</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/TwoLayerNet.scala">Complete code</a></p>

</div>
</div>
</div>
 

