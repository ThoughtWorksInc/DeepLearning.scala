{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In this article, we will use [softmax](https://en.wikipedia.org/wiki/Softmax_function) classifier to build a simple image classification neural network with an accuracy of 32%. In a Softmax classifier, binary logic is generalized and regressed to multiple logic. Softmax classifier will output the probability of the corresponding category.\n",
    "\n",
    "We will first define a softmax classifier, then use the training set of [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) to train the neural network, and finally use the test set to verify the accuracy of the neural network.\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous course [GettingStarted](https://thoughtworksinc.github.io/DeepLearning.scala/demo/GettingStarted.html), we need to introduce each class of DeepLearning.scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                            \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.ExecutionContext.Implicits.global\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.plugins.Builtins\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.feature.Factory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.future._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.Await\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.duration.Duration\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.each.Monadic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscalaz.std.stream._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.nd4j::nd4s:0.8.0`\n",
    "import $ivy.`org.nd4j:nd4j-native-platform:0.8.0`\n",
    "import $ivy.`com.chuusai::shapeless:2.3.2`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::plugins-builtins:2.0.0`\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.2`\n",
    "import $ivy.`com.thoughtworks.each::each:3.3.1`\n",
    "import $plugin.$ivy.`org.scalamacros:paradise_2.11.11:2.1.0`\n",
    "\n",
    "import scala.concurrent.ExecutionContext.Implicits.global\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import com.thoughtworks.deeplearning.plugins.Builtins\n",
    "import com.thoughtworks.feature.Factory\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "import com.thoughtworks.future._\n",
    "import scala.concurrent.Await\n",
    "import scala.concurrent.duration.Duration\n",
    "import com.thoughtworks.each.Monadic._\n",
    "import scalaz.std.stream._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the line numbers outputted by `jupyter-scala` and to make sure that the page output will not be too long, we need to set `pprintConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprintConfig() = pprintConfig().copy(height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate need to be set for the full connection layer. Learning rate visually describes the change rate of `weight`. A too-low learning rate will result in slow decrease of `loss`, which will require longer time for training; A too-high learning rate will result in rapid decrease of `loss` at first while fluctuation around the lowest point afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mINDArrayLearningRatePluginUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"https://gist.githubusercontent.com/TerrorJack/118487016d7973d67feb489449dee156/raw/778bb1b68a664c752b0945111220326731310214/INDArrayLearningRate.sc\"\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val INDArrayLearningRatePluginUrl = \"https://gist.githubusercontent.com/TerrorJack/118487016d7973d67feb489449dee156/raw/778bb1b68a664c752b0945111220326731310214/INDArrayLearningRate.sc\"\n",
    "interp.load(scala.io.Source.fromURL(new java.net.URL(INDArrayLearningRatePluginUrl)).mkString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// `interp.load` is a workaround for https://github.com/lihaoyi/Ammonite/issues/649 and https://github.com/scala/bug/issues/10390\n",
    "interp.load(\"\"\"\n",
    "  val hyperparameters = Factory[Builtins with INDArrayLearningRate].newInstance(learningRate = 0.1)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `softmax` classifier (softmax classifier is a neural network combined by `softmax` and a full connection), we first need to write softmax function, formula: ![](https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.implicits._\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.INDArrayLayer\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.INDArrayLayer\n",
    "\n",
    "def softmax(scores: INDArrayLayer): INDArrayLayer = {\n",
    "  val expScores = hyperparameters.exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose your  neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a full connection layer and [initialize Weight](https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization), `Weight` shall be a two-dimension `INDArray` of `NumberOfPixels × NumberOfClasses`. `scores` is the score of each image corresponding to each category, representing the feasible probability of each category corresponding to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mNumberOfPixels\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3072\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "val NumberOfPixels: Int = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.INDArrayWeight\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mweight\u001b[39m: \u001b[32mObject\u001b[39m with \u001b[32mhyperparameters\u001b[39m.\u001b[32mINDArrayWeightApi\u001b[39m with \u001b[32mhyperparameters\u001b[39m.\u001b[32mWeightApi\u001b[39m with \u001b[32mhyperparameters\u001b[39m.\u001b[32mWeightApi\u001b[39m = Weight[fullName=$sess.cmd9Wrapper.Helper.weight]\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmyNeuralNetwork\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.INDArrayWeight\n",
    "\n",
    "val weight = {\n",
    "    import org.nd4s.Implicits._\n",
    "    INDArrayWeight(Nd4j.randn(NumberOfPixels, NumberOfClasses) * 0.001)\n",
    "}\n",
    "\n",
    "def myNeuralNetwork(input: INDArray): INDArrayLayer = {\n",
    "    softmax(input.dot(weight))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LossFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about the prediction result of the neural network, we need to write the loss function `lossFunction`. We use [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy) to make comparison between this result and the actual result before return the score. Formula:\n",
    "![](https://zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.DoubleLayer\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.DoubleLayer\n",
    "\n",
    "def lossFunction(input: INDArray, expectOutput: INDArray): DoubleLayer = {\n",
    "    val probabilities = myNeuralNetwork(input)\n",
    "    -(hyperparameters.log(probabilities) * expectOutput).mean\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the images and corresponding label information for test data from CIFAR10 database and process them, we need [`import $file.ReadCIFAR10ToNDArray`](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/ReadCIFAR10ToNDArray.sc). This is a script file containing the read and processed CIFAR10 data, provided in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/v1.0.0-doc/ipynbs/ReadCIFAR10ToNDArray.sc\n",
      "downloading data...\n",
      "unzip file...\n",
      "download and unzip done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$url.$                                                                                                                                           \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mtrainNDArray\u001b[39m: \u001b[32mshapeless\u001b[39m.\u001b[32m::\u001b[39m[\u001b[32mINDArray\u001b[39m, \u001b[32mshapeless\u001b[39m.\u001b[32m::\u001b[39m[\u001b[32mINDArray\u001b[39m, \u001b[32mshapeless\u001b[39m.\u001b[32mHNil\u001b[39m]] = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mshapeless\u001b[39m.\u001b[32m::\u001b[39m[\u001b[32mINDArray\u001b[39m, \u001b[32mshapeless\u001b[39m.\u001b[32m::\u001b[39m[\u001b[32mINDArray\u001b[39m, \u001b[32mshapeless\u001b[39m.\u001b[32mHNil\u001b[39m]] = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $url.{` https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/v1.0.0-doc/ipynbs/ReadCIFAR10ToNDArray.sc` => ReadCIFAR10ToNDArray}\n",
    "\n",
    "val trainNDArray = ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/data_batch_1.bin\", 1000)\n",
    "\n",
    "val testNDArray = ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing data to the softmax classifier, we first process label data with ([one hot encoding](https://en.wikipedia.org/wiki/One-hot)): transform INDArray of `NumberOfPixels × 1` into INDArray of `NumberOfPixels × NumberOfClasses`. The value of correct classification corresponding to each line is 1, and the values of other columns are 0. The reason for differentiating the training set and test set is to make it clear that whether the network is over trained which leads to [overfitting](https://en.wikipedia.org/wiki/Overfitting). While processing label data, we used [Utils](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/Utils.sc), which is also provided in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/v1.0.0-doc/ipynbs/Utils.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [6.00, 9.00, 9.00, 4.00, 1.00, 1.00, 2.00, 7.00, 8.00, 3.00, 4.00, 7.00, 7.00, 2.00, 9.00, 9.00, 9.00, 3.00, 2.00, 6.00, 4.00, 3.00, 6.00, 6.00, 2.00, 6.00, 3.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$url.$                                                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mvectorizedTrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainData = trainNDArray.head\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val trainExpectResult = trainNDArray.tail.head\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "import $url.{`https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/v1.0.0-doc/ipynbs/Utils.sc` => Utils}\n",
    "\n",
    "val vectorizedTrainExpectResult = Utils.makeVectorized(trainExpectResult, NumberOfClasses)\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe the training process of the neural network, we need to output `loss`; while training the neural network, the `loss` shall be decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m()\n",
       "\u001b[36mtrainTask\u001b[39m: \u001b[32mFuture\u001b[39m[\u001b[32mUnit\u001b[39m] = scalaz.IndexedContsT@25ef3ca5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var lossSeq: IndexedSeq[Double] = IndexedSeq.empty\n",
    "\n",
    "@monadic[Future]\n",
    "val trainTask: Future[Unit] = {\n",
    "  val lossStream = for (_ <- (1 to 2000).toStream) yield {\n",
    "    lossFunction(trainData, vectorizedTrainExpectResult).train.each\n",
    "  }\n",
    "  lossSeq = IndexedSeq.concat(lossStream)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict  your Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the processed test data to verify the prediction result of the neural network and compute the accuracy. The accuracy shall be about 32%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Await.result(trainTask.toScalaFuture, Duration.Inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 32.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredictResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.03, 0.05, 0.17, 0.13, 0.01, 0.13, 0.42, 0.00, 0.04, 0.00],\n",
       " [0.03, 0.17, 0.00, 0.05, 0.00, 0.01, 0.00, 0.00, 0.18, 0.55],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictResult = Await.result(myNeuralNetwork(testData).predict.toScalaFuture, Duration.Inf)\n",
    "println(\"The accuracy is \" + Utils.getAccuracy(predictResult,testExpectResult) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1479964318\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],\"y\":[0.230278076171875,0.228672314453125,0.2277049072265625,0.226808447265625,0.22594287109375,0.2251033203125,0.22428828125,0.2234966796875,0.22272744140625,0.221979736328125,0.2212526611328125,0.220545361328125,0.21985703125,0.21918681640625,0.2185341064453125,0.217898095703125,0.217278125,0.2166736083984375,0.21608388671875,0.215508349609375,0.214946533203125,0.214397900390625,0.2138617919921875,0.213337939453125,0.2128257568359375,0.2123248291015625,0.211834814453125,0.211355224609375,0.210885693359375,0.210425927734375,0.209975537109375,0.2095342041015625,0.2091015869140625,0.2086774169921875,0.2082614013671875,0.2078532958984375,0.207452783203125,0.20705966796875,0.2066736572265625,0.206294580078125,0.205922119140625,0.2055561767578125,0.205196533203125,0.2048429443359375,0.20449527587890626,0.20415328369140626,0.2038168701171875,0.20348585205078126,0.20316005859375,0.2028393310546875,0.2025235107421875,0.20221256103515625,0.201906201171875,0.20160439453125,0.201306982421875,0.2010138916015625,0.200724951171875,0.2004400634765625,0.200159130859375,0.199882080078125,0.199608740234375,0.19933909912109374,0.199072998046875,0.1988103515625,0.198551123046875,0.19829522705078126,0.1980425048828125,0.19779296875,0.197546484375,0.1973030029296875,0.19706243896484374,0.19682474365234376,0.19658985595703124,0.19635767822265626,0.196128173828125,0.1959012939453125,0.19567696533203124,0.19545511474609376,0.1952357421875,0.19501871337890625,0.194804052734375,0.194591650390625,0.1943815185546875,0.1941735595703125,0.1939677490234375,0.1937640625,0.19356240234375,0.19336279296875,0.1931651611328125,0.1929694580078125,0.19277564697265626,0.19258369140625,0.19239361572265626,0.19220528564453124,0.1920187255859375,0.1918339111328125,0.1916507568359375,0.191469287109375,0.19128944091796876,0.19111119384765626,0.19093453369140626,0.19075938720703126,0.190585791015625,0.190413671875,0.1902429931640625,0.190073779296875,0.18990596923828126,0.18973955078125,0.18957447509765624,0.18941077880859375,0.1892483642578125,0.1890872802734375,0.18892744140625,0.1887689208984375,0.1886115478515625,0.188455419921875,0.1883005126953125,0.18814677734375,0.1879941650390625,0.187842724609375,0.18769239501953125,0.1875431640625,0.18739503173828126,0.1872479736328125,0.187101953125,0.18695697021484375,0.18681298828125,0.1866700439453125,0.1865281005859375,0.186387109375,0.1862471435546875,0.1861080322265625,0.18596990966796875,0.1858326904296875,0.1856964111328125,0.185560986328125,0.18542647705078125,0.1852928466796875,0.18516005859375,0.185028125,0.1848970458984375,0.184766748046875,0.1846373046875,0.1845086181640625,0.18438077392578126,0.1842536865234375,0.1841273681640625,0.184001806640625,0.1838770263671875,0.1837530029296875,0.1836296630859375,0.18350706787109375,0.1833851806640625,0.183264013671875,0.18314351806640625,0.18302373046875,0.18290462646484376,0.182786181640625,0.1826684326171875,0.1825512939453125,0.18243482666015626,0.18231898193359375,0.182203759765625,0.1820891845703125,0.181975244140625,0.181861865234375,0.181749169921875,0.18163697509765625,0.1815254150390625,0.1814144287109375,0.1813040283203125,0.18119420166015626,0.181084912109375,0.1809761962890625,0.1808680419921875,0.18076043701171876,0.1806533447265625,0.1805468017578125,0.1804407470703125,0.1803352783203125,0.1802302734375,0.1801258056640625,0.180021826171875,0.17991834716796876,0.179815380859375,0.1797128662109375,0.17961085205078126,0.1795093505859375,0.17940830078125,0.17930770263671875,0.1792075927734375,0.1791079345703125,0.179008740234375,0.1789099853515625,0.1788116455078125,0.1787137939453125,0.178616357421875,0.1785193603515625,0.17842279052734375,0.1783266357421875,0.1782309326171875,0.178135595703125,0.17804071044921874,0.1779462158203125,0.17785213623046875,0.177758447265625,0.1776651611328125,0.1775722900390625,0.17747978515625,0.177387646484375,0.1772958984375,0.1772045654296875,0.17711358642578126,0.1770229736328125,0.17693272705078125,0.17684287109375,0.1767533447265625,0.1766641845703125,0.176575390625,0.17648692626953125,0.1763988525390625,0.176311083984375,0.1762236572265625,0.17613660888671875,0.17604986572265624,0.17596348876953125,0.17587742919921875,0.1757916748046875,0.1757062744140625,0.1756212158203125,0.1755364501953125,0.175451953125,0.1753678466796875,0.17528402099609375,0.1752005126953125,0.17511728515625,0.17503441162109376,0.17495179443359374,0.174869482421875,0.1747874755859375,0.17470579833984376,0.1746243896484375,0.1745432373046875,0.17446241455078124,0.1743818603515625,0.174301611328125,0.174221630859375,0.17414189453125,0.1740625,0.1739833251953125,0.173904443359375,0.17382584228515624,0.17374752197265625,0.17366943359375,0.1735916015625,0.17351405029296876,0.1734367431640625,0.173359716796875,0.1732829345703125,0.17320643310546874,0.1731301513671875,0.1730541259765625,0.172978369140625,0.1729028564453125,0.1728275634765625,0.1727525390625,0.17267772216796876,0.172603173828125,0.17252886962890626,0.17245478515625,0.1723809326171875,0.1723072998046875,0.1722339599609375,0.17216077880859376,0.17208785400390625,0.1720151611328125,0.1719427001953125,0.171870458984375,0.17179840087890624,0.17172659912109375,0.1716550048828125,0.171583642578125,0.1715124755859375,0.17144154052734376,0.17137080078125,0.17130028076171874,0.1712299560546875,0.17115986328125,0.1710899658203125,0.17102027587890625,0.17095079345703126,0.17088150634765625,0.1708124267578125,0.1707435546875,0.17067484130859376,0.1706063720703125,0.17053807373046875,0.1704699951171875,0.1704020751953125,0.170334375,0.17026685791015625,0.17019952392578125,0.170132373046875,0.1700654296875,0.16999864501953124,0.1699320556640625,0.16986566162109376,0.16979945068359376,0.1697334228515625,0.1696675537109375,0.16960186767578125,0.169536376953125,0.16947103271484376,0.169405908203125,0.1693409423828125,0.16927613525390625,0.16921151123046874,0.1691470458984375,0.1690827880859375,0.16901864013671875,0.1689546875,0.16889090576171875,0.1688273193359375,0.16876387939453125,0.1687005859375,0.1686374755859375,0.16857451171875,0.1685116943359375,0.16844908447265625,0.1683865966796875,0.16832427978515624,0.16826212158203124,0.16820009765625,0.1681382568359375,0.1680765625,0.1680150634765625,0.1679536376953125,0.1678924072265625,0.1678313232421875,0.16777039794921875,0.16770960693359374,0.167648974609375,0.1675884765625,0.1675281494140625,0.16746795654296875,0.16740789794921876,0.167347998046875,0.1672882568359375,0.167228662109375,0.167169140625,0.167109814453125,0.1670506103515625,0.16699156494140624,0.1669326416015625,0.166873876953125,0.16681524658203126,0.16675672607421874,0.1666983642578125,0.16664013671875,0.1665820068359375,0.1665240478515625,0.166466259765625,0.16640855712890626,0.1663509521484375,0.1662935302734375,0.16623623046875,0.166179052734375,0.1661219970703125,0.166065087890625,0.1660082763671875,0.165951611328125,0.165895068359375,0.1658386474609375,0.165782373046875,0.16572618408203124,0.165670166015625,0.1656142333984375,0.1655584228515625,0.16550277099609376,0.165447216796875,0.1653917724609375,0.165336474609375,0.16528125,0.16522620849609376,0.165171240234375,0.1651163818359375,0.16506165771484374,0.1650070556640625,0.1649525634765625,0.1648981689453125,0.164843896484375,0.16478975830078124,0.16473572998046876,0.16468179931640625,0.164627978515625,0.16457427978515626,0.1645206787109375,0.1644672119140625,0.1644138427734375,0.16436058349609375,0.164307421875,0.1642543701171875,0.1642014404296875,0.1641486083984375,0.16409588623046875,0.1640432861328125,0.163990771484375,0.1639383544921875,0.16388607177734374,0.16383387451171874,0.1637817626953125,0.1637297607421875,0.163677880859375,0.16362611083984374,0.1635744384765625,0.1635228515625,0.1634713623046875,0.163419970703125,0.163368701171875,0.163317529296875,0.16326644287109374,0.1632154541015625,0.1631645751953125,0.16311378173828125,0.1630630859375,0.16301248779296876,0.1629619873046875,0.1629115966796875,0.1628613037109375,0.162811083984375,0.1627609619140625,0.1627109619140625,0.1626610107421875,0.162611181640625,0.16256142578125,0.1625117919921875,0.1624622314453125,0.1624127685546875,0.16236337890625,0.162314111328125,0.1622649169921875,0.16221578369140624,0.1621667724609375,0.1621178466796875,0.1620690185546875,0.162020263671875,0.1619716064453125,0.161923046875,0.161874560546875,0.1618261474609375,0.16177784423828126,0.1617296142578125,0.1616814697265625,0.161633447265625,0.16158546142578126,0.1615375732421875,0.1614897705078125,0.16144205322265626,0.16139442138671875,0.161346875,0.16129942626953125,0.16125201416015625,0.1612047119140625,0.16115750732421874,0.1611103759765625,0.16106331787109374,0.161016357421875,0.16096943359375,0.1609226318359375,0.16087587890625,0.1608292236328125,0.160782666015625,0.1607361328125,0.16068974609375,0.1606433837890625,0.1605971435546875,0.1605509521484375,0.16050484619140626,0.1604588134765625,0.160412841796875,0.16036700439453125,0.16032119140625,0.1602754638671875,0.1602298095703125,0.160184228515625,0.160138720703125,0.16009329833984376,0.16004794921875,0.160002685546875,0.15995748291015624,0.15991234130859375,0.15986729736328126,0.15982230224609376,0.15977738037109376,0.15973255615234375,0.15968780517578124,0.1596430908203125,0.159598486328125,0.15955390625,0.15950943603515624,0.15946502685546876,0.15942069091796876,0.159376416015625,0.1593322265625,0.1592880859375,0.15924403076171875,0.159200048828125,0.1591561279296875,0.159112255859375,0.15906845703125,0.159024755859375,0.15898111572265625,0.15893751220703126,0.15889400634765624,0.158850537109375,0.158807177734375,0.1587638427734375,0.15872060546875,0.1586774169921875,0.158634326171875,0.15859124755859375,0.158548291015625,0.1585053466796875,0.1584625,0.1584197021484375,0.1583769775390625,0.1583343017578125,0.15829169921875,0.158249169921875,0.158206689453125,0.1581642822265625,0.1581219482421875,0.1580796630859375,0.1580374267578125,0.157995263671875,0.1579531982421875,0.1579111328125,0.157869189453125,0.1578272705078125,0.157785400390625,0.1577436279296875,0.15770189208984375,0.15766021728515625,0.15761861572265626,0.15757706298828125,0.157535595703125,0.157494140625,0.1574528076171875,0.157411474609375,0.15737021484375,0.157329052734375,0.1572879150390625,0.15724683837890624,0.157205810546875,0.15716484375,0.1571239501953125,0.1570831298828125,0.15704232177734376,0.15700159912109374,0.1569609375,0.1569203369140625,0.1568797607421875,0.1568392578125,0.156798828125,0.15675845947265624,0.156718115234375,0.15667783203125,0.15663759765625,0.1565974609375,0.1565573486328125,0.15651728515625,0.15647728271484376,0.156437353515625,0.1563974609375,0.1563576416015625,0.15631785888671876,0.15627811279296874,0.156238427734375,0.156198828125,0.1561592529296875,0.15611973876953125,0.15608028564453125,0.156040869140625,0.156001513671875,0.1559622314453125,0.15592298583984374,0.1558837646484375,0.15584462890625,0.15580555419921874,0.1557664794921875,0.1557275146484375,0.15568857421875,0.1556496826171875,0.1556108642578125,0.1555720703125,0.155533349609375,0.1554946533203125,0.15545601806640624,0.155417431640625,0.15537891845703125,0.1553404296875,0.1553019775390625,0.155263623046875,0.1552252685546875,0.15518701171875,0.1551487548828125,0.1551105712890625,0.1550724365234375,0.1550343505859375,0.154996337890625,0.15495831298828125,0.154920361328125,0.1548824951171875,0.15484466552734374,0.15480684814453124,0.1547691162109375,0.154731396484375,0.15469375,0.1546561279296875,0.1546185791015625,0.1545810546875,0.15454359130859374,0.154506201171875,0.1544687744140625,0.15443148193359374,0.154394189453125,0.1543569580078125,0.1543197998046875,0.1542826416015625,0.154245556640625,0.15420849609375,0.154171484375,0.15413453369140626,0.154097607421875,0.1540607421875,0.15402392578125,0.1539871337890625,0.15395040283203126,0.153913720703125,0.1538770751953125,0.15384046630859374,0.15380391845703126,0.1537674072265625,0.1537309326171875,0.15369453125,0.1536581298828125,0.15362181396484376,0.153585498046875,0.153549267578125,0.1535130615234375,0.1534768798828125,0.15344075927734374,0.1534046875,0.15336865234375,0.153332666015625,0.1532967041015625,0.1532608154296875,0.1532249267578125,0.153189111328125,0.1531533447265625,0.153117578125,0.1530819091796875,0.153046240234375,0.15301060791015625,0.152975048828125,0.1529395263671875,0.1529040283203125,0.1528685791015625,0.15283316650390624,0.15279781494140626,0.1527624755859375,0.152727197265625,0.152691943359375,0.1526567138671875,0.15262156982421876,0.1525864501953125,0.1525513671875,0.1525163330078125,0.1524813232421875,0.15244635009765625,0.1524114013671875,0.1523765625,0.15234168701171874,0.1523069091796875,0.152272119140625,0.15223741455078124,0.152202685546875,0.15216806640625,0.15213343505859375,0.1520988525390625,0.1520643310546875,0.152029833984375,0.1519953857421875,0.151960986328125,0.1519265869140625,0.1518922607421875,0.1518579345703125,0.1518236572265625,0.151789453125,0.1517552490234375,0.15172109375,0.15168697509765625,0.1516529052734375,0.1516188720703125,0.15158487548828126,0.15155087890625,0.1515169677734375,0.15148306884765625,0.15144921875,0.151415380859375,0.1513816162109375,0.15134786376953124,0.15131414794921874,0.1512804931640625,0.1512468505859375,0.1512132568359375,0.15117967529296875,0.1511461669921875,0.15111265869140625,0.15107919921875,0.1510457763671875,0.15101239013671874,0.15097906494140625,0.1509457275390625,0.150912451171875,0.15087919921875,0.15084600830078124,0.150812841796875,0.1507797119140625,0.15074658203125,0.150713525390625,0.15068046875,0.15064747314453125,0.15061451416015625,0.150581591796875,0.15054869384765626,0.15051583251953124,0.1504830078125,0.15045020751953125,0.1504174560546875,0.15038472900390626,0.15035205078125,0.150319384765625,0.150286767578125,0.1502541748046875,0.1502216064453125,0.150189111328125,0.150156591796875,0.1501241455078125,0.1500917236328125,0.1500593505859375,0.1500269775390625,0.1499946533203125,0.149962353515625,0.1499301025390625,0.1498978759765625,0.14986571044921876,0.1498335205078125,0.149801416015625,0.149769287109375,0.149737255859375,0.149705224609375,0.1496732177734375,0.14964124755859376,0.14960931396484375,0.1495774169921875,0.149545556640625,0.149513720703125,0.1494819091796875,0.14945013427734374,0.1494183837890625,0.149386669921875,0.1493550048828125,0.14932335205078126,0.14929173583984376,0.14926015625,0.1492285888671875,0.1491970703125,0.14916558837890626,0.149134130859375,0.14910269775390625,0.1490713134765625,0.14903990478515625,0.14900859375,0.14897728271484376,0.14894599609375,0.14891475830078124,0.14888355712890625,0.1488523681640625,0.14882119140625,0.1487900634765625,0.148758984375,0.14872791748046876,0.148696875,0.1486658935546875,0.14863489990234374,0.1486039794921875,0.148573046875,0.1485421630859375,0.1485113037109375,0.1484804931640625,0.1484496826171875,0.1484189208984375,0.14838818359375,0.148357470703125,0.148326806640625,0.148296142578125,0.14826552734375,0.1482349365234375,0.1482043701171875,0.148173828125,0.14814332275390624,0.14811285400390625,0.1480823974609375,0.1480519775390625,0.14802159423828126,0.1479912353515625,0.147960888671875,0.1479305908203125,0.1479003173828125,0.147870068359375,0.14783984375,0.14780965576171876,0.14777947998046875,0.14774935302734374,0.1477192626953125,0.1476891357421875,0.147659130859375,0.14762908935546876,0.1475990966796875,0.1475691162109375,0.1475391845703125,0.1475092529296875,0.1474793701171875,0.14744951171875,0.147419677734375,0.1473898681640625,0.147360107421875,0.1473303466796875,0.1473006103515625,0.1472709228515625,0.14724127197265624,0.14721162109375,0.147181982421875,0.1471524169921875,0.1471228515625,0.1470933349609375,0.147063818359375,0.147034326171875,0.1470048583984375,0.146975439453125,0.146946044921875,0.14691666259765626,0.14688731689453124,0.1468580078125,0.1468286865234375,0.14679942626953124,0.1467701904296875,0.146740966796875,0.14671175537109374,0.1466826171875,0.1466534423828125,0.1466243408203125,0.146595263671875,0.1465661865234375,0.14653714599609374,0.146508154296875,0.14647916259765625,0.1464501953125,0.146421240234375,0.14639234619140626,0.14636346435546874,0.1463345947265625,0.14630577392578126,0.146276953125,0.146248193359375,0.1462194091796875,0.14619068603515625,0.1461619873046875,0.14613328857421876,0.146104638671875,0.14607601318359376,0.1460473876953125,0.146018798828125,0.145990234375,0.14596171875,0.14593321533203124,0.1459047119140625,0.1458762451171875,0.145847802734375,0.14581939697265625,0.145791015625,0.145762646484375,0.14573431396484374,0.1457060302734375,0.1456777099609375,0.14564945068359375,0.14562119140625,0.1455929931640625,0.14556478271484374,0.14553660888671874,0.145508447265625,0.14548033447265624,0.14545223388671874,0.14542415771484374,0.1453961181640625,0.1453680908203125,0.1453400634765625,0.145312109375,0.1452841552734375,0.1452562255859375,0.1452282958984375,0.14520040283203126,0.14517254638671875,0.14514471435546875,0.14511689453125,0.1450890869140625,0.145061328125,0.14503359375,0.145005859375,0.1449781494140625,0.1449504638671875,0.14492281494140624,0.1448951904296875,0.144867578125,0.1448399658203125,0.1448124267578125,0.1447848876953125,0.144757373046875,0.14472987060546874,0.1447024169921875,0.14467493896484376,0.144647509765625,0.1446201171875,0.144592724609375,0.1445653564453125,0.144538037109375,0.1445107177734375,0.14448341064453124,0.1444561279296875,0.1444288818359375,0.14440164794921875,0.144374462890625,0.14434727783203125,0.14432012939453126,0.14429296875,0.14426585693359376,0.1442387451171875,0.1442116943359375,0.1441846435546875,0.1441575927734375,0.1441305908203125,0.1441035888671875,0.1440766357421875,0.1440496826171875,0.1440227783203125,0.14399588623046874,0.143968994140625,0.14394212646484375,0.143915283203125,0.1438885009765625,0.1438616943359375,0.143834912109375,0.14380816650390624,0.1437814453125,0.14375472412109375,0.1437280517578125,0.1437013671875,0.1436747314453125,0.14364808349609376,0.143621484375,0.143594921875,0.1435683349609375,0.143541796875,0.143515283203125,0.14348876953125,0.1434623046875,0.1434358642578125,0.143409423828125,0.1433830078125,0.1433566162109375,0.14333023681640625,0.1433038818359375,0.14327752685546874,0.143251220703125,0.1432249267578125,0.1431986328125,0.1431723876953125,0.1431461669921875,0.1431199462890625,0.14309376220703124,0.1430676025390625,0.143041455078125,0.1430153076171875,0.14298919677734376,0.14296309814453126,0.1429370361328125,0.142910986328125,0.1428849365234375,0.142858935546875,0.1428329345703125,0.142806982421875,0.142781005859375,0.142755078125,0.14272916259765625,0.142703271484375,0.1426774169921875,0.14265155029296875,0.14262572021484374,0.1425999267578125,0.1425740966796875,0.1425483154296875,0.1425225830078125,0.14249686279296875,0.142471142578125,0.1424454345703125,0.14241973876953126,0.142394091796875,0.14236845703125,0.14234281005859376,0.14231722412109374,0.14229163818359375,0.14226607666015625,0.14224051513671876,0.14221500244140625,0.14218951416015624,0.142164013671875,0.1421385498046875,0.1421131103515625,0.1420876708984375,0.1420622314453125,0.142036865234375,0.1420114990234375,0.1419861083984375,0.141960791015625,0.1419354736328125,0.14191016845703125,0.1418848876953125,0.141859619140625,0.1418343994140625,0.1418091796875,0.1417839599609375,0.1417587890625,0.14173359375,0.141708447265625,0.14168333740234376,0.14165821533203124,0.1416331298828125,0.14160802001953124,0.14158297119140625,0.141557958984375,0.14153291015625,0.14150789794921875,0.14148292236328125,0.14145794677734375,0.1414330078125,0.1414080810546875,0.141383154296875,0.141358251953125,0.1413333740234375,0.1413085205078125,0.14128369140625,0.141258837890625,0.141234033203125,0.14120927734375,0.14118447265625,0.1411597412109375,0.14113499755859374,0.1411102783203125,0.1410855712890625,0.1410609130859375,0.1410362548828125,0.141011572265625,0.14098697509765626,0.140962353515625,0.14093775634765626,0.1409131591796875,0.1408885986328125,0.1408640869140625,0.1408395263671875,0.1408150634765625,0.1407905517578125,0.140766064453125,0.1407416259765625,0.1407171875,0.1406927734375,0.140668359375,0.140643994140625,0.14061962890625,0.14059525146484375,0.1405709228515625,0.14054658203125,0.1405222900390625,0.140497998046875,0.1404737548828125,0.1404494873046875,0.1404252685546875,0.1404010498046875,0.1403768310546875,0.1403526611328125,0.14032850341796874,0.140304345703125,0.140280224609375,0.14025609130859376,0.14023199462890626,0.14020792236328125,0.1401838623046875,0.14015980224609376,0.1401357666015625,0.14011175537109374,0.14008775634765624,0.14006376953125,0.1400398193359375,0.140015869140625,0.1399919189453125,0.139968017578125,0.139944091796875,0.1399202392578125,0.13989635009765625,0.139872509765625,0.139848681640625,0.1398248291015625,0.1398010498046875,0.13977724609375,0.13975347900390625,0.13972972412109375,0.139706005859375,0.139682275390625,0.1396585693359375,0.1396348876953125,0.139611181640625,0.139587548828125,0.13956390380859374,0.139540283203125,0.139516650390625,0.13949306640625,0.139469482421875,0.139445947265625,0.13942237548828126,0.1393988525390625,0.1393753173828125,0.1393518310546875,0.139328369140625,0.1393049072265625,0.13928143310546875,0.1392580078125,0.1392345947265625,0.139211181640625,0.1391877685546875,0.1391644287109375,0.13914105224609374,0.139117724609375,0.139094384765625,0.1390710693359375,0.1390477783203125,0.13902449951171875,0.13900123291015626,0.1389780029296875,0.1389547607421875,0.1389315185546875,0.1389083251953125,0.1388851318359375,0.138861962890625,0.1388387939453125,0.138815673828125,0.138792529296875,0.13876942138671874,0.13874630126953125,0.1387232177734375,0.13870018310546875,0.1386771240234375,0.1386540771484375,0.1386310546875,0.138608056640625,0.13858507080078125,0.1385620849609375,0.1385391357421875,0.138516162109375,0.13849324951171876,0.1384703369140625,0.1384474365234375,0.1384245361328125,0.13840166015625,0.1383787841796875,0.13835595703125,0.138333154296875,0.1383103271484375,0.1382875244140625,0.1382647216796875,0.138241943359375,0.1382192138671875,0.1381964599609375,0.1381737548828125,0.138151025390625,0.13812833251953124,0.1381056640625,0.1380829833984375,0.1380603271484375,0.13803770751953126,0.1380150634765625,0.1379924560546875,0.1379698486328125,0.13794727783203126,0.13792470703125,0.1379021484375,0.13787962646484375,0.137857080078125,0.1378345703125,0.13781209716796874,0.137789599609375,0.137767138671875,0.1377447021484375,0.137722265625,0.13769984130859375,0.13767740478515625,0.137655029296875,0.13763265380859374,0.137610302734375,0.1375879150390625,0.137565576171875,0.13754326171875,0.137520947265625,0.13749864501953124,0.13747635498046876,0.1374541015625,0.1374318359375,0.137409619140625,0.137387353515625,0.13736514892578125,0.13734295654296874,0.1373207763671875,0.1372986083984375,0.1372764404296875,0.137254296875,0.1372321533203125,0.13721005859375,0.13718795166015624,0.137165869140625,0.13714378662109375,0.1371217041015625,0.13709967041015625,0.13707762451171876,0.137055615234375,0.13703359375,0.13701162109375,0.13698961181640626,0.1369676513671875,0.1369456787109375,0.1369237548828125,0.1369018310546875,0.1368799072265625,0.13685802001953126,0.1368361328125,0.1368142578125,0.13679239501953125,0.13677054443359374,0.13674871826171875,0.13672689208984376,0.13670509033203124,0.13668328857421874,0.1366614990234375,0.13663975830078126,0.1366179931640625,0.13659625244140625,0.13657451171875,0.13655279541015625,0.136531103515625,0.136509423828125,0.136487744140625,0.136466064453125,0.1364444091796875,0.136422802734375,0.136401171875,0.136379541015625,0.13635797119140625,0.1363364013671875,0.136314794921875,0.13629326171875,0.13627171630859375,0.1362501708984375,0.13622864990234376,0.1362071533203125,0.13618564453125,0.13616416015625,0.1361427001953125,0.13612125244140624,0.13609979248046875,0.13607838134765626,0.13605694580078126,0.1360355712890625,0.13601416015625,0.13599278564453124,0.1359714111328125,0.13595006103515625,0.1359287353515625,0.1359073974609375,0.135886083984375,0.135864794921875,0.1358434814453125,0.13582222900390625,0.1358009521484375,0.1357796875,0.13575845947265625,0.13573721923828125,0.13571597900390625,0.13569482421875,0.13567362060546875,0.13565244140625,0.1356312744140625,0.1356101318359375,0.13558897705078124,0.1355678466796875,0.1355467529296875,0.135525634765625,0.135504541015625,0.1354834716796875,0.13546240234375,0.13544134521484374,0.13542030029296875,0.135399267578125,0.1353782470703125,0.13535723876953126,0.13533624267578126,0.13531527099609375,0.1352943115234375,0.13527335205078125,0.13525240478515624,0.13523145751953125,0.13521053466796876,0.1351896484375,0.1351687255859375,0.1351478515625,0.1351269775390625,0.1351061279296875,0.13508529052734375,0.1350644287109375,0.1350436279296875,0.135022802734375,0.1350019775390625,0.13498121337890626,0.13496043701171875,0.13493968505859374,0.13491890869140624,0.1348981689453125,0.1348774658203125,0.1348567138671875,0.13483603515625,0.13481531982421874,0.13479462890625,0.134773974609375,0.1347533203125,0.134732666015625,0.1347120361328125,0.13469140625,0.1346707763671875,0.1346501953125,0.1346296142578125,0.1346090576171875,0.1345884765625,0.13456793212890625,0.13454736328125,0.13452684326171874,0.1345063232421875,0.1344858154296875,0.13446533203125,0.13444483642578126,0.134424365234375,0.13440390625,0.134383447265625,0.1343630126953125,0.134342578125,0.13432216796875,0.13430177001953125,0.1342813720703125,0.13426099853515625,0.134240625,0.1342202392578125,0.13419990234375,0.13417957763671876,0.13415924072265625,0.1341389404296875,0.13411865234375,0.13409833984375,0.134078076171875,0.1340577880859375,0.134037548828125,0.1340173095703125,0.1339970703125,0.13397684326171874,0.133956640625,0.13393642578125,0.1339162353515625,0.13389605712890626,0.13387589111328124,0.13385574951171875,0.13383558349609376,0.1338154541015625,0.1337953369140625,0.1337751953125,0.133755126953125,0.1337350341796875,0.13371495361328126,0.133694873046875,0.13367484130859375,0.13365478515625,0.13363472900390624,0.13361470947265625,0.13359471435546874,0.13357470703125,0.13355472412109376,0.1335347412109375,0.13351475830078124,0.1334947998046875,0.13347484130859374,0.1334549072265625,0.13343499755859375,0.133415087890625,0.13339515380859376,0.13337528076171876,0.1333553955078125,0.1333355224609375,0.1333156494140625,0.13329580078125,0.1332759521484375,0.13325614013671874,0.13323631591796875,0.13321650390625,0.1331967041015625,0.13317694091796875,0.13315714111328125,0.1331373779296875,0.133117626953125,0.133097900390625,0.1330781494140625,0.1330584228515625,0.133038720703125,0.13301900634765626,0.13299931640625,0.132979638671875,0.13295994873046876,0.13294029541015626,0.13292066650390624,0.1329010009765625,0.13288138427734375,0.13286177978515626,0.13284217529296874,0.13282255859375,0.1328029541015625,0.1327834228515625,0.132763818359375,0.1327442626953125,0.1327247314453125,0.1327052001953125,0.1326856689453125,0.13266614990234374,0.13264664306640625,0.1326271728515625,0.13260767822265626,0.1325882080078125,0.13256875,0.13254927978515624,0.132529833984375,0.13251041259765625,0.132491015625,0.13247158203125,0.1324521728515625,0.132432763671875,0.13241339111328124,0.1323940185546875,0.1323746826171875,0.13235531005859374,0.1323359619140625,0.1323166259765625,0.132297314453125,0.1322780029296875,0.1322587158203125,0.1322394287109375,0.13222012939453126,0.1322008544921875,0.1321816162109375,0.132162353515625,0.1321430908203125,0.13212388916015624,0.13210467529296874,0.13208544921875,0.1320662353515625,0.1320470458984375,0.132027880859375,0.13200870361328124,0.13198953857421875,0.1319703857421875,0.1319512451171875,0.13193211669921875,0.1319130126953125,0.131893896484375,0.1318747802734375,0.1318556884765625,0.13183662109375,0.1318175537109375,0.13179847412109374,0.1317794189453125,0.1317603759765625,0.13174134521484376,0.13172232666015626,0.1317032958984375,0.13168431396484376,0.13166529541015626,0.1316463134765625,0.13162733154296874,0.1316083740234375,0.131589404296875,0.1315704833984375,0.1315515380859375,0.1315326171875,0.1315136962890625,0.13149478759765626,0.13147587890625,0.13145699462890625,0.13143812255859375,0.1314192626953125,0.131400390625,0.13138154296875,0.13136270751953125,0.13134384765625,0.131325048828125,0.13130623779296874,0.1312874267578125,0.13126864013671874,0.131249853515625,0.1312310546875,0.1312123046875,0.1311935546875,0.1311748291015625,0.1311560791015625,0.1311373291015625,0.13111864013671876,0.13109991455078124,0.1310812255859375,0.1310625244140625,0.13104384765625,0.1310251708984375,0.1310065185546875,0.1309878662109375,0.13096922607421874,0.1309505859375,0.1309319580078125,0.13091334228515625,0.1308947265625,0.13087613525390626,0.1308575439453125,0.13083897705078126,0.1308203857421875,0.1308018310546875,0.13078330078125,0.13076474609375,0.1307462158203125,0.130727685546875,0.1307091552734375,0.13069066162109375,0.13067216796875,0.1306536865234375,0.130635205078125,0.130616748046875,0.13059827880859376,0.130579833984375,0.130561376953125,0.13054296875,0.13052452392578126,0.13050611572265625,0.1304877197265625,0.13046932373046874,0.1304509033203125,0.1304325439453125,0.1304141845703125,0.13039580078125,0.13037745361328126,0.130359130859375,0.1303407958984375,0.1303224609375,0.1303041259765625,0.13028585205078125,0.1302675537109375,0.1302492431640625,0.13023096923828126,0.13021270751953126,0.13019443359375,0.130176171875,0.1301579345703125,0.1301397216796875,0.130121484375,0.130103271484375,0.13008505859375,0.130066845703125,0.1300486328125,0.13003048095703126,0.1300123046875,0.129994140625,0.12997596435546874,0.1299578125,0.129939697265625,0.12992154541015624,0.1299034423828125,0.12988531494140626,0.1298672119140625,0.12984912109375,0.1298310302734375,0.1298129638671875,0.1297948974609375,0.129776806640625,0.1297587890625,0.1297407470703125,0.129722705078125,0.1297046875,0.12968665771484375,0.12966865234375,0.1296506591796875,0.12963265380859376,0.12961468505859375,0.1295966796875,0.12957872314453125,0.129560791015625,0.129542822265625,0.1295248779296875,0.1295069580078125,0.12948902587890626,0.1294711181640625,0.12945322265625,0.12943531494140625,0.129417431640625,0.1293995361328125,0.1293816650390625,0.1293637939453125,0.129345947265625,0.1293281005859375,0.12931026611328125,0.129292431640625,0.129274609375,0.12925677490234375,0.1292389892578125,0.12922119140625,0.12920340576171874,0.1291856201171875,0.1291678466796875,0.12915008544921874,0.12913232421875,0.12911455078125,0.12909683837890626,0.1290791015625,0.129061376953125,0.1290436767578125,0.12902596435546876,0.1290082763671875,0.12899056396484376,0.128972900390625,0.128955224609375,0.12893756103515625,0.1289198974609375,0.12890225830078125,0.1288846435546875,0.1288669677734375,0.128849365234375,0.12883173828125,0.1288141357421875,0.128796533203125,0.12877894287109376,0.12876136474609376,0.1287437744140625,0.128726220703125,0.1287086669921875,0.12869110107421874,0.12867357177734376,0.1286560302734375,0.1286385009765625,0.12862099609375,0.12860347900390626,0.128585986328125,0.1285684814453125,0.1285510009765625,0.1285335205078125,0.1285160400390625,0.12849859619140624,0.12848114013671874,0.1284636962890625,0.1284462646484375,0.12842880859375,0.1284114013671875,0.128393994140625,0.1283765869140625,0.12835919189453124,0.12834180908203124,0.12832440185546876,0.12830703125,0.128289697265625,0.128272314453125,0.12825498046875,0.1282376220703125,0.1282202880859375,0.128202978515625,0.12818564453125,0.1281683349609375,0.128151025390625,0.128133740234375,0.128116455078125,0.128099169921875,0.128081884765625,0.12806463623046874,0.1280473876953125,0.12803011474609374,0.128012890625,0.12799566650390626,0.12797841796875,0.1279612060546875,0.12794398193359374,0.12792679443359375,0.1279095947265625,0.127892431640625,0.1278752197265625,0.12785806884765624,0.12784091796875,0.1278237548828125,0.1278066162109375,0.12778948974609375,0.1277723388671875,0.1277552001953125,0.1277380859375,0.12772098388671874,0.1277038818359375,0.12768677978515625,0.12766971435546876,0.1276526123046875,0.127635546875,0.1276184814453125,0.1276014404296875,0.12758438720703125,0.12756734619140625,0.12755030517578125,0.12753330078125,0.1275162841796875,0.12749925537109374,0.1274822509765625,0.127465234375,0.1274482666015625,0.1274312744140625,0.127414306640625,0.1273973388671875,0.1273803955078125,0.12736343994140625,0.12734649658203126,0.1273295654296875,0.12731263427734374,0.12729571533203124,0.12727880859375,0.1272618896484375,0.1272449951171875,0.1272281005859375,0.12721123046875,0.12719434814453126,0.1271774658203125,0.1271606201171875,0.1271437744140625,0.12712691650390626,0.12711005859375,0.12709326171875,0.12707642822265625,0.1270595947265625,0.1270427978515625,0.12702598876953125,0.12700919189453125,0.1269924072265625,0.126975634765625,0.1269588623046875,0.12694208984375,0.126925341796875,0.1269085693359375,0.126891845703125,0.1268751220703125,0.12685836181640625,0.12684166259765625,0.126824951171875,0.1268082275390625,0.1267915283203125,0.126774853515625,0.1267581787109375,0.1267414794921875,0.12672481689453124,0.1267081298828125,0.1266914794921875,0.12667484130859374,0.12665819091796876,0.12664156494140624,0.126624951171875,0.12660831298828126,0.12659168701171875,0.12657508544921875,0.12655849609375,0.1265419189453125,0.12652529296875,0.12650872802734375,0.12649217529296874,0.1264756103515625,0.12645904541015626,0.1264425048828125,0.1264259521484375,0.126409423828125,0.1263928955078125,0.12637637939453125,0.12635986328125,0.126343359375,0.12632685546875,0.12631036376953125,0.1262938720703125,0.126277392578125,0.12626092529296876,0.1262444580078125,0.1262280029296875,0.12621156005859374,0.1261951171875,0.126178662109375,0.12616224365234374,0.12614581298828126,0.12612939453125,0.12611298828125,0.1260966064453125,0.1260802001953125,0.1260637939453125,0.12604742431640625,0.1260310546875,0.1260146728515625,0.12599833984375,0.125981982421875,0.125965625,0.1259492919921875,0.125932958984375,0.1259166259765625,0.1259003173828125,0.1258840087890625,0.12586768798828124,0.1258513916015625,0.125835107421875,0.1258188232421875,0.1258025390625,0.1257862548828125,0.1257699951171875,0.1257537353515625,0.12573751220703125,0.12572125244140625,0.12570501708984375,0.1256887939453125,0.12567255859375,0.12565633544921875,0.12564013671875,0.12562393798828125,0.125607763671875,0.125591552734375,0.1255753662109375,0.1255591796875,0.12554302978515625,0.12552686767578125,0.125510693359375,0.1254945556640625,0.12547841796875,0.1254622802734375,0.1254461669921875,0.1254300537109375,0.125413916015625,0.12539781494140625,0.1253817138671875,0.125365625,0.1253495361328125,0.125333447265625,0.1253173828125,0.12530126953125,0.12528524169921876,0.1252691650390625,0.12525313720703124,0.12523707275390625,0.12522103271484375,0.125205029296875,0.125189013671875,0.125172998046875,0.12515699462890625,0.1251409912109375,0.1251249755859375,0.125108984375,0.125093017578125,0.12507705078125,0.1250610595703125,0.12504508056640626,0.1250291259765625,0.1250132080078125,0.1249972412109375,0.12498131103515625,0.124965380859375,0.12494945068359375,0.12493353271484375,0.12491761474609375,0.12490172119140625,0.12488582763671875,0.124869921875,0.124854052734375,0.12483817138671875,0.1248222900390625,0.12480645751953125,0.124790576171875,0.1247747314453125,0.12475887451171876,0.124743017578125,0.124727197265625,0.124711376953125,0.124695556640625,0.124679736328125,0.1246639404296875,0.1246481201171875,0.12463231201171875,0.12461654052734375,0.1246007568359375,0.1245849853515625,0.12456922607421875,0.1245534423828125,0.1245376953125,0.1245219482421875,0.124506201171875,0.124490478515625,0.1244747314453125,0.124458984375,0.1244432861328125,0.124427587890625,0.124411865234375,0.1243961669921875,0.1243804931640625,0.12436480712890625,0.12434912109375,0.1243334228515625,0.1243177734375,0.124302099609375,0.124286474609375,0.12427080078125,0.1242551513671875,0.1242395263671875,0.124223876953125,0.1242082763671875,0.12419266357421875,0.1241770263671875,0.1241614501953125,0.1241458251953125,0.12413023681640625,0.1241146484375,0.124099072265625,0.12408349609375,0.1240678955078125,0.12405234375,0.12403681640625,0.12402127685546875,0.124005712890625,0.12399017333984375,0.123974658203125,0.12395911865234376,0.12394359130859375,0.1239281005859375,0.12391258544921875,0.1238970703125,0.123881591796875,0.12386611328125,0.12385059814453125,0.12383514404296875,0.1238196533203125,0.12380419921875,0.1237887451171875,0.123773291015625,0.12375784912109375,0.12374241943359375,0.123726953125,0.1237115478515625,0.12369613037109375,0.123680712890625,0.1236653076171875,0.12364990234375,0.1236344970703125,0.123619091796875,0.1236037109375,0.12358834228515625,0.1235729736328125,0.12355760498046875,0.123542236328125,0.12352689208984376,0.1235115478515625,0.12349619140625,0.123480859375,0.1234655029296875,0.1234501953125,0.12343486328125,0.1234195556640625,0.1234042724609375,0.12338896484375,0.1233736572265625,0.1233583740234375,0.1233430908203125,0.1233278076171875,0.123312548828125,0.123297265625]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-1479964318', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres16_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1479964318\"\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotly.JupyterScala.init()\n",
    "Seq(Scatter(lossSeq.indices, lossSeq)).plot(title = \"loss by time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have learned the follows in this article:\n",
    "\n",
    "* Prepare and process CIFAR10 data\n",
    "* Write softmax classifier\n",
    "* Use the prediction image of the neural network written by softmax classifier to match with the probability of each category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
