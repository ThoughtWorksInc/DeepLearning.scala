---
layout: demo
title: SoftmaxLinearClassifier
download_path: demo_download/.
filename: SoftmaxLinearClassifier.ipynb
---
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Background">Background<a class="anchor-link" href="#Background">&#182;</a></h2><p>In this article, we will use <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> classifier to build a simple image classification neural network with an accuracy of 32%. In a Softmax classifier, binary logic is generalized and regressed to multiple logic. Softmax classifier will output the probability of the corresponding category.</p>
<p>We will first define a softmax classifier, then use the training set of <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a> to train the neural network, and finally use the test set to verify the accuracy of the neural network.</p>
<p>Let’s get started.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Import-dependencies">Import dependencies<a class="anchor-link" href="#Import-dependencies">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Like the previous course <a href="https://thoughtworksinc.github.io/DeepLearning.scala/demo/GettingStarted.html">GettingStarted</a>, we need to introduce each class of DeepLearning.scala.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$plugin.$ivy.</span><span class="n">`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableany:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablenothing:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableseq:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiabledouble:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablefloat:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablehlist:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableindarray:1.0.0`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.nd4j:nd4j-native-platform:0.7.2`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.rauschig:jarchivelib:0.5.0`</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.plotly-scala::plotly-jupyter-scala:0.3.0`</span>

<span class="k">import</span> <span class="nn">java.io.</span><span class="o">{</span><span class="nc">FileInputStream</span><span class="o">,</span> <span class="nc">InputStream</span><span class="o">}</span>


<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableHList._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableDouble._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableINDArray._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableAny._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.</span><span class="o">{</span>
  <span class="nc">DifferentiableHList</span><span class="o">,</span>
  <span class="nc">DifferentiableINDArray</span><span class="o">,</span>
  <span class="nc">Layer</span><span class="o">,</span>
  <span class="nc">Symbolic</span>
<span class="o">}</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Layer.Tape</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Symbolic.Layers.Identity</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Symbolic._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathFunctions._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathMethods.</span><span class="o">/</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathOps</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.factory.Nd4j</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.indexing.</span><span class="o">{</span><span class="nc">INDArrayIndex</span><span class="o">,</span> <span class="nc">NDArrayIndex</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.ops.transforms.Transforms</span>
<span class="k">import</span> <span class="nn">org.nd4s.Implicits._</span>
<span class="k">import</span> <span class="nn">shapeless._</span>

<span class="k">import</span> <span class="nn">plotly._</span>
<span class="k">import</span> <span class="nn">plotly.element._</span>
<span class="k">import</span> <span class="nn">plotly.layout._</span>
<span class="k">import</span> <span class="nn">plotly.JupyterScala._</span>

<span class="k">import</span> <span class="nn">scala.collection.immutable.IndexedSeq</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[1]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$plugin.$                                                                             

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                       
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                           
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                       
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                          
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                         
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                         
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                             
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                            
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                    
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                               

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                             

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">java.io.{FileInputStream, InputStream}


</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableHList._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableDouble._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableINDArray._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableAny._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.{
  DifferentiableHList,
  DifferentiableINDArray,
  Layer,
  Symbolic
}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Layer.Tape
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Symbolic.Layers.Identity
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Symbolic._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathFunctions._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathMethods./
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathOps
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.factory.Nd4j
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.ops.transforms.Transforms
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4s.Implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">shapeless._

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.element._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.layout._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.JupyterScala._

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.collection.immutable.IndexedSeq</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To reduce the line numbers outputted by <code>jupyter-scala</code> and to make sure that the page output will not be too long, we need to set <code>pprintConfig</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">pprintConfig</span><span class="o">()</span> <span class="k">=</span> <span class="n">pprintConfig</span><span class="o">().</span><span class="n">copy</span><span class="o">(</span><span class="n">height</span> <span class="k">=</span> <span class="mi">2</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Build-the-neural-network.">Build the neural network.<a class="anchor-link" href="#Build-the-neural-network.">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Write-softmax">Write softmax<a class="anchor-link" href="#Write-softmax">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To use <code>softmax</code> classifier (softmax classifier is a neural network combined by <code>softmax</code> and a full connection), we first need to write softmax function, formula: <img src="https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">softmax</span><span class="o">(</span><span class="k">implicit</span> <span class="n">scores</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">expScores</span> <span class="k">=</span> <span class="n">exp</span><span class="o">(</span><span class="n">scores</span><span class="o">)</span>
  <span class="n">expScores</span> <span class="o">/</span> <span class="n">expScores</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[3]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">softmax</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Set-learning-rate">Set learning rate<a class="anchor-link" href="#Set-learning-rate">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Learning rate need to be set for the full connection layer. Learning rate visually describes the change rate of <code>weight</code>. A too-low learning rate will result in slow decrease of <code>loss</code>, which will require longer time for training; A too-high learning rate will result in rapid decrease of <code>loss</code> at first while fluctuation around the lowest point afterward.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">implicit</span> <span class="k">def</span> <span class="n">optimizer</span><span class="k">:</span> <span class="kt">Optimizer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">LearningRate</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">currentLearningRate</span><span class="o">()</span> <span class="k">=</span> <span class="mf">0.00001</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[4]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">optimizer</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Combine-neural-network">Combine neural network<a class="anchor-link" href="#Combine-neural-network">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Define a full connection layer and <a href="https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization">initialize Weight</a>, <code>Weight</code> shall be a two-dimension <code>INDArray</code> of <code>NumberOfPixels × NumberOfClasses</code>. <code>scores</code> is the score of each image corresponding to each category, representing the feasible probability of each category corresponding to each image.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)</span>
<span class="k">val</span> <span class="nc">NumberOfClasses</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">val</span> <span class="nc">NumberOfPixels</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">3072</span>

<span class="k">def</span> <span class="n">createMyNeuralNetwork</span><span class="o">(</span><span class="k">implicit</span> <span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">initialValueOfWeight</span> <span class="k">=</span> <span class="nc">Nd4j</span><span class="o">.</span><span class="n">randn</span><span class="o">(</span><span class="nc">NumberOfPixels</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span> <span class="o">*</span> <span class="mf">0.001</span>
  <span class="k">val</span> <span class="n">weight</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="n">initialValueOfWeight</span><span class="o">.</span><span class="n">toWeight</span>
  <span class="k">val</span> <span class="n">scores</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="n">input</span> <span class="n">dot</span> <span class="n">weight</span>
  <span class="n">softmax</span><span class="o">.</span><span class="n">compose</span><span class="o">(</span><span class="n">scores</span><span class="o">)</span>
<span class="o">}</span>
<span class="k">val</span> <span class="n">myNeuralNetwork</span> <span class="k">=</span> <span class="n">createMyNeuralNetwork</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stderr output_text">
<pre>SLF4J: Failed to load class &#34;org.slf4j.impl.StaticLoggerBinder&#34;.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[5]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">NumberOfClasses</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">10</span>
<span class="ansi-cyan-fg">NumberOfPixels</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">3072</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">createMyNeuralNetwork</span>
<span class="ansi-cyan-fg">myNeuralNetwork</span>: (<span class="ansi-green-fg">Symbolic</span>.<span class="ansi-green-fg">To</span>[<span class="ansi-green-fg">INDArray</span>]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#<span class="ansi-green-fg">@</span> = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[0.00, -0.00, 0.00, -0.00, -0.00, 0.00, 0.00, <span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Combine-LossFunction">Combine LossFunction<a class="anchor-link" href="#Combine-LossFunction">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To learn about the prediction result of the neural network, we need to write the loss function <code>lossFunction</code>. We use <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy loss</a> to make comparison between this result and the actual result before return the score. Formula:
<img src="https://zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">lossFunction</span><span class="o">(</span><span class="k">implicit</span> <span class="n">pair</span><span class="k">:</span> <span class="o">(</span><span class="kt">INDArray</span> <span class="kt">::</span> <span class="kt">INDArray</span> <span class="kt">::</span> <span class="kt">HNil</span><span class="o">)</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">input</span> <span class="k">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">head</span>
  <span class="k">val</span> <span class="n">expectedOutput</span> <span class="k">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>
  <span class="k">val</span> <span class="n">probabilities</span> <span class="k">=</span> <span class="n">myNeuralNetwork</span><span class="o">.</span><span class="n">compose</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>

  <span class="o">-(</span><span class="n">expectedOutput</span> <span class="o">*</span> <span class="n">log</span><span class="o">(</span><span class="n">probabilities</span><span class="o">)).</span><span class="n">mean</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[6]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">lossFunction</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Prepare-data">Prepare data<a class="anchor-link" href="#Prepare-data">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Read-data">Read data<a class="anchor-link" href="#Read-data">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To read the images and corresponding label information for test data from CIFAR10 database and process them, we need <a href="https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/ReadCIFAR10ToNDArray.sc"><code>import $file.ReadCIFAR10ToNDArray</code></a>. This is a script file containing the read and processed CIFAR10 data, provided in this course.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$file.ReadCIFAR10ToNDArray</span>

<span class="k">val</span> <span class="n">trainNDArray</span> <span class="k">=</span>
  <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="n">readFromResource</span><span class="o">(</span><span class="s">&quot;/cifar-10-batches-bin/data_batch_1.bin&quot;</span><span class="o">,</span> <span class="mi">1000</span><span class="o">)</span>

<span class="k">val</span> <span class="n">testNDArray</span> <span class="k">=</span>
  <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="n">readFromResource</span><span class="o">(</span><span class="s">&quot;/cifar-10-batches-bin/test_batch.bin&quot;</span><span class="o">,</span> <span class="mi">100</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Compiling ReadCIFAR10ToNDArray.sc
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[7]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$file.$                   

</span>
<span class="ansi-cyan-fg">trainNDArray</span>: <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">HNil</span> = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">testNDArray</span>: <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">HNil</span> = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Process-data">Process data<a class="anchor-link" href="#Process-data">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before passing data to the softmax classifier, we first process label data with (<a href="https://en.wikipedia.org/wiki/One-hot">one hot encoding</a>): transform INDArray of <code>NumberOfPixels × 1</code> into INDArray of <code>NumberOfPixels × NumberOfClasses</code>. The value of correct classification corresponding to each line is 1, and the values of other columns are 0. The reason for differentiating the training set and test set is to make it clear that whether the network is over trained which leads to <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>. While processing label data, we used <a href="https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/Utils.sc">Utils</a>, which is also provided in this course.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">trainData</span> <span class="k">=</span> <span class="n">trainNDArray</span><span class="o">.</span><span class="n">head</span>
<span class="k">val</span> <span class="n">testData</span> <span class="k">=</span> <span class="n">testNDArray</span><span class="o">.</span><span class="n">head</span>

<span class="k">val</span> <span class="n">trainExpectResult</span> <span class="k">=</span> <span class="n">trainNDArray</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>
<span class="k">val</span> <span class="n">testExpectResult</span> <span class="k">=</span> <span class="n">testNDArray</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>

<span class="k">import</span> <span class="nn">$file.Utils</span>

<span class="k">val</span> <span class="n">vectorizedTrainExpectResult</span> <span class="k">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">makeVectorized</span><span class="o">(</span><span class="n">trainExpectResult</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span>
<span class="k">val</span> <span class="n">vectorizedTestExpectResult</span> <span class="k">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">makeVectorized</span><span class="o">(</span><span class="n">testExpectResult</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Compiling Utils.sc
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[8]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">trainData</span>: <span class="ansi-green-fg">INDArray</span> = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">testData</span>: <span class="ansi-green-fg">INDArray</span> = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">trainExpectResult</span>: <span class="ansi-green-fg">INDArray</span> = [6.00, 9.00, 9.00, 4.00, 1.00, 1.00, 2.00, 7.00, 8.00, 3.00, 4.00, 7.00, 7.00, 2.00, 9.00, 9.00, 9.00, 3.00, 2.00, 6.00, 4.00, 3.00, 6.00, 6.00, 2.00, 6.00, 3.0<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">testExpectResult</span>: <span class="ansi-green-fg">INDArray</span> = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0<span class="ansi-yellow-fg">...</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$file.$    

</span>
<span class="ansi-cyan-fg">vectorizedTrainExpectResult</span>: <span class="ansi-green-fg">INDArray</span> = [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],
<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">vectorizedTestExpectResult</span>: <span class="ansi-green-fg">INDArray</span> = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],
<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-the-neural-network">Train the neural network<a class="anchor-link" href="#Train-the-neural-network">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To observe the training process of the neural network, we need to output <code>loss</code>; while training the neural network, the <code>loss</code> shall be deceasing.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">lossSeq</span> <span class="k">=</span> <span class="k">for</span> <span class="o">(</span><span class="n">iteration</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="mi">2000</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">loss</span> <span class="k">=</span> <span class="n">lossFunction</span><span class="o">.</span><span class="n">train</span><span class="o">(</span><span class="n">trainData</span> <span class="o">::</span> <span class="n">vectorizedTrainExpectResult</span> <span class="o">::</span> <span class="nc">HNil</span><span class="o">)</span>
  <span class="k">if</span><span class="o">(</span><span class="n">iteration</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="o">){</span>
    <span class="n">println</span><span class="o">(</span><span class="s">s&quot;at iteration </span><span class="si">$iteration</span><span class="s"> loss is </span><span class="si">$loss</span><span class="s">&quot;</span><span class="o">)</span>
  <span class="o">}</span>
  <span class="n">loss</span>
<span class="o">}</span>

<span class="n">plotly</span><span class="o">.</span><span class="nc">JupyterScala</span><span class="o">.</span><span class="n">init</span><span class="o">()</span>

<span class="k">val</span> <span class="n">plot</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
  <span class="nc">Scatter</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">.</span><span class="n">indices</span><span class="o">,</span> <span class="n">lossSeq</span><span class="o">)</span>
<span class="o">)</span>

<span class="n">plot</span><span class="o">.</span><span class="n">plot</span><span class="o">(</span>
  <span class="n">title</span> <span class="k">=</span> <span class="s">&quot;loss by time&quot;</span>
<span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>at iteration 0 loss is 0.23022568359375
at iteration 100 loss is 0.1909324462890625
at iteration 200 loss is 0.1782318603515625
at iteration 300 loss is 0.17026954345703124
at iteration 400 loss is 0.1643105224609375
at iteration 500 loss is 0.159467919921875
at iteration 600 loss is 0.155342822265625
at iteration 700 loss is 0.15172296142578126
at iteration 800 loss is 0.1484817626953125
at iteration 900 loss is 0.145537353515625
at iteration 1000 loss is 0.14283314208984374
at iteration 1100 loss is 0.14032823486328125
at iteration 1200 loss is 0.13799169921875
at iteration 1300 loss is 0.13579979248046875
at iteration 1400 loss is 0.1337334716796875
at iteration 1500 loss is 0.1317774658203125
at iteration 1600 loss is 0.12991929931640625
at iteration 1700 loss is 0.12814844970703124
at iteration 1800 loss is 0.12645621337890625
at iteration 1900 loss is 0.12483504638671875
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">

      <script type="text/javascript">
        require.config({
  paths: {
    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',
    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'
  },

  shim: {
    plotly: {
      deps: ['d3', 'jquery'],
      exports: 'plotly'
    }
  }
});
        

        require(['plotly'], function(Plotly) {
          window.Plotly = Plotly;
        });
      </script>
    
</div>

</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div class="chart" id="plot-1030091756"></div>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>




<div id="ddff95c4-ee1b-4546-b14d-fa4313aaba7a"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#ddff95c4-ee1b-4546-b14d-fa4313aaba7a');
requirejs(["plotly"], function(Plotly) {
  (function () {
  var data0 = {"type":"scatter","x":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],"y":[0.23022568359375,0.2286566162109375,0.2276935791015625,0.2267984130859375,0.22593359375,0.2250947509765625,0.2242803955078125,0.22348935546875,0.222720751953125,0.221973583984375,0.22124697265625,0.2205400390625,0.219852099609375,0.2191822265625,0.21852978515625,0.217894091796875,0.2172743896484375,0.2166700927734375,0.21608056640625,0.215505224609375,0.2149435791015625,0.2143949951171875,0.2138591064453125,0.2133353515625,0.212823291015625,0.2123224609375,0.211832470703125,0.211352978515625,0.210883544921875,0.210423828125,0.209973486328125,0.20953212890625,0.209099560546875,0.208675439453125,0.2082593994140625,0.207851318359375,0.2074508056640625,0.2070576416015625,0.2066716796875,0.2062925537109375,0.2059201904296875,0.20555419921875,0.20519453125,0.2048409423828125,0.20449324951171874,0.2041512939453125,0.20381484375,0.2034837646484375,0.20315797119140624,0.20283720703125,0.20252138671875,0.202210400390625,0.201904052734375,0.2016022216796875,0.2013048095703125,0.20101168212890624,0.2007227294921875,0.20043782958984374,0.20015689697265626,0.1998798095703125,0.199606494140625,0.1993367919921875,0.199070703125,0.198808056640625,0.1985488037109375,0.198292919921875,0.19804017333984375,0.1977906005859375,0.197544091796875,0.197300634765625,0.19706005859375,0.1968223876953125,0.19658748779296875,0.19635531005859375,0.1961258056640625,0.19589891357421876,0.195674560546875,0.1954527587890625,0.195233349609375,0.19501634521484376,0.19480164794921875,0.19458929443359374,0.19437916259765625,0.19417119140625,0.19396541748046875,0.1937616943359375,0.1935600830078125,0.19336051025390624,0.1931628662109375,0.1929671875,0.192773388671875,0.19258145751953126,0.1923913818359375,0.1922030517578125,0.19201651611328124,0.19183173828125,0.1916485595703125,0.19146708984375,0.19128729248046875,0.1911091064453125,0.1909324462890625,0.19075732421875,0.190583740234375,0.1904116455078125,0.1902409912109375,0.19007177734375,0.18990401611328125,0.18973760986328125,0.1895725830078125,0.1894088623046875,0.1892465087890625,0.18908546142578125,0.18892569580078125,0.188767138671875,0.188609814453125,0.1884537109375,0.188298828125,0.1881451416015625,0.1879925537109375,0.18784114990234374,0.18769085693359375,0.1875416259765625,0.187393505859375,0.187246484375,0.18710048828125,0.18695555419921875,0.1868115966796875,0.186668701171875,0.186526806640625,0.1863858154296875,0.186245849609375,0.186106787109375,0.1859687255859375,0.1858315185546875,0.185695263671875,0.1855598876953125,0.1854254150390625,0.18529180908203124,0.18515904541015624,0.1850271728515625,0.18489609375,0.1847658203125,0.184636474609375,0.1845078125,0.18437996826171876,0.1842529541015625,0.18412666015625,0.184001171875,0.183876416015625,0.1837523681640625,0.1836291015625,0.183506494140625,0.183384619140625,0.1832634765625,0.1831430419921875,0.183023291015625,0.1829042236328125,0.18278585205078124,0.18266806640625,0.18255098876953124,0.1824345703125,0.1823187744140625,0.18220357666015624,0.182089013671875,0.18197509765625,0.1818617919921875,0.18174908447265625,0.1816368896484375,0.18152540283203125,0.1814144775390625,0.1813040771484375,0.1811942626953125,0.1810850830078125,0.18097633056640625,0.180868212890625,0.18076063232421874,0.180653564453125,0.1805470703125,0.18044110107421876,0.1803355712890625,0.1802306640625,0.180126171875,0.18002227783203126,0.1799187744140625,0.1798158447265625,0.17971341552734374,0.17961142578125,0.1795099609375,0.1794089111328125,0.17930830078125,0.179208251953125,0.17910859375,0.1790093994140625,0.17891072998046875,0.17881240234375,0.17871458740234375,0.1786171875,0.1785202392578125,0.178423681640625,0.178327587890625,0.1782318603515625,0.1781365478515625,0.178041650390625,0.1779472412109375,0.177853173828125,0.17775950927734374,0.177666259765625,0.17757337646484375,0.1774809326171875,0.177388818359375,0.177297119140625,0.17720576171875,0.1771148193359375,0.17702423095703124,0.176933984375,0.176844140625,0.1767546875,0.1766655517578125,0.1765767822265625,0.1764883544921875,0.176400244140625,0.17631253662109375,0.1762251220703125,0.17613809814453124,0.17605140380859374,0.17596502685546875,0.175878955078125,0.17579327392578126,0.17570789794921876,0.1756228515625,0.17553807373046876,0.175453662109375,0.1753695068359375,0.175285693359375,0.17520220947265625,0.17511904296875,0.17503616943359376,0.1749535888671875,0.1748713134765625,0.174789306640625,0.1747076416015625,0.1746262451171875,0.1745451416015625,0.17446435546875,0.1743838134765625,0.17430355224609376,0.1742236083984375,0.174143896484375,0.174064501953125,0.1739852783203125,0.173906494140625,0.17382789306640625,0.17374957275390626,0.173671484375,0.1735936767578125,0.1735161865234375,0.173438916015625,0.173361865234375,0.173285107421875,0.1732086181640625,0.17313233642578124,0.1730563720703125,0.17298057861328126,0.172905078125,0.17282982177734374,0.17275478515625,0.17268001708984376,0.1726054931640625,0.17253115234375,0.172457080078125,0.1723832763671875,0.17230966796875,0.17223631591796876,0.1721631591796875,0.172090283203125,0.172017578125,0.1719451171875,0.1718728515625,0.17180086669921876,0.17172904052734375,0.1716574951171875,0.1715861083984375,0.171514990234375,0.17144404296875,0.1713733154296875,0.17130283203125,0.17123251953125,0.1711624267578125,0.1710925048828125,0.17102286376953124,0.1709533203125,0.17088411865234376,0.1708150146484375,0.1707461181640625,0.17067750244140625,0.1706090087890625,0.17054072265625,0.1704726318359375,0.17040474853515625,0.17033704833984376,0.17026954345703124,0.17020220947265624,0.1701350830078125,0.170068115234375,0.17000137939453125,0.169934814453125,0.1698684326171875,0.169802197265625,0.169736181640625,0.1696703125,0.169604638671875,0.16953916015625,0.169473828125,0.169408740234375,0.1693437255859375,0.169278955078125,0.16921434326171875,0.1691498779296875,0.16908560791015625,0.169021484375,0.1689575439453125,0.16889378662109375,0.1688301513671875,0.16876673583984375,0.16870347900390625,0.1686403564453125,0.16857738037109374,0.16851458740234376,0.168451953125,0.168389501953125,0.1683271728515625,0.168264990234375,0.16820302734375,0.16814117431640624,0.1680794921875,0.16801798095703124,0.167956591796875,0.1678953369140625,0.16783426513671876,0.1677733642578125,0.16771256103515625,0.1676519287109375,0.16759149169921875,0.1675311279296875,0.16747095947265625,0.167410888671875,0.1673509765625,0.167291259765625,0.1672316162109375,0.16717215576171876,0.1671128173828125,0.1670536376953125,0.1669945556640625,0.16693565673828126,0.166876904296875,0.16681824951171875,0.1667597412109375,0.16670137939453125,0.1666431396484375,0.16658504638671875,0.16652711181640625,0.1664692626953125,0.166411572265625,0.16635400390625,0.16629658203125,0.1662392578125,0.1661821044921875,0.166125,0.166068115234375,0.166011328125,0.1659547119140625,0.1658981201171875,0.1658417236328125,0.1657854248046875,0.16572926025390625,0.16567322998046874,0.1656173095703125,0.16556153564453124,0.16550582275390624,0.16545030517578124,0.165394873046875,0.16533956298828126,0.165284326171875,0.16522923583984375,0.1651742919921875,0.16511943359375,0.16506473388671875,0.16501011962890624,0.1649556396484375,0.16490126953125,0.164847021484375,0.16479283447265625,0.164738818359375,0.164684912109375,0.16463106689453125,0.16457734375,0.164523779296875,0.1644703125,0.164416943359375,0.16436365966796876,0.1643105224609375,0.164257470703125,0.164204541015625,0.1641516845703125,0.164098974609375,0.1640463623046875,0.16399384765625,0.163941455078125,0.16388916015625,0.163836962890625,0.16378486328125,0.16373287353515625,0.1636809814453125,0.16362921142578124,0.16357752685546875,0.163525927734375,0.163474462890625,0.16342308349609375,0.16337176513671875,0.16332060546875,0.1632695068359375,0.1632185546875,0.16316766357421875,0.16311683349609374,0.16306617431640624,0.16301558837890626,0.16296512451171874,0.16291466064453125,0.1628643798828125,0.16281416015625,0.16276405029296875,0.16271402587890624,0.1626640380859375,0.1626142333984375,0.16256448974609375,0.16251485595703125,0.16246529541015625,0.16241580810546874,0.16236640625,0.1623171875,0.16226796875,0.1622188232421875,0.16216986083984375,0.1621208984375,0.162072021484375,0.16202333984375,0.16197464599609376,0.1619260498046875,0.1618775634765625,0.16182919921875,0.1617808349609375,0.1617326416015625,0.1616844970703125,0.16163642578125,0.16158843994140626,0.16154058837890625,0.1614927734375,0.1614450439453125,0.1613974853515625,0.1613498779296875,0.16130244140625,0.161255029296875,0.16120771484375,0.1611605224609375,0.16111336669921875,0.1610662841796875,0.1610193115234375,0.1609724365234375,0.1609256103515625,0.1608788818359375,0.16083218994140625,0.16078564453125,0.16073916015625,0.16069271240234376,0.1606463623046875,0.1606000732421875,0.16055394287109376,0.1605077880859375,0.1604617431640625,0.160415771484375,0.1603699462890625,0.16032412109375,0.1602783935546875,0.16023275146484375,0.160187158203125,0.160141650390625,0.1600962158203125,0.1600508544921875,0.160005615234375,0.1599603759765625,0.1599152099609375,0.15987017822265626,0.15982520751953125,0.15978028564453126,0.1597354248046875,0.15969068603515624,0.1596459716796875,0.1596013671875,0.15955682373046876,0.1595122802734375,0.159467919921875,0.15942354736328124,0.159379296875,0.15933509521484376,0.15929091796875,0.15924686279296876,0.1592028564453125,0.15915892333984374,0.15911510009765625,0.15907132568359375,0.15902757568359374,0.1589839111328125,0.15894033203125,0.1588968017578125,0.1588533447265625,0.1588099609375,0.1587666748046875,0.1587234375,0.1586802001953125,0.158637109375,0.15859398193359375,0.1585510498046875,0.15850810546875,0.158465283203125,0.1584224609375,0.1583797119140625,0.158337060546875,0.15829447021484375,0.1582519287109375,0.1582094482421875,0.15816700439453124,0.1581246826171875,0.15808238525390625,0.158040185546875,0.157997998046875,0.15795589599609375,0.15791385498046875,0.1578718994140625,0.15782996826171874,0.1577880859375,0.157746337890625,0.15770458984375,0.1576629150390625,0.1576212890625,0.15757974853515624,0.1575382568359375,0.1574968505859375,0.157455419921875,0.1574141357421875,0.1573728515625,0.157331689453125,0.1572905029296875,0.157249462890625,0.157208447265625,0.15716749267578126,0.15712657470703126,0.1570857177734375,0.15704495849609376,0.15700418701171875,0.15696353759765624,0.1569229248046875,0.1568823486328125,0.156841845703125,0.156801416015625,0.1567610595703125,0.1567206787109375,0.1566803955078125,0.156640185546875,0.1566,0.15655986328125,0.156519873046875,0.15647982177734376,0.156439892578125,0.15640001220703126,0.15636015625,0.1563203857421875,0.15628062744140625,0.1562409423828125,0.156201318359375,0.15616177978515625,0.1561221923828125,0.156082763671875,0.1560433349609375,0.15600400390625,0.155964697265625,0.1559254150390625,0.1558862548828125,0.1558470947265625,0.1558080078125,0.15576895751953124,0.1557299560546875,0.1556909912109375,0.1556521240234375,0.15561326904296874,0.1555744873046875,0.1555357666015625,0.1554970703125,0.15545845947265624,0.15541982421875,0.15538128662109374,0.155342822265625,0.1553043701171875,0.1552659912109375,0.15522769775390624,0.15518936767578126,0.1551510986328125,0.155112939453125,0.15507481689453126,0.15503671875,0.1549986572265625,0.15496064453125,0.1549227294921875,0.1548848388671875,0.154847021484375,0.1548091552734375,0.15477142333984376,0.1547337158203125,0.1546960205078125,0.15465848388671874,0.1546208740234375,0.154583349609375,0.1545458740234375,0.154508447265625,0.15447109375,0.1544337646484375,0.15439647216796876,0.15435921630859376,0.1543219970703125,0.1542849365234375,0.15424781494140624,0.15421072998046875,0.1541737060546875,0.15413673095703126,0.15409984130859375,0.1540629638671875,0.15402613525390624,0.15398935546875,0.1539526123046875,0.15391591796875,0.15387926025390625,0.1538426513671875,0.153806103515625,0.153769580078125,0.15373310546875,0.1536966552734375,0.1536602783203125,0.15362396240234374,0.153587646484375,0.1535513916015625,0.1535152099609375,0.15347901611328124,0.15344287109375,0.153406787109375,0.153370751953125,0.15333472900390624,0.1532988037109375,0.1532628662109375,0.153227001953125,0.1531911865234375,0.15315537109375,0.1531196533203125,0.153083984375,0.15304830322265625,0.1530126708984375,0.15297711181640625,0.15294156494140626,0.1529060791015625,0.15287060546875,0.1528352294921875,0.1527998291015625,0.15276446533203125,0.152729248046875,0.1526939453125,0.15265872802734376,0.152623583984375,0.152588427734375,0.1525533447265625,0.15251829833984376,0.1524832763671875,0.15244830322265626,0.15241337890625,0.1523784912109375,0.15234364013671875,0.1523088134765625,0.1522740478515625,0.15223934326171876,0.1522046630859375,0.15216998291015624,0.1521353515625,0.15210081787109375,0.15206622314453125,0.1520317626953125,0.151997265625,0.1519628662109375,0.15192843017578125,0.15189410400390624,0.15185980224609374,0.15182550048828125,0.1517913330078125,0.15175711669921876,0.15172296142578126,0.1516888427734375,0.1516547607421875,0.151620751953125,0.151586669921875,0.15155272216796875,0.15151878662109375,0.1514848876953125,0.1514510009765625,0.1514172119140625,0.15138341064453126,0.151349658203125,0.15131591796875,0.1512822509765625,0.15124862060546876,0.1512150146484375,0.15118143310546875,0.15114791259765625,0.15111439208984376,0.15108092041015625,0.15104749755859376,0.151014111328125,0.150980810546875,0.1509474365234375,0.1509141845703125,0.15088089599609375,0.1508476806640625,0.15081451416015626,0.150781396484375,0.1507482666015625,0.150715185546875,0.1506821533203125,0.150649169921875,0.15061619873046875,0.1505832763671875,0.1505503173828125,0.15051748046875,0.15048463134765624,0.15045185546875,0.150419091796875,0.1503863525390625,0.15035364990234376,0.150320947265625,0.15028834228515625,0.1502557373046875,0.150223193359375,0.1501906982421875,0.1501581787109375,0.150125732421875,0.1500932861328125,0.1500609130859375,0.15002855224609374,0.14999620361328125,0.1499638916015625,0.149931640625,0.1498994384765625,0.1498672119140625,0.14983504638671874,0.149802880859375,0.1497708251953125,0.14973873291015624,0.14970673828125,0.14967471923828124,0.14964278564453126,0.14961082763671876,0.1495788818359375,0.1495469970703125,0.149515185546875,0.1494833740234375,0.1494515869140625,0.149419873046875,0.149388134765625,0.14935645751953125,0.14932476806640624,0.14929315185546874,0.149261572265625,0.149230029296875,0.1491985107421875,0.1491669921875,0.149135546875,0.1491041015625,0.1490726806640625,0.14904132080078125,0.1490099365234375,0.14897864990234375,0.14894736328125,0.14891611328125,0.14888486328125,0.1488537109375,0.14882254638671874,0.14879140625,0.148760302734375,0.1487292724609375,0.14869818115234376,0.1486672119140625,0.1486362060546875,0.1486052490234375,0.14857431640625,0.1485434326171875,0.1485125732421875,0.1484817626953125,0.14845098876953125,0.14842017822265624,0.148389453125,0.1483587158203125,0.14832802734375,0.1482973876953125,0.148266748046875,0.1482361572265625,0.14820560302734376,0.1481750732421875,0.1481445068359375,0.14811407470703125,0.14808359375,0.14805316162109375,0.1480227783203125,0.1479923828125,0.147962060546875,0.147931787109375,0.14790150146484374,0.14787120361328124,0.1478409912109375,0.1478108154296875,0.14778065185546874,0.1477504638671875,0.14772041015625,0.1476903076171875,0.14766025390625,0.1476302001953125,0.147600244140625,0.14757021484375,0.147540283203125,0.1475103759765625,0.1474804931640625,0.147450634765625,0.14742078857421875,0.1473909423828125,0.1473612060546875,0.1473314208984375,0.14730166015625,0.1472719970703125,0.147242333984375,0.1472126708984375,0.14718302001953126,0.14715343017578125,0.14712388916015626,0.1470943115234375,0.147064794921875,0.1470353271484375,0.1470058837890625,0.146976416015625,0.1469470458984375,0.14691767578125,0.146888330078125,0.146858984375,0.1468296630859375,0.146800390625,0.146771142578125,0.1467419189453125,0.14671270751953125,0.146683544921875,0.14665439453125,0.1466252685546875,0.14659619140625,0.146567138671875,0.1465380615234375,0.1465090576171875,0.14648009033203124,0.14645111083984375,0.1464221435546875,0.14639326171875,0.14636435546875,0.14633551025390626,0.146306640625,0.14627784423828125,0.146249072265625,0.14622027587890624,0.1461915283203125,0.14616280517578126,0.146134130859375,0.1461054931640625,0.14607684326171874,0.1460482177734375,0.14601962890625,0.14599105224609374,0.1459625244140625,0.1459340087890625,0.145905517578125,0.1458770751953125,0.1458486328125,0.14582021484375,0.145791796875,0.14576343994140625,0.14573509521484376,0.1457067626953125,0.14567845458984374,0.1456501953125,0.14562193603515625,0.14559375,0.1455655029296875,0.145537353515625,0.14550921630859376,0.14548106689453125,0.1454529541015625,0.1454248779296875,0.1453968505859375,0.1453687744140625,0.14534078369140624,0.1453127685546875,0.1452848388671875,0.145256884765625,0.1452289794921875,0.14520107421875,0.14517318115234376,0.14514537353515625,0.1451175537109375,0.14508974609375,0.1450619873046875,0.1450342041015625,0.145006494140625,0.14497880859375,0.14495111083984374,0.1449234375,0.1448958251953125,0.144868212890625,0.14484061279296875,0.144813037109375,0.144785498046875,0.14475797119140624,0.1447304443359375,0.144702978515625,0.14467548828125,0.1446480712890625,0.1446206787109375,0.1445932861328125,0.14456591796875,0.1445385498046875,0.144511279296875,0.1444839599609375,0.1444566650390625,0.14442939453125,0.14440218505859376,0.144374951171875,0.1443477783203125,0.14432061767578125,0.1442934814453125,0.144266357421875,0.144239208984375,0.14421217041015624,0.1441851318359375,0.144158056640625,0.1441310302734375,0.14410404052734374,0.144077099609375,0.14405013427734376,0.1440232177734375,0.14399632568359375,0.14396943359375,0.14394256591796875,0.1439156982421875,0.14388890380859376,0.14386212158203124,0.14383531494140625,0.14380855712890625,0.1437818359375,0.14375513916015625,0.14372838134765625,0.1437017333984375,0.14367508544921875,0.143648486328125,0.14362183837890624,0.14359525146484375,0.14356871337890625,0.1435421630859375,0.143515625,0.1434891357421875,0.143462646484375,0.143436181640625,0.1434097412109375,0.1433833251953125,0.14335697021484375,0.14333052978515626,0.1433041748046875,0.14327781982421875,0.1432515380859375,0.1432252197265625,0.14319893798828126,0.14317271728515624,0.1431464599609375,0.14312025146484375,0.1430940185546875,0.1430678466796875,0.1430417236328125,0.14301552734375,0.1429894287109375,0.1429633544921875,0.14293726806640625,0.1429112060546875,0.142885205078125,0.14285911865234374,0.14283314208984374,0.1428072021484375,0.1427812255859375,0.1427552734375,0.14272935791015626,0.1427034912109375,0.14267757568359374,0.1426517822265625,0.1426259033203125,0.14260003662109375,0.14257425537109375,0.142548486328125,0.1425227294921875,0.14249698486328125,0.1424712646484375,0.14244556884765625,0.14241990966796875,0.14239423828125,0.1423685791015625,0.14234295654296875,0.142317333984375,0.142291748046875,0.14226617431640626,0.142240625,0.1422150634765625,0.1421895751953125,0.1421640869140625,0.142138623046875,0.14211318359375,0.1420877685546875,0.1420623291015625,0.1420368896484375,0.14201151123046876,0.141986181640625,0.1419608154296875,0.141935498046875,0.1419102294921875,0.141884912109375,0.14185966796875,0.14183438720703126,0.14180919189453126,0.141783984375,0.1417588134765625,0.1417336181640625,0.1417084228515625,0.14168330078125,0.14165816650390625,0.14163309326171875,0.1416080078125,0.14158291015625,0.14155791015625,0.1415328857421875,0.141507861328125,0.14148289794921876,0.14145792236328125,0.14143294677734375,0.1414080078125,0.1413830810546875,0.14135819091796875,0.1413333251953125,0.14130843505859375,0.1412836181640625,0.1412587646484375,0.141233984375,0.14120916748046874,0.141184375,0.14115963134765624,0.14113486328125,0.14111019287109375,0.1410854736328125,0.141060791015625,0.14103609619140625,0.14101143798828125,0.14098681640625,0.1409621826171875,0.14093759765625,0.1409130126953125,0.1408884521484375,0.14086392822265625,0.14083939208984375,0.14081484375,0.14079036865234376,0.14076588134765625,0.1407414306640625,0.140716943359375,0.140692578125,0.1406681640625,0.14064376220703126,0.1406194091796875,0.1405950439453125,0.1405706787109375,0.14054635009765626,0.1405220703125,0.1404977783203125,0.140473486328125,0.14044923095703124,0.14042501220703124,0.14040078125,0.1403765869140625,0.140352392578125,0.14032823486328125,0.1403041015625,0.14027994384765624,0.140255810546875,0.14023172607421874,0.14020762939453124,0.14018358154296875,0.14015950927734375,0.14013544921875,0.1401114013671875,0.14008743896484374,0.1400634033203125,0.1400394775390625,0.1400155517578125,0.1399915771484375,0.13996766357421875,0.13994375,0.139919873046875,0.13989599609375,0.13987216796875,0.139848291015625,0.13982447509765625,0.1398006591796875,0.1397768798828125,0.139753076171875,0.13972930908203124,0.1397056396484375,0.13968187255859374,0.139658154296875,0.13963448486328126,0.139610791015625,0.13958712158203124,0.1395634765625,0.1395398681640625,0.13951622314453124,0.13949261474609376,0.1394690673828125,0.1394454833984375,0.13942193603515626,0.13939840087890626,0.13937484130859376,0.1393513671875,0.139327880859375,0.13930440673828126,0.1392809326171875,0.1392574951171875,0.1392341064453125,0.139210693359375,0.1391873046875,0.139163916015625,0.1391405517578125,0.13911719970703126,0.1390938720703125,0.1390705322265625,0.1390472412109375,0.1390239501953125,0.13900068359375,0.13897744140625,0.13895421142578124,0.13893096923828124,0.1389077392578125,0.13888455810546874,0.1388614013671875,0.138838232421875,0.138815087890625,0.1387919677734375,0.1387688232421875,0.13874571533203126,0.13872261962890625,0.13869957275390624,0.138676513671875,0.1386534423828125,0.1386304443359375,0.13860740966796875,0.1385844482421875,0.13856143798828124,0.138538525390625,0.1385155517578125,0.138492578125,0.1384697021484375,0.1384468017578125,0.138423876953125,0.1384010009765625,0.13837813720703124,0.138355322265625,0.13833248291015626,0.138309619140625,0.1382868408203125,0.1382640625,0.1382412841796875,0.1382185302734375,0.138195751953125,0.1381730224609375,0.138150341796875,0.1381276123046875,0.13810491943359374,0.138082275390625,0.138059619140625,0.13803695068359376,0.13801431884765625,0.13799169921875,0.13796912841796874,0.137946533203125,0.1379239501953125,0.13790142822265625,0.137878857421875,0.13785634765625,0.137833837890625,0.1378113525390625,0.137788818359375,0.1377663818359375,0.1377439208984375,0.13772147216796876,0.1376990478515625,0.1376766357421875,0.13765419921875,0.1376318359375,0.13760943603515624,0.137587109375,0.1375647705078125,0.13754241943359374,0.1375201416015625,0.1374978515625,0.13747554931640624,0.137453271484375,0.1374309814453125,0.1374087646484375,0.1373865234375,0.137364306640625,0.13734208984375,0.1373198974609375,0.13729775390625,0.137275537109375,0.13725341796875,0.13723126220703125,0.13720914306640625,0.13718704833984374,0.13716494140625,0.13714288330078125,0.13712078857421875,0.1370987548828125,0.13707666015625,0.1370546875,0.137032666015625,0.1370106689453125,0.1369886962890625,0.1369667236328125,0.1369447509765625,0.13692281494140626,0.1369009033203125,0.136878955078125,0.13685706787109375,0.1368351806640625,0.13681331787109374,0.1367914306640625,0.1367695556640625,0.1367477783203125,0.136725927734375,0.1367041259765625,0.1366822998046875,0.1366605224609375,0.13663875732421876,0.13661697998046876,0.1365952392578125,0.136573486328125,0.136551806640625,0.1365301025390625,0.1365084228515625,0.1364866943359375,0.13646505126953126,0.1364433837890625,0.13642174072265625,0.136400146484375,0.13637855224609374,0.1363569091796875,0.13633533935546874,0.1363137451171875,0.1362921630859375,0.136270654296875,0.13624910888671876,0.13622760009765625,0.1362060546875,0.1361845458984375,0.1361630615234375,0.1361416259765625,0.1361201171875,0.1360987060546875,0.13607724609375,0.13605584716796876,0.13603443603515625,0.1360130615234375,0.13599166259765624,0.1359703125,0.1359489013671875,0.13592760009765625,0.13590626220703125,0.13588489990234376,0.13586363525390624,0.13584234619140625,0.135821044921875,0.13579979248046875,0.13577850341796874,0.135757275390625,0.1357360595703125,0.13571480712890624,0.13569364013671875,0.13567244873046874,0.13565123291015624,0.135630078125,0.1356089111328125,0.13558778076171876,0.1355666259765625,0.1355455322265625,0.1355244140625,0.13550333251953126,0.135482275390625,0.1354611572265625,0.1354401123046875,0.13541905517578126,0.135398046875,0.135377001953125,0.1353559814453125,0.13533502197265626,0.135314013671875,0.135293017578125,0.1352720703125,0.135251123046875,0.13523021240234376,0.13520926513671874,0.135188330078125,0.1351674560546875,0.1351465576171875,0.13512568359375,0.135104833984375,0.13508399658203124,0.1350631591796875,0.13504229736328124,0.1350215087890625,0.13500064697265626,0.1349798828125,0.134959130859375,0.1349383544921875,0.134917578125,0.1348968017578125,0.134876123046875,0.134855419921875,0.1348346923828125,0.13481396484375,0.1347932861328125,0.1347726318359375,0.13475194091796874,0.1347312744140625,0.1347106689453125,0.1346900390625,0.13466943359375,0.13464879150390624,0.13462822265625,0.1346076416015625,0.1345870849609375,0.1345665283203125,0.13454595947265624,0.1345254150390625,0.1345049072265625,0.1344843994140625,0.134463916015625,0.13444344482421874,0.1344229248046875,0.13440245361328124,0.13438201904296876,0.134361572265625,0.13434114990234375,0.134320751953125,0.1343003173828125,0.1342799072265625,0.13425953369140625,0.13423916015625,0.134218798828125,0.1341984619140625,0.134178125,0.13415777587890626,0.134137451171875,0.134117138671875,0.134096875,0.1340765869140625,0.134056298828125,0.13403607177734375,0.13401580810546876,0.13399554443359374,0.13397532958984376,0.13395509033203126,0.133934912109375,0.1339147216796875,0.13389454345703125,0.1338743408203125,0.1338541748046875,0.1338340576171875,0.1338138916015625,0.133793798828125,0.133773681640625,0.133753564453125,0.1337334716796875,0.13371339111328126,0.13369329833984375,0.1336732666015625,0.1336531982421875,0.1336331787109375,0.13361314697265625,0.133593115234375,0.133573095703125,0.133553125,0.1335331298828125,0.1335131591796875,0.13349322509765624,0.13347325439453125,0.1334532958984375,0.1334333740234375,0.133413427734375,0.13339356689453125,0.13337364501953125,0.13335374755859375,0.1333339111328125,0.1333140380859375,0.1332941650390625,0.13327431640625,0.1332544921875,0.13323466796875,0.1332148681640625,0.133195068359375,0.1331752685546875,0.1331554931640625,0.13313572998046874,0.1331159423828125,0.13309620361328126,0.13307646484375,0.1330567626953125,0.1330370361328125,0.1330173095703125,0.13299761962890624,0.1329779052734375,0.132958251953125,0.13293857421875,0.13291893310546876,0.13289927978515625,0.1328796875,0.13286005859375,0.1328404296875,0.13282086181640626,0.1328012451171875,0.132781640625,0.13276209716796875,0.1327425048828125,0.1327229736328125,0.13270341796875,0.13268392333984375,0.13266439208984376,0.13264488525390625,0.13262537841796876,0.13260589599609374,0.13258641357421874,0.1325669677734375,0.132547509765625,0.132528076171875,0.1325086181640625,0.13248919677734375,0.1324697998046875,0.1324503662109375,0.13243096923828124,0.132411572265625,0.1323922119140625,0.1323728515625,0.1323534912109375,0.1323341552734375,0.1323148193359375,0.1322954833984375,0.132276220703125,0.1322568603515625,0.1322375732421875,0.1322182861328125,0.13219898681640624,0.1321797607421875,0.1321604736328125,0.132141259765625,0.1321220458984375,0.132102783203125,0.1320836181640625,0.1320643798828125,0.1320451904296875,0.1320260009765625,0.1320068115234375,0.1319876708984375,0.1319685302734375,0.1319493408203125,0.131930224609375,0.13191112060546875,0.1318919921875,0.1318728759765625,0.1318537841796875,0.131834716796875,0.13181563720703124,0.1317965576171875,0.1317774658203125,0.131758447265625,0.13173941650390625,0.13172041015625,0.1317013671875,0.131682373046875,0.13166337890625,0.13164437255859374,0.131625390625,0.1316064208984375,0.131587451171875,0.131568505859375,0.1315495849609375,0.13153065185546875,0.131511767578125,0.131492822265625,0.13147392578125,0.131455029296875,0.1314361572265625,0.13141722412109375,0.131398388671875,0.1313795654296875,0.1313606689453125,0.131341845703125,0.1313230224609375,0.131304248046875,0.131285400390625,0.1312666259765625,0.1312478271484375,0.131229052734375,0.131210302734375,0.13119151611328125,0.1311727783203125,0.1311540283203125,0.13113531494140626,0.1311165771484375,0.13109783935546876,0.1310791748046875,0.1310604736328125,0.131041796875,0.13102310791015626,0.131004443359375,0.1309857666015625,0.130967138671875,0.130948486328125,0.13092987060546876,0.1309112548828125,0.13089266357421875,0.13087403564453126,0.13085546875,0.13083685302734374,0.13081827392578124,0.1307997314453125,0.1307811767578125,0.1307626220703125,0.130744091796875,0.1307255615234375,0.1307070556640625,0.13068856201171875,0.1306700439453125,0.130651513671875,0.1306330810546875,0.13061461181640624,0.13059613037109374,0.130577685546875,0.1305592041015625,0.1305407958984375,0.13052236328125,0.130503955078125,0.130485546875,0.1304671630859375,0.13044876708984374,0.1304303955078125,0.130411962890625,0.1303936279296875,0.13037529296875,0.130356982421875,0.130338623046875,0.130320263671875,0.13030194091796876,0.13028363037109375,0.13026533203125,0.1302470947265625,0.13022880859375,0.13021051025390626,0.1301922119140625,0.13017398681640624,0.13015570068359375,0.1301374755859375,0.13011923828125,0.130101025390625,0.13008280029296876,0.1300646240234375,0.13004642333984376,0.1300282470703125,0.130010009765625,0.12999189453125,0.12997373046875,0.12995555419921875,0.129937451171875,0.12991929931640625,0.129901171875,0.129883056640625,0.1298649169921875,0.1298468505859375,0.129828759765625,0.1298106689453125,0.129792626953125,0.1297745361328125,0.1297565185546875,0.1297384521484375,0.12972041015625,0.1297023681640625,0.12968433837890625,0.12966636962890624,0.12964835205078126,0.12963037109375,0.1296123291015625,0.129594384765625,0.129576416015625,0.1295584228515625,0.12954049072265625,0.12952254638671876,0.1295046142578125,0.129486669921875,0.1294687744140625,0.12945087890625,0.129432958984375,0.1294150634765625,0.12939715576171876,0.12937930908203124,0.12936143798828126,0.1293435791015625,0.12932572021484376,0.12930789794921874,0.12929005126953125,0.1292722412109375,0.12925439453125,0.12923660888671876,0.12921878662109376,0.1292010009765625,0.129183251953125,0.12916546630859374,0.1291477294921875,0.1291299072265625,0.12911217041015624,0.12909442138671876,0.12907669677734376,0.1290589599609375,0.12904124755859375,0.12902353515625,0.1290058349609375,0.12898817138671875,0.128970458984375,0.1289528076171875,0.12893511962890625,0.12891746826171874,0.1288998046875,0.12888218994140624,0.1288645263671875,0.128846923828125,0.1288292724609375,0.1288116943359375,0.1287941162109375,0.1287764892578125,0.12875889892578124,0.12874130859375,0.12872376708984376,0.1287061767578125,0.128688671875,0.1286710693359375,0.1286535400390625,0.128635986328125,0.1286184814453125,0.1286009765625,0.1285834716796875,0.12856595458984374,0.1285485107421875,0.1285309814453125,0.1285135009765625,0.1284960693359375,0.12847861328125,0.128461181640625,0.12844373779296875,0.1284262939453125,0.1284088623046875,0.1283914794921875,0.1283740478515625,0.12835665283203124,0.12833924560546875,0.128321875,0.1283045166015625,0.128287158203125,0.128269775390625,0.12825238037109374,0.12823507080078125,0.12821771240234375,0.128200390625,0.1281830810546875,0.12816575927734375,0.12814844970703124,0.1281311767578125,0.1281138427734375,0.12809658203125,0.128079296875,0.12806202392578125,0.1280447998046875,0.1280275634765625,0.12801029052734375,0.12799306640625,0.127975830078125,0.12795858154296874,0.12794139404296875,0.1279241943359375,0.127906982421875,0.12788980712890624,0.127872607421875,0.12785545654296876,0.12783829345703124,0.12782110595703125,0.127803955078125,0.12778681640625,0.127769677734375,0.127752587890625,0.12773543701171874,0.12771834716796876,0.127701220703125,0.127684130859375,0.12766705322265626,0.1276499755859375,0.1276328857421875,0.1276158203125,0.1275987548828125,0.12758172607421875,0.12756466064453126,0.12754761962890626,0.127530615234375,0.12751357421875,0.1274965576171875,0.12747955322265625,0.1274625244140625,0.127445556640625,0.12742857666015625,0.12741162109375,0.12739464111328125,0.12737763671875,0.1273607421875,0.127343798828125,0.12732684326171875,0.1273099365234375,0.12729296875,0.12727607421875,0.1272591552734375,0.1272422607421875,0.1272253662109375,0.1272084716796875,0.1271916015625,0.12717474365234374,0.1271578857421875,0.12714102783203124,0.12712415771484376,0.12710732421875,0.127090478515625,0.1270736572265625,0.1270568603515625,0.1270400390625,0.1270232177734375,0.12700643310546875,0.1269896484375,0.1269728515625,0.12695606689453126,0.126939306640625,0.12692254638671874,0.1269057861328125,0.1268890625,0.12687230224609375,0.12685557861328126,0.12683883056640624,0.12682213134765624,0.126805419921875,0.1267887451171875,0.126772021484375,0.1267553466796875,0.1267386962890625,0.12672197265625,0.1267052978515625,0.1266886474609375,0.12667198486328124,0.12665535888671875,0.1266387451171875,0.12662208251953125,0.12660546875,0.12658883056640624,0.1265722412109375,0.1265556396484375,0.1265390380859375,0.1265224609375,0.126505908203125,0.1264893310546875,0.1264727294921875,0.12645621337890625,0.1264396484375,0.12642308349609374,0.12640654296875,0.126389990234375,0.12637349853515625,0.1263569580078125,0.12634046630859375,0.126323974609375,0.126307470703125,0.126290966796875,0.12627451171875,0.1262580078125,0.126241552734375,0.1262250732421875,0.126208642578125,0.1261921875,0.1261757568359375,0.126159326171875,0.12614287109375,0.1261264892578125,0.1261100830078125,0.126093701171875,0.1260772705078125,0.126060888671875,0.12604447021484375,0.1260281494140625,0.12601177978515626,0.12599539794921874,0.1259790283203125,0.12596268310546874,0.1259463623046875,0.1259300048828125,0.1259136962890625,0.12589735107421876,0.12588101806640625,0.1258647216796875,0.1258484375,0.1258321044921875,0.12581583251953124,0.125799560546875,0.1257832763671875,0.1257670166015625,0.12575079345703125,0.1257344970703125,0.12571826171875,0.1257020263671875,0.1256857666015625,0.125669580078125,0.12565335693359375,0.1256371337890625,0.1256209228515625,0.125604736328125,0.12558856201171875,0.12557237548828126,0.1255561767578125,0.1255400390625,0.12552384033203126,0.1255076904296875,0.12549156494140626,0.125475390625,0.1254592529296875,0.12544312744140626,0.1254269775390625,0.1254108642578125,0.1253947509765625,0.125378662109375,0.125362548828125,0.125346484375,0.12533037109375,0.1253143310546875,0.1252982421875,0.12528218994140625,0.1252660888671875,0.12525006103515626,0.1252340087890625,0.12521798095703124,0.125201953125,0.12518594970703126,0.1251698974609375,0.12515390625,0.1251379150390625,0.1251218994140625,0.1251058837890625,0.125089892578125,0.1250739501953125,0.1250579833984375,0.1250420166015625,0.1250260498046875,0.1250100830078125,0.12499417724609375,0.1249781982421875,0.12496226806640626,0.124946337890625,0.12493043212890625,0.1249145263671875,0.124898583984375,0.12488271484375,0.124866796875,0.124850927734375,0.12483504638671875,0.1248191650390625,0.1248032958984375,0.12478743896484375,0.12477159423828126,0.1247557373046875,0.12473985595703126,0.1247240478515625,0.124708203125,0.1246923828125,0.124676611328125,0.1246607666015625,0.1246449951171875,0.1246291748046875,0.12461339111328125,0.1245975830078125,0.1245818115234375,0.124566064453125,0.12455028076171876,0.124534521484375,0.1245187744140625,0.12450302734375,0.1244873046875,0.12447158203125,0.12445579833984376,0.124440087890625,0.124424365234375,0.12440867919921875,0.12439300537109375,0.12437730712890625,0.12436156005859375,0.1243458984375,0.12433023681640624,0.1243145263671875,0.12429888916015625,0.124283251953125,0.12426756591796875,0.1242519287109375,0.124236279296875,0.12422064208984375,0.124205029296875,0.12418939208984375,0.12417381591796875,0.124158203125,0.124142578125,0.1241269775390625,0.1241114013671875,0.12409578857421875,0.12408023681640624,0.1240646484375,0.124049072265625,0.124033544921875,0.124018017578125,0.1240024658203125,0.1239869140625,0.12397138671875,0.123955859375,0.1239403076171875,0.1239248291015625,0.123909326171875,0.12389381103515625,0.1238782958984375,0.123862841796875,0.12384732666015626,0.1238318359375,0.123816357421875,0.123800927734375,0.12378544921875,0.1237699951171875,0.12375458984375,0.123739111328125,0.1237236572265625,0.123708203125,0.12369278564453125,0.1236773681640625,0.1236619873046875,0.12364654541015625,0.1236311767578125,0.123615771484375,0.123600390625,0.123585009765625,0.1235696533203125,0.1235542724609375,0.1235389404296875,0.1235235595703125,0.1235081787109375,0.12349287109375,0.1234775390625,0.12346217041015625,0.1234468505859375,0.12343154296875,0.123416259765625,0.12340087890625,0.123385595703125,0.1233702880859375,0.1233550048828125,0.1233397216796875,0.12332447509765625,0.1233091796875,0.12329390869140625]};

  var data = [data0];
  var layout = {"title":"loss by time"};

  Plotly.plot('plot-1030091756', data, layout);
})();
});
      
</script>
</div>

</div>

<div class="output_area">
<div class="prompt output_prompt">Out[9]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">lossSeq</span>: <span class="ansi-green-fg">IndexedSeq</span>[<span class="ansi-green-fg">cmd5Wrapper</span>.<span class="ansi-green-fg">&lt;refinement&gt;</span>.this.type.<span class="ansi-green-fg">OutputData</span>] = <span class="ansi-yellow-fg">Vector</span>(
  <span class="ansi-green-fg">0.23022568359375</span>,
<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">plot</span>: <span class="ansi-green-fg">Seq</span>[<span class="ansi-green-fg">Scatter</span>] = <span class="ansi-yellow-fg">List</span>(
  <span class="ansi-yellow-fg">Scatter</span>(
<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">res8_3</span>: <span class="ansi-green-fg">String</span> = <span class="ansi-green-fg">&#34;plot-1030091756&#34;</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Verify-the-neural-network-and-predict-the-accuracy">Verify the neural network and predict the accuracy<a class="anchor-link" href="#Verify-the-neural-network-and-predict-the-accuracy">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We use the processed test data to verify the prediction result of the neural network and compute the accuracy. The accuracy shall be about 32%.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">getAccuracy</span><span class="o">(</span><span class="n">myNeuralNetwork</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">testData</span><span class="o">),</span> <span class="n">testExpectResult</span><span class="o">)</span>
<span class="n">println</span><span class="o">(</span><span class="s">s&quot;the result is </span><span class="si">$right</span><span class="s"> %&quot;</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>the result is 32.0 %
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[10]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">right</span>: <span class="ansi-green-fg">Double</span> = <span class="ansi-green-fg">32.0</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h2><p>We have learned the follows in this article:</p>
<ul>
<li>Prepare and process CIFAR10 data</li>
<li>Write softmax classifier</li>
<li>Use the prediction image of the neural network written by softmax classifier to match with the probability of each category.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/SoftmaxLinearClassifier.scala">Complete code</a></p>

</div>
</div>
</div>
 

