---
layout: demo
title: SoftmaxLinearClassifier
download_path: demo_download/.
filename: SoftmaxLinearClassifier.ipynb
---
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Sofxmax&#20998;&#31867;&#22120;">Sofxmax&#20998;&#31867;&#22120;<a class="anchor-link" href="#Sofxmax&#20998;&#31867;&#22120;">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#32972;&#26223;">&#32972;&#26223;<a class="anchor-link" href="#&#32972;&#26223;">&#182;</a></h2><p>通过使用softmax分类器构建神经网络。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#20934;&#22791;&#24037;&#20316;">&#20934;&#22791;&#24037;&#20316;<a class="anchor-link" href="#&#20934;&#22791;&#24037;&#20316;">&#182;</a></h2><p>1.创建一个SBT项目，并引入相关依赖（参照<a href="https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started">Getting Started</a> 或者将下面的依赖引入build.sbt, 注意DeepLearning.scala暂不支持scala2.12.X )</p>

<pre><code>libraryDependencies += "com.thoughtworks.deeplearning" %% "differentiableany" % "latest.release"

libraryDependencies += "com.thoughtworks.deeplearning" %% "differentiablenothing" % "latest.release"

libraryDependencies += "com.thoughtworks.deeplearning" %% "differentiableseq" % "latest.release"

libraryDependencies += "com.thoughtworks.deeplearning" %% "differentiabledouble" % "latest.release"

libraryDependencies += "com.thoughtworks.deeplearning" %% "differentiablefloat" % "latest.release"

libraryDependencies += "com.thoughtworks.deeplearning" %% "differentiablehlist" % "latest.release"

libraryDependencies += "com.thoughtworks.deeplearning" %% "differentiablecoproduct" % "latest.release"

libraryDependencies += "com.thoughtworks.deeplearning" %% "differentiableindarray" % "latest.release"

addCompilerPlugin("com.thoughtworks.implicit-dependent-type" %% "implicit-dependent-type" % "latest.release")

addCompilerPlugin("org.scalamacros" % "paradise" % "2.1.0" cross CrossVersion.full)

fork := true</code></pre>
<p>2.<a href="https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz">下载CIFAR-10 binary version (suitable for C programs)</a>，文件大小162 MB，md5sum：c32a1d4ab5d03f1284b67883e8d87530</p>
<p>3.将下载好的文件解压到src/main/resources目录。</p>
<p>4.Scala类ReadCIFAR10ToNDArray用于从上面的文件中读取图片及其标签数据并做归一化处理（<a href="https://www.cs.toronto.edu/~kriz/cifar.html">更多信息</a>）：</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$plugin.$ivy.</span><span class="n">`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:1.0.0`</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC5`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC5`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC5`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC5`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC5`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC5`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC5`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC5`</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.plotly-scala::plotly-jupyter-scala:0.3.0`</span>

<span class="k">import</span> <span class="nn">java.io.</span><span class="o">{</span><span class="nc">FileInputStream</span><span class="o">,</span> <span class="nc">InputStream</span><span class="o">}</span>


<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableHList._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableDouble._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableINDArray._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableAny._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.</span><span class="o">{</span><span class="nc">DifferentiableHList</span><span class="o">,</span><span class="nc">DifferentiableINDArray</span><span class="o">,</span><span class="nc">Layer</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Layer.Batch</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Lift.Layers.Identity</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Lift._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathFunctions._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathMethods.</span><span class="o">/</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathOps</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.cpu.nativecpu.NDArray</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.factory.Nd4j</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.indexing.</span><span class="o">{</span><span class="nc">INDArrayIndex</span><span class="o">,</span> <span class="nc">NDArrayIndex</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.ops.transforms.Transforms</span>
<span class="k">import</span> <span class="nn">org.nd4s.Implicits._</span>
<span class="k">import</span> <span class="nn">shapeless._</span>

<span class="k">import</span> <span class="nn">plotly._</span>
<span class="k">import</span> <span class="nn">plotly.element._</span>
<span class="k">import</span> <span class="nn">plotly.layout._</span>
<span class="k">import</span> <span class="nn">plotly.JupyterScala._</span>

<span class="k">import</span> <span class="nn">scala.collection.immutable.IndexedSeq</span>

<span class="n">pprintConfig</span><span class="o">()</span> <span class="k">=</span> <span class="n">pprintConfig</span><span class="o">().</span><span class="n">copy</span><span class="o">(</span><span class="n">height</span> <span class="k">=</span> <span class="mi">5</span><span class="o">)</span><span class="c1">//减少输出的行数，避免页面输出太长</span>

<span class="k">import</span> <span class="nn">$file.ReadCIFAR10ToNDArray</span><span class="o">,</span><span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="k">_</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[1]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$plugin.$                                                                             

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                           
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                               
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                           
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                              
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                             
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                             
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                                 
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                                

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                             

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">java.io.{FileInputStream, InputStream}


</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableHList._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableDouble._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableINDArray._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableAny._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.{DifferentiableHList,DifferentiableINDArray,Layer}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Layer.Batch
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Lift.Layers.Identity
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Lift._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathFunctions._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathMethods./
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathOps
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.cpu.nativecpu.NDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.factory.Nd4j
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.ops.transforms.Transforms
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4s.Implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">shapeless._

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.element._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.layout._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.JupyterScala._

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.collection.immutable.IndexedSeq

//pprintConfig() = pprintConfig().copy(height = 5 )

</span>
defined <span class="ansi-green-fg">object</span> <span class="ansi-cyan-fg">ReadCIFAR10ToNDArray</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>5.<a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>: softmax是<a href="https://en.wikipedia.org/wiki/Logistic_function">logistic</a> 对多分类的一般化归纳。公式：<img src="https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D" alt=""></p>
<p><a href="https://en.wikipedia.org/wiki/Cross-entropy">交叉熵损失（cross-entropy loss）</a>:p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。公式：<img src="https://zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29" alt=""></p>
<p>6.如果你使用IntelliJ或者eclipse等其它IDE，智能提示可能会失效，代码有部分可能会爆红，这是IDE的问题，代码本身并无问题。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;">&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;<a class="anchor-link" href="#&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;">&#182;</a></h2><p>1.新建一个Scala类SoftmaxLinearClassifier</p>
<p>2.从CIFAR10 database中读取训练数据和测试数据的图片和标签信息</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)</span>
  <span class="k">val</span> <span class="nc">CLASSES</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span>

  <span class="c1">//加载train数据,我们读取1000条数据作为训练数据</span>
  <span class="k">val</span> <span class="n">trainNDArray</span> <span class="k">=</span>
    <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="n">readFromResource</span><span class="o">(</span><span class="s">&quot;/cifar-10-batches-bin/data_batch_1.bin&quot;</span><span class="o">,</span> <span class="mi">1000</span><span class="o">)</span>

  <span class="c1">//加载测试数据，我们读取100条作为测试数据</span>
  <span class="k">val</span> <span class="n">testNDArray</span> <span class="k">=</span>
    <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="n">readFromResource</span><span class="o">(</span><span class="s">&quot;/cifar-10-batches-bin/test_batch.bin&quot;</span><span class="o">,</span> <span class="mi">100</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stderr output_text">
<pre>SLF4J: Failed to load class &#34;org.slf4j.impl.StaticLoggerBinder&#34;.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[2]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">CLASSES</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">10</span>
<span class="ansi-cyan-fg">trainNDArray</span>: <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">HNil</span> = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.66, 0.62, 0.62, 0.62, 0.59, 0.58, 0.06, 0.00, 0.07, 0.20, 0.34, 0.47, 0.50, 0.50, 0.49, 0.45, 0.41, 0.39, 0.41, 0.44, 0.43, 0.44, 0.46, 0.43, 0.41, 0.49, 0.50, 0.48, 0.51, 0.48, 0.47, 0.51, 0.52, 0.52, 0.52, 0.48, 0.46, 0.48, 0.10, 0.06, 0.19, 0.32, 0.43, 0.50, 0.51, 0.47, 0.44, 0.44, 0.44, 0.41, 0.41, 0.50, 0.48, 0.51, 0.50, 0.48, 0.45, 0.47, 0.51, 0.51, 0.54, 0.50, 0.49, 0.50, 0.51, 0.55, 0.51, 0.46, 0.47, 0.43, 0.13, 0.15, 0.34, 0.41, 0.45, 0.46, 0.45, 0.41, 0.42, 0.47, 0.49, 0.43, 0.44, 0.57, 0.52, 0.50, 0.46, 0.46, 0.50, 0.48, 0.52, 0.54, 0.53, 0.51, 0.48, 0.51, 0.52, 0.53, 0.51, 0.49, 0.47, 0.37, 0.20, 0.23, 0.40, 0.50, 0.48, 0.47, 0.47, 0.45, 0.42, 0.49, 0.50, 0.41, 0.42, 0.48, 0.47, 0.42, 0.38, 0.43, 0.46, 0.47, 0.52, 0.55, 0.51, 0.55, 0.53, 0.50, 0.47, 0.46, 0.40, 0.34, 0.29, 0.26<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">testNDArray</span>: <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">HNil</span> = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.55, 0.55, 0.56, 0.54, 0.49, 0.45, 0.59, 0.59, 0.62, 0.65, 0.63, 0.62, 0.64, 0.63, 0.64, 0.61, 0.61, 0.62, 0.64, 0.66, 0.67, 0.67, 0.66, 0.62, 0.60, 0.59, 0.57, 0.54, 0.55, 0.55, 0.58, 0.57, 0.57, 0.55, 0.56, 0.53, 0.49, 0.46, 0.59, 0.59, 0.62, 0.65, 0.62, 0.64, 0.64, 0.64, 0.64, 0.63, 0.62, 0.61, 0.63, 0.65, 0.65, 0.66, 0.66, 0.62, 0.57, 0.47, 0.43, 0.38, 0.39, 0.45, 0.47, 0.52, 0.56, 0.55, 0.55, 0.54, 0.51, 0.47, 0.61, 0.61, 0.62, 0.68, 0.65, 0.65, 0.66, 0.66, 0.64, 0.64, 0.65, 0.75, 0.69, 0.61, 0.63, 0.64, 0.62, 0.58, 0.41, 0.40, 0.38, 0.36, 0.31, 0.29, 0.34, 0.32, 0.44, 0.52, 0.55, 0.55, 0.53, 0.50, 0.61, 0.61, 0.63, 0.66, 0.66, 0.64, 0.66, 0.65, 0.64, 0.64, 0.68, 0.96, 0.76, 0.59, 0.57, 0.55, 0.43, 0.30, 0.33, 0.44, 0.44, 0.41, 0.38, 0.36, 0.29, 0.33, 0.33, 0.41, 0.50, 0.54, 0.52, 0.50<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>3.编写处理标签数据的工具方法，将N行一列的NDArray转换为N行CLASSES列的NDArray，每行对应的正确分类的值为1，其它列的值为0。这样做是为了向cross-entropy loss公式靠拢</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="cm">/**</span>
<span class="cm">    * 处理标签数据：将N行一列的NDArray转换为N行CLASSES列的NDArray，每行对应的正确分类的值为1，其它列的值为0</span>
<span class="cm">    *</span>
<span class="cm">    * @param ndArray 标签数据</span>
<span class="cm">    * @return N行CLASSES列的NDArray</span>
<span class="cm">    */</span>
  <span class="k">def</span> <span class="n">makeVectorized</span><span class="o">(</span><span class="n">ndArray</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">shape</span> <span class="k">=</span> <span class="n">ndArray</span><span class="o">.</span><span class="n">shape</span><span class="o">()</span>

    <span class="k">val</span> <span class="n">p</span> <span class="k">=</span> <span class="nc">Nd4j</span><span class="o">.</span><span class="n">zeros</span><span class="o">(</span><span class="n">shape</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="nc">CLASSES</span><span class="o">)</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">i</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="n">shape</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">double</span> <span class="k">=</span> <span class="n">ndArray</span><span class="o">.</span><span class="n">getDouble</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="mi">0</span><span class="o">)</span>
      <span class="k">val</span> <span class="n">column</span> <span class="k">=</span> <span class="n">double</span><span class="o">.</span><span class="n">toInt</span>
      <span class="n">p</span><span class="o">.</span><span class="n">put</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">column</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span>
    <span class="o">}</span>
    <span class="n">p</span>
  <span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[3]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">makeVectorized</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>4.分离和处理图像和标签数据</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">train_data</span> <span class="k">=</span> <span class="n">trainNDArray</span><span class="o">.</span><span class="n">head</span>
  <span class="k">val</span> <span class="n">test_data</span> <span class="k">=</span> <span class="n">testNDArray</span><span class="o">.</span><span class="n">head</span>

  <span class="k">val</span> <span class="n">train_expect_result</span> <span class="k">=</span> <span class="n">trainNDArray</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>
  <span class="k">val</span> <span class="n">test_expect_result</span> <span class="k">=</span> <span class="n">testNDArray</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>
  
  <span class="k">val</span> <span class="n">p</span> <span class="k">=</span> <span class="n">makeVectorized</span><span class="o">(</span><span class="n">train_expect_result</span><span class="o">)</span>
  <span class="k">val</span> <span class="n">test_p</span> <span class="k">=</span> <span class="n">makeVectorized</span><span class="o">(</span><span class="n">test_expect_result</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[4]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">train_data</span>: <span class="ansi-green-fg">INDArray</span> = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.66, 0.62, 0.62, 0.62, 0.59, 0.58, 0.06, 0.00, 0.07, 0.20, 0.34, 0.47, 0.50, 0.50, 0.49, 0.45, 0.41, 0.39, 0.41, 0.44, 0.43, 0.44, 0.46, 0.43, 0.41, 0.49, 0.50, 0.48, 0.51, 0.48, 0.47, 0.51, 0.52, 0.52, 0.52, 0.48, 0.46, 0.48, 0.10, 0.06, 0.19, 0.32, 0.43, 0.50, 0.51, 0.47, 0.44, 0.44, 0.44, 0.41, 0.41, 0.50, 0.48, 0.51, 0.50, 0.48, 0.45, 0.47, 0.51, 0.51, 0.54, 0.50, 0.49, 0.50, 0.51, 0.55, 0.51, 0.46, 0.47, 0.43, 0.13, 0.15, 0.34, 0.41, 0.45, 0.46, 0.45, 0.41, 0.42, 0.47, 0.49, 0.43, 0.44, 0.57, 0.52, 0.50, 0.46, 0.46, 0.50, 0.48, 0.52, 0.54, 0.53, 0.51, 0.48, 0.51, 0.52, 0.53, 0.51, 0.49, 0.47, 0.37, 0.20, 0.23, 0.40, 0.50, 0.48, 0.47, 0.47, 0.45, 0.42, 0.49, 0.50, 0.41, 0.42, 0.48, 0.47, 0.42, 0.38, 0.43, 0.46, 0.47, 0.52, 0.55, 0.51, 0.55, 0.53, 0.50, 0.47, 0.46, 0.40, 0.34, 0.29, 0.26<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">test_data</span>: <span class="ansi-green-fg">INDArray</span> = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.55, 0.55, 0.56, 0.54, 0.49, 0.45, 0.59, 0.59, 0.62, 0.65, 0.63, 0.62, 0.64, 0.63, 0.64, 0.61, 0.61, 0.62, 0.64, 0.66, 0.67, 0.67, 0.66, 0.62, 0.60, 0.59, 0.57, 0.54, 0.55, 0.55, 0.58, 0.57, 0.57, 0.55, 0.56, 0.53, 0.49, 0.46, 0.59, 0.59, 0.62, 0.65, 0.62, 0.64, 0.64, 0.64, 0.64, 0.63, 0.62, 0.61, 0.63, 0.65, 0.65, 0.66, 0.66, 0.62, 0.57, 0.47, 0.43, 0.38, 0.39, 0.45, 0.47, 0.52, 0.56, 0.55, 0.55, 0.54, 0.51, 0.47, 0.61, 0.61, 0.62, 0.68, 0.65, 0.65, 0.66, 0.66, 0.64, 0.64, 0.65, 0.75, 0.69, 0.61, 0.63, 0.64, 0.62, 0.58, 0.41, 0.40, 0.38, 0.36, 0.31, 0.29, 0.34, 0.32, 0.44, 0.52, 0.55, 0.55, 0.53, 0.50, 0.61, 0.61, 0.63, 0.66, 0.66, 0.64, 0.66, 0.65, 0.64, 0.64, 0.68, 0.96, 0.76, 0.59, 0.57, 0.55, 0.43, 0.30, 0.33, 0.44, 0.44, 0.41, 0.38, 0.36, 0.29, 0.33, 0.33, 0.41, 0.50, 0.54, 0.52, 0.50<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">train_expect_result</span>: <span class="ansi-green-fg">INDArray</span> = [6.00, 9.00, 9.00, 4.00, 1.00, 1.00, 2.00, 7.00, 8.00, 3.00, 4.00, 7.00, 7.00, 2.00, 9.00, 9.00, 9.00, 3.00, 2.00, 6.00, 4.00, 3.00, 6.00, 6.00, 2.00, 6.00, 3.00, 5.00, 4.00, 0.00, 0.00, 9.00, 1.00, 3.00, 4.00, 0.00, 3.00, 7.00, 3.00, 3.00, 5.00, 2.00, 2.00, 7.00, 1.00, 1.00, 1.00, 2.00, 2.00, 0.00, 9.00, 5.00, 7.00, 9.00, 2.00, 2.00, 5.00, 2.00, 4.00, 3.00, 1.00, 1.00, 8.00, 2.00, 1.00, 1.00, 4.00, 9.00, 7.00, 8.00, 5.00, 9.00, 6.00, 7.00, 3.00, 1.00, 9.00, 0.00, 3.00, 1.00, 3.00, 5.00, 4.00, 5.00, 7.00, 7.00, 4.00, 7.00, 9.00, 4.00, 2.00, 3.00, 8.00, 0.00, 1.00, 6.00, 1.00, 1.00, 4.00, 1.00, 8.00, 3.00, 9.00, 6.00, 6.00, 1.00, 8.00, 5.00, 2.00, 9.00, 9.00, 8.00, 1.00, 7.00, 7.00, 0.00, 0.00, 6.00, 9.00, 1.00, 2.00, 2.00, 9.00, 2.00, 6.00, 6.00, 1.00, 9.00, 5.00, 0.00, 4.00, 7.00, 6.00, 7.00, 1.00, 8.00, 1.00, 1.00, 2.00, 8.00, 1.00, 3.00, 3.00, 6.00, 2.00, 4.00, 9.00, 9.00, 5.00, 4.00, 3.00, 6.00, 7.00, 4.00, 6.00, 8.00, 5.00, 5.00, 4.00, 3.00,<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">test_expect_result</span>: <span class="ansi-green-fg">INDArray</span> = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.00, 0.00, 9.00, 6.00, 6.00, 5.00, 4.00, 5.00, 9.00, 2.00, 4.00, 1.00, 9.00, 5.00, 4.00, 6.00, 5.00, 6.00, 0.00, 9.00, 3.00, 9.00, 7.00, 6.00, 9.00, 8.00, 0.00, 3.00, 8.00, 8.00, 7.00, 7.00, 4.00, 6.00, 7.00, 3.00, 6.00, 3.00, 6.00, 2.00, 1.00, 2.00, 3.00, 7.00, 2.00, 6.00, 8.00, 8.00, 0.00, 2.00, 9.00, 3.00, 3.00, 8.00, 8.00, 1.00, 1.00, 7.00, 2.00, 5.00, 2.00, 7.00, 8.00, 9.00, 0.00, 3.00, 8.00, 6.00, 4.00, 6.00, 6.00, 0.00, 0.00, 7.00]
<span class="ansi-cyan-fg">p</span>: <span class="ansi-green-fg">INDArray</span> = [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],
 [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],
 [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00],
<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">test_p</span>: <span class="ansi-green-fg">INDArray</span> = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],
 [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],
 [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],
<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>5.编写softmax函数,和准备一节中的softmax公式对应</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">softmax</span><span class="o">(</span><span class="k">implicit</span> <span class="n">scores</span><span class="k">:</span> <span class="kt">From</span><span class="o">[</span><span class="kt">INDArray</span><span class="o">]</span> <span class="o">#</span><span class="k">#</span><span class="n">T</span><span class="o">)</span><span class="k">:</span> <span class="kt">To</span><span class="o">[</span><span class="kt">INDArray</span><span class="o">]</span> <span class="o">#</span><span class="k">#</span><span class="n">T</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">expScores</span> <span class="k">=</span> <span class="n">exp</span><span class="o">(</span><span class="n">scores</span><span class="o">)</span>
    <span class="n">expScores</span> <span class="o">/</span> <span class="n">expScores</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[5]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">softmax</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>6.设置学习率，学习率是Weight变化的快慢的直观描述，学习率设置的过小会导致loss下降的很慢，需要更长时间来训练，学习率设置的过大虽然刚开始下降很快但是会导致在接近最低点的时候在附近徘徊loss下降会非常慢。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">implicit</span> <span class="k">def</span> <span class="n">optimizer</span><span class="k">:</span> <span class="kt">Optimizer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">LearningRate</span> <span class="o">{</span>
    <span class="k">def</span> <span class="n">currentLearningRate</span><span class="o">()</span> <span class="k">=</span> <span class="mf">0.00001</span>
  <span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[6]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">optimizer</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>7.跟定义一个方法一样定义一个神经网络并初始化Weight，Weight应该是一个N*CLASSES的INDArray,每个图片对应每个分类都有一个评分。<a href="https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization">什么是Weight</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">createMyNeuralNetwork</span><span class="o">(</span><span class="k">implicit</span> <span class="n">input</span><span class="k">:</span> <span class="kt">From</span><span class="o">[</span><span class="kt">INDArray</span><span class="o">]</span> <span class="o">#</span><span class="k">#</span><span class="n">T</span><span class="o">)</span><span class="k">:</span> <span class="kt">To</span><span class="o">[</span><span class="kt">INDArray</span><span class="o">]</span> <span class="o">#</span><span class="k">#</span><span class="n">T</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">initialValueOfWeight</span> <span class="k">=</span> <span class="nc">Nd4j</span><span class="o">.</span><span class="n">randn</span><span class="o">(</span><span class="mi">3072</span><span class="o">,</span> <span class="nc">CLASSES</span><span class="o">)</span> <span class="o">*</span> <span class="mf">0.001</span>
    <span class="k">val</span> <span class="n">weight</span><span class="k">:</span> <span class="kt">To</span><span class="o">[</span><span class="kt">INDArray</span><span class="o">]</span> <span class="o">#</span><span class="k">#</span><span class="n">T</span> <span class="k">=</span> <span class="n">initialValueOfWeight</span><span class="o">.</span><span class="n">toWeight</span>
    <span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">To</span><span class="o">[</span><span class="kt">INDArray</span><span class="o">]</span> <span class="o">#</span><span class="k">#</span><span class="n">T</span> <span class="k">=</span> <span class="n">input</span> <span class="n">dot</span> <span class="n">weight</span>
    <span class="n">softmax</span><span class="o">.</span><span class="n">compose</span><span class="o">(</span><span class="n">result</span><span class="o">)</span> <span class="c1">//对结果调用softmax方法，压缩结果值在0到1之间方便处理</span>
  <span class="o">}</span>
  <span class="k">val</span> <span class="n">myNeuralNetwork</span><span class="k">:</span> <span class="kt">FromTo</span><span class="o">[</span><span class="kt">INDArray</span>, <span class="kt">INDArray</span><span class="o">]</span> <span class="o">#</span><span class="k">#</span><span class="n">T</span> <span class="k">=</span> <span class="n">createMyNeuralNetwork</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[7]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">createMyNeuralNetwork</span>
<span class="ansi-cyan-fg">myNeuralNetwork</span>: (<span class="ansi-green-fg">FromTo</span>[<span class="ansi-green-fg">INDArray</span>, <span class="ansi-green-fg">INDArray</span>]{type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray;type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray})#<span class="ansi-green-fg">T</span> = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[-0.00, 0.00, 0.00, 0.00, 0.00, 0.00, -0.00, 0.00, -0.00, -0.00],
 [-0.00, 0.00, 0.00, 0.00, 0.00, -0.00, 0.00, -0.00, -0.00, -0.00],
 [0.00, 0.00, 0.00, -0.00, -0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, -0.00, -0.00, 0.00, -0.00, -0.00, 0.00, -0.00, 0.00, -0.00],
 [0.00, -0.00, -0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, -0.00],
 [-0.00, -0.00, -0.00, -0.00, 0.00, -0.00, -0.00, -0.00, -0.00, -0.00],
 [0.00, 0.00, -0.00, -0.00, 0.00, 0.00, 0.00, 0.00, 0.00, -0.00],
 [-0.00, -0.00, -0.00, 0.00, -0.00, 0.00, 0.00, 0.00, 0.00, -0.00],
 [0.00, -0.00, -0.00, 0.00, 0.00, 0.00, 0.00, 0.00, -0.00, 0.00],
 [-0.00, -0.00, -0.00, -0.00, -0.00, 0.00, 0.00, 0.00, -0.00, -0.00],
<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>8.编写损失函数Loss Function，将此次判断的结果和真实结果进行计算得出cross-entropy loss并返回</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">def</span> <span class="n">lossFunction</span><span class="o">(</span><span class="k">implicit</span> <span class="n">pair</span><span class="k">:</span> <span class="kt">From</span><span class="o">[</span><span class="kt">INDArray</span> <span class="kt">::</span> <span class="kt">INDArray</span> <span class="kt">::</span> <span class="kt">HNil</span><span class="o">]</span> <span class="o">#</span><span class="k">#</span><span class="n">T</span><span class="o">)</span><span class="k">:</span> <span class="kt">To</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="o">#</span><span class="k">#</span><span class="n">T</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">input</span> <span class="k">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">head</span>
    <span class="k">val</span> <span class="n">expectedOutput</span> <span class="k">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>
    <span class="k">val</span> <span class="n">probabilities</span> <span class="k">=</span> <span class="n">myNeuralNetwork</span><span class="o">.</span><span class="n">compose</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>

    <span class="o">-(</span><span class="n">expectedOutput</span> <span class="o">*</span> <span class="n">log</span><span class="o">(</span><span class="n">probabilities</span><span class="o">)).</span><span class="n">sum</span> <span class="c1">//此处和准备一节中的交叉熵损失对应</span>
  <span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[8]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">lossFunction</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>9.训练神经网络并观察每次训练loss的变化，loss的变化趋势应该是越来越低的</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">lossSeq</span> <span class="k">=</span> <span class="k">for</span> <span class="o">(</span><span class="k">_</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="mi">2000</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
    <span class="n">lossFunction</span><span class="o">.</span><span class="n">train</span><span class="o">(</span><span class="n">train_data</span> <span class="o">::</span> <span class="n">p</span> <span class="o">::</span> <span class="nc">HNil</span><span class="o">)</span>
  <span class="o">}</span>

  <span class="n">plotly</span><span class="o">.</span><span class="nc">JupyterScala</span><span class="o">.</span><span class="n">init</span><span class="o">()</span>
  <span class="k">val</span> <span class="n">plot</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
    <span class="nc">Scatter</span><span class="o">(</span>
      <span class="mi">0</span> <span class="n">until</span> <span class="mi">2000</span> <span class="n">by</span> <span class="mi">1</span><span class="o">,</span>
      <span class="n">lossSeq</span>
    <span class="o">)</span>
  <span class="o">)</span>

  <span class="n">plot</span><span class="o">.</span><span class="n">plot</span><span class="o">(</span>
    <span class="n">title</span> <span class="k">=</span> <span class="s">&quot;loss on time&quot;</span>
  <span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">

      <script type="text/javascript">
        require.config({
  paths: {
    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',
    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'
  },

  shim: {
    plotly: {
      deps: ['d3', 'jquery'],
      exports: 'plotly'
    }
  }
});
        

        require(['plotly'], function(Plotly) {
          window.Plotly = Plotly;
        });
      </script>
    
</div>

</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div class="chart" id="plot-2000235454"></div>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>




<div id="408c0cb6-017a-4641-bbd3-0f45405dc9b0"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#408c0cb6-017a-4641-bbd3-0f45405dc9b0');
requirejs(["plotly"], function(Plotly) {
  (function () {
  var data0 = {"type":"scatter","x":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],"y":[2304.05615234375,2284.93359375,2275.099365234375,2266.16552734375,2257.55029296875,2249.195068359375,2241.08251953125,2233.203125,2225.54736328125,2218.10400390625,2210.8662109375,2203.822998046875,2196.9697265625,2190.2958984375,2183.795654296875,2177.4619140625,2171.28662109375,2165.26513671875,2159.390869140625,2153.657958984375,2148.060302734375,2142.59423828125,2137.2529296875,2132.0322265625,2126.92919921875,2121.93701171875,2117.052734375,2112.272705078125,2107.593017578125,2103.01025390625,2098.52001953125,2094.1201171875,2089.807373046875,2085.578125,2081.43017578125,2077.3603515625,2073.3671875,2069.4462890625,2065.59716796875,2061.81591796875,2058.10205078125,2054.4521484375,2050.86474609375,2047.33837890625,2043.8699951171875,2040.4588623046875,2037.102294921875,2033.8001708984375,2030.5496826171875,2027.3497314453125,2024.19921875,2021.095947265625,2018.03955078125,2015.02783203125,2012.060791015625,2009.1357421875,2006.25244140625,2003.4102783203125,2000.6064453125,1997.841796875,1995.1138916015625,1992.4228515625,1989.76708984375,1987.14599609375,1984.55859375,1982.004638671875,1979.482421875,1976.991455078125,1974.5316162109375,1972.10107421875,1969.699951171875,1967.3271484375,1964.98291015625,1962.6650390625,1960.374755859375,1958.1094970703125,1955.8702392578125,1953.6561279296875,1951.465576171875,1949.2994384765625,1947.15673828125,1945.03662109375,1942.938720703125,1940.86279296875,1938.80859375,1936.774658203125,1934.761474609375,1932.768798828125,1930.7955322265625,1928.84228515625,1926.9072265625,1924.9906005859375,1923.0933837890625,1921.213134765625,1919.3505859375,1917.50537109375,1915.677490234375,1913.865234375,1912.0703125,1910.2904052734375,1908.5269775390625,1906.7784423828125,1905.045166015625,1903.3267822265625,1901.6226806640625,1899.93359375,1898.2578125,1896.5966796875,1894.94873046875,1893.314208984375,1891.69287109375,1890.084716796875,1888.4892578125,1886.9061279296875,1885.335205078125,1883.7767333984375,1882.23046875,1880.6951904296875,1879.1719970703125,1877.65966796875,1876.158935546875,1874.6690673828125,1873.1903076171875,1871.7218017578125,1870.263916015625,1868.81689453125,1867.3798828125,1865.952880859375,1864.535888671875,1863.1279296875,1861.73046875,1860.341796875,1858.963134765625,1857.593505859375,1856.2325439453125,1854.881103515625,1853.53759765625,1852.203857421875,1850.8785400390625,1849.561279296875,1848.252685546875,1846.9521484375,1845.65966796875,1844.37548828125,1843.0987548828125,1841.830078125,1840.569091796875,1839.3157958984375,1838.0701904296875,1836.831787109375,1835.6005859375,1834.376708984375,1833.16015625,1831.950927734375,1830.748046875,1829.552001953125,1828.363037109375,1827.1806640625,1826.0048828125,1824.836181640625,1823.673095703125,1822.517333984375,1821.36669921875,1820.22314453125,1819.08544921875,1817.9544677734375,1816.828369140625,1815.70947265625,1814.5955810546875,1813.48779296875,1812.3857421875,1811.289306640625,1810.198486328125,1809.1131591796875,1808.0338134765625,1806.958984375,1805.890625,1804.826416015625,1803.7686767578125,1802.71533203125,1801.6671142578125,1800.624267578125,1799.586669921875,1798.5540771484375,1797.5260009765625,1796.503173828125,1795.484619140625,1794.4716796875,1793.4630126953125,1792.458740234375,1791.458984375,1790.464599609375,1789.474609375,1788.4888916015625,1787.50732421875,1786.530517578125,1785.5576171875,1784.58984375,1783.6259765625,1782.666015625,1781.7109375,1780.759521484375,1779.8125,1778.8692626953125,1777.929931640625,1776.9951171875,1776.063720703125,1775.13671875,1774.213623046875,1773.2939453125,1772.378173828125,1771.466552734375,1770.5584716796875,1769.6541748046875,1768.753173828125,1767.856201171875,1766.962646484375,1766.0731201171875,1765.1865234375,1764.3037109375,1763.42431640625,1762.5484619140625,1761.67626953125,1760.806884765625,1759.941162109375,1759.079345703125,1758.22021484375,1757.3643798828125,1756.511962890625,1755.662353515625,1754.816650390625,1753.9736328125,1753.1339111328125,1752.2972412109375,1751.4638671875,1750.633056640625,1749.805419921875,1748.981689453125,1748.16015625,1747.341552734375,1746.5260009765625,1745.713134765625,1744.903564453125,1744.096923828125,1743.29296875,1742.491943359375,1741.69384765625,1740.898193359375,1740.1048583984375,1739.3153076171875,1738.527587890625,1737.7431640625,1736.96142578125,1736.1817626953125,1735.4052734375,1734.631103515625,1733.85986328125,1733.0909423828125,1732.3245849609375,1731.5604248046875,1730.7994384765625,1730.0406494140625,1729.28466796875,1728.5302734375,1727.7791748046875,1727.0303955078125,1726.28369140625,1725.53955078125,1724.7977294921875,1724.05859375,1723.321533203125,1722.58642578125,1721.854248046875,1721.1239013671875,1720.3963623046875,1719.67041015625,1718.947265625,1718.2259521484375,1717.5068359375,1716.790283203125,1716.07568359375,1715.36328125,1714.65283203125,1713.94482421875,1713.239013671875,1712.5352783203125,1711.833251953125,1711.13330078125,1710.435791015625,1709.73974609375,1709.046630859375,1708.3551025390625,1707.66552734375,1706.977783203125,1706.2923583984375,1705.608642578125,1704.9273681640625,1704.247314453125,1703.5693359375,1702.8939208984375,1702.2198486328125,1701.5479736328125,1700.87744140625,1700.209228515625,1699.542724609375,1698.8782958984375,1698.21533203125,1697.554443359375,1696.895263671875,1696.238037109375,1695.5821533203125,1694.928466796875,1694.2769775390625,1693.626220703125,1692.977783203125,1692.330810546875,1691.685791015625,1691.04248046875,1690.4005126953125,1689.7607421875,1689.1224365234375,1688.486083984375,1687.8507080078125,1687.2171630859375,1686.585693359375,1685.955322265625,1685.327392578125,1684.700439453125,1684.0751953125,1683.45166015625,1682.8291015625,1682.208984375,1681.589599609375,1680.97265625,1680.3570556640625,1679.7421875,1679.12939453125,1678.518310546875,1677.908447265625,1677.300537109375,1676.693603515625,1676.0885009765625,1675.484619140625,1674.8822021484375,1674.28125,1673.68212890625,1673.083984375,1672.487060546875,1671.89208984375,1671.298095703125,1670.706298828125,1670.115478515625,1669.5257568359375,1668.9378662109375,1668.350830078125,1667.7655029296875,1667.181396484375,1666.598876953125,1666.01708984375,1665.4375,1664.8587646484375,1664.28173828125,1663.70556640625,1663.130615234375,1662.5576171875,1661.9853515625,1661.414794921875,1660.8450927734375,1660.2767333984375,1659.710205078125,1659.144287109375,1658.579833984375,1658.0166015625,1657.4547119140625,1656.8939208984375,1656.334228515625,1655.77587890625,1655.21923828125,1654.663330078125,1654.108642578125,1653.55517578125,1653.00341796875,1652.4521484375,1651.9024658203125,1651.3533935546875,1650.8055419921875,1650.259521484375,1649.714599609375,1649.169921875,1648.6275634765625,1648.0855712890625,1647.544921875,1647.005126953125,1646.4671630859375,1645.93017578125,1645.393798828125,1644.85888671875,1644.32470703125,1643.7919921875,1643.2601318359375,1642.729736328125,1642.2000732421875,1641.67138671875,1641.14453125,1640.6177978515625,1640.09228515625,1639.56787109375,1639.044921875,1638.522705078125,1638.0015869140625,1637.4815673828125,1636.96240234375,1636.4444580078125,1635.927490234375,1635.4111328125,1634.896240234375,1634.38232421875,1633.8695068359375,1633.3572998046875,1632.846435546875,1632.3363037109375,1631.8272705078125,1631.31982421875,1630.8125,1630.3060302734375,1629.801025390625,1629.2967529296875,1628.793701171875,1628.291015625,1627.7900390625,1627.2891845703125,1626.7900390625,1626.2916259765625,1625.793701171875,1625.297119140625,1624.8017578125,1624.3065185546875,1623.8125,1623.320068359375,1622.8277587890625,1622.33642578125,1621.8460693359375,1621.356689453125,1620.867919921875,1620.3807373046875,1619.8939208984375,1619.408203125,1618.923095703125,1618.438720703125,1617.9560546875,1617.473388671875,1616.99169921875,1616.51123046875,1616.03125,1615.552001953125,1615.073974609375,1614.5968017578125,1614.1204833984375,1613.644775390625,1613.1700439453125,1612.696044921875,1612.222900390625,1611.75048828125,1611.2789306640625,1610.808349609375,1610.338623046875,1609.8695068359375,1609.40087890625,1608.933837890625,1608.467041015625,1608.001220703125,1607.5362548828125,1607.0718994140625,1606.6083984375,1606.1455078125,1605.683837890625,1605.2225341796875,1604.761962890625,1604.302490234375,1603.84326171875,1603.38525390625,1602.92822265625,1602.4713134765625,1602.015869140625,1601.5604248046875,1601.106201171875,1600.652587890625,1600.1998291015625,1599.747314453125,1599.2960205078125,1598.8455810546875,1598.3955078125,1597.946533203125,1597.497802734375,1597.050048828125,1596.60302734375,1596.156982421875,1595.711181640625,1595.266357421875,1594.82177734375,1594.37890625,1593.93603515625,1593.4937744140625,1593.05224609375,1592.6116943359375,1592.1715087890625,1591.7322998046875,1591.293701171875,1590.85546875,1590.418212890625,1589.981689453125,1589.5457763671875,1589.110595703125,1588.67626953125,1588.2421875,1587.808837890625,1587.376220703125,1586.944580078125,1586.5130615234375,1586.08251953125,1585.652587890625,1585.2232666015625,1584.79443359375,1584.36669921875,1583.9393310546875,1583.512451171875,1583.0865478515625,1582.6611328125,1582.236328125,1581.8125,1581.388427734375,1580.965576171875,1580.54345703125,1580.121826171875,1579.70068359375,1579.2802734375,1578.8603515625,1578.44091796875,1578.0225830078125,1577.6044921875,1577.1873779296875,1576.770751953125,1576.354248046875,1575.93896484375,1575.5244140625,1575.10986328125,1574.69580078125,1574.28271484375,1573.870361328125,1573.458251953125,1573.0469970703125,1572.6358642578125,1572.2257080078125,1571.816162109375,1571.406982421875,1570.99853515625,1570.59033203125,1570.183349609375,1569.7764892578125,1569.370361328125,1568.965087890625,1568.5596923828125,1568.155029296875,1567.7515869140625,1567.34814453125,1566.9453125,1566.5426025390625,1566.1412353515625,1565.740478515625,1565.339599609375,1564.939208984375,1564.539794921875,1564.140869140625,1563.74267578125,1563.3447265625,1562.947509765625,1562.550537109375,1562.154296875,1561.75830078125,1561.363525390625,1560.9691162109375,1560.57470703125,1560.18115234375,1559.7880859375,1559.3956298828125,1559.003662109375,1558.6119384765625,1558.2208251953125,1557.830322265625,1557.4404296875,1557.05126953125,1556.662109375,1556.2735595703125,1555.885986328125,1555.49853515625,1555.11181640625,1554.72509765625,1554.33935546875,1553.953857421875,1553.5693359375,1553.185302734375,1552.8011474609375,1552.41748046875,1552.034912109375,1551.652587890625,1551.270263671875,1550.8887939453125,1550.5078125,1550.127685546875,1549.74755859375,1549.3681640625,1548.989013671875,1548.6107177734375,1548.23291015625,1547.85498046875,1547.4779052734375,1547.101318359375,1546.725341796875,1546.349853515625,1545.974365234375,1545.5997314453125,1545.225830078125,1544.851806640625,1544.478271484375,1544.105712890625,1543.73291015625,1543.361328125,1542.98974609375,1542.618896484375,1542.248291015625,1541.878173828125,1541.5087890625,1541.139404296875,1540.7706298828125,1540.40234375,1540.0347900390625,1539.667236328125,1539.30029296875,1538.93359375,1538.567626953125,1538.2022705078125,1537.83740234375,1537.4722900390625,1537.107666015625,1536.744384765625,1536.380615234375,1536.0177001953125,1535.6553955078125,1535.29345703125,1534.931396484375,1534.5703125,1534.209228515625,1533.8487548828125,1533.4892578125,1533.129150390625,1532.77001953125,1532.4111328125,1532.0535888671875,1531.6951904296875,1531.337890625,1530.981201171875,1530.624755859375,1530.2679443359375,1529.912353515625,1529.556884765625,1529.2025146484375,1528.84765625,1528.49365234375,1528.1396484375,1527.7864990234375,1527.43359375,1527.0810546875,1526.7288818359375,1526.3773193359375,1526.0257568359375,1525.675048828125,1525.324462890625,1524.974365234375,1524.62451171875,1524.2757568359375,1523.9263916015625,1523.5782470703125,1523.22998046875,1522.882568359375,1522.53515625,1522.188232421875,1521.842041015625,1521.495361328125,1521.149658203125,1520.8040771484375,1520.4593505859375,1520.114501953125,1519.770263671875,1519.42626953125,1519.082763671875,1518.740234375,1518.397216796875,1518.0548095703125,1517.713134765625,1517.37109375,1517.02978515625,1516.689453125,1516.3486328125,1516.008544921875,1515.668701171875,1515.329833984375,1514.990478515625,1514.6522216796875,1514.313720703125,1513.976318359375,1513.638671875,1513.3016357421875,1512.964599609375,1512.6282958984375,1512.2921142578125,1511.9561767578125,1511.621337890625,1511.2861328125,1510.9515380859375,1510.617431640625,1510.283203125,1509.9498291015625,1509.61669921875,1509.283935546875,1508.951416015625,1508.6192626953125,1508.28759765625,1507.9560546875,1507.624755859375,1507.29443359375,1506.9637451171875,1506.633544921875,1506.3040771484375,1505.974609375,1505.6458740234375,1505.31689453125,1504.989013671875,1504.66064453125,1504.333251953125,1504.006103515625,1503.67919921875,1503.352294921875,1503.02587890625,1502.699951171875,1502.374755859375,1502.049072265625,1501.724365234375,1501.3995361328125,1501.075439453125,1500.751220703125,1500.427978515625,1500.1043701171875,1499.7816162109375,1499.459228515625,1499.13671875,1498.814697265625,1498.4932861328125,1498.172119140625,1497.8507080078125,1497.5302734375,1497.2100830078125,1496.8897705078125,1496.5703125,1496.2508544921875,1495.931884765625,1495.6129150390625,1495.294677734375,1494.976318359375,1494.6585693359375,1494.3411865234375,1494.0238037109375,1493.70751953125,1493.390625,1493.07470703125,1492.7586669921875,1492.44287109375,1492.127685546875,1491.812744140625,1491.4979248046875,1491.183837890625,1490.869873046875,1490.5560302734375,1490.242431640625,1489.929443359375,1489.616455078125,1489.3043212890625,1488.9921875,1488.679931640625,1488.368408203125,1488.0577392578125,1487.746337890625,1487.435791015625,1487.1251220703125,1486.815185546875,1486.50537109375,1486.1956787109375,1485.88720703125,1485.578125,1485.2691650390625,1484.9609375,1484.65283203125,1484.3453369140625,1484.037841796875,1483.7305908203125,1483.424072265625,1483.117431640625,1482.8111572265625,1482.5048828125,1482.1993408203125,1481.89404296875,1481.5889892578125,1481.2841796875,1480.9796142578125,1480.675048828125,1480.371337890625,1480.0677490234375,1479.7646484375,1479.461181640625,1479.158447265625,1478.85595703125,1478.553466796875,1478.25146484375,1477.9501953125,1477.648193359375,1477.347412109375,1477.046630859375,1476.74609375,1476.445556640625,1476.1453857421875,1475.845947265625,1475.54638671875,1475.2471923828125,1474.948486328125,1474.64990234375,1474.3511962890625,1474.05322265625,1473.75537109375,1473.4580078125,1473.16064453125,1472.86376953125,1472.567138671875,1472.2708740234375,1471.974365234375,1471.6783447265625,1471.3826904296875,1471.087158203125,1470.792236328125,1470.4970703125,1470.202392578125,1469.9083251953125,1469.614501953125,1469.32080078125,1469.026611328125,1468.733642578125,1468.440673828125,1468.1478271484375,1467.85546875,1467.562744140625,1467.271484375,1466.9796142578125,1466.688232421875,1466.39697265625,1466.10595703125,1465.815185546875,1465.5247802734375,1465.235107421875,1464.94482421875,1464.6551513671875,1464.3658447265625,1464.07666015625,1463.78759765625,1463.4990234375,1463.21044921875,1462.9227294921875,1462.6346435546875,1462.34716796875,1462.0596923828125,1461.7724609375,1461.486083984375,1461.198974609375,1460.91259765625,1460.626220703125,1460.3406982421875,1460.05517578125,1459.76953125,1459.484619140625,1459.1995849609375,1458.914794921875,1458.630615234375,1458.3463134765625,1458.0623779296875,1457.7789306640625,1457.4951171875,1457.212158203125,1456.9296875,1456.6466064453125,1456.364013671875,1456.081787109375,1455.7998046875,1455.51806640625,1455.236328125,1454.955078125,1454.674072265625,1454.3931884765625,1454.11279296875,1453.832275390625,1453.552490234375,1453.2728271484375,1452.992919921875,1452.7135009765625,1452.43408203125,1452.155517578125,1451.876708984375,1451.5982666015625,1451.320068359375,1451.042236328125,1450.7645263671875,1450.4869384765625,1450.20947265625,1449.93212890625,1449.655517578125,1449.3790283203125,1449.1026611328125,1448.826416015625,1448.551025390625,1448.2750244140625,1447.99951171875,1447.72412109375,1447.4490966796875,1447.174560546875,1446.9000244140625,1446.625732421875,1446.3514404296875,1446.077880859375,1445.803955078125,1445.530517578125,1445.257080078125,1444.984375,1444.711181640625,1444.4390869140625,1444.166748046875,1443.894287109375,1443.6224365234375,1443.350830078125,1443.07958984375,1442.808349609375,1442.537353515625,1442.2669677734375,1441.995849609375,1441.7257080078125,1441.45556640625,1441.185546875,1440.915771484375,1440.6463623046875,1440.377197265625,1440.107666015625,1439.8388671875,1439.570556640625,1439.30224609375,1439.033935546875,1438.76611328125,1438.4979248046875,1438.2303466796875,1437.963134765625,1437.6962890625,1437.42919921875,1437.16259765625,1436.8961181640625,1436.6295166015625,1436.3634033203125,1436.0975341796875,1435.8321533203125,1435.566650390625,1435.30126953125,1435.0364990234375,1434.771484375,1434.5068359375,1434.24267578125,1433.978271484375,1433.71435546875,1433.4500732421875,1433.1866455078125,1432.923095703125,1432.660400390625,1432.3974609375,1432.134521484375,1431.8721923828125,1431.609619140625,1431.3475341796875,1431.0853271484375,1430.823974609375,1430.561767578125,1430.300537109375,1430.03955078125,1429.7783203125,1429.517822265625,1429.25732421875,1428.99658203125,1428.7366943359375,1428.47705078125,1428.2166748046875,1427.95751953125,1427.6982421875,1427.4385986328125,1427.1796875,1426.921142578125,1426.6627197265625,1426.404296875,1426.1461181640625,1425.887939453125,1425.630126953125,1425.37255859375,1425.115234375,1424.85791015625,1424.600830078125,1424.3438720703125,1424.08740234375,1423.830810546875,1423.5748291015625,1423.31884765625,1423.0625,1422.8072509765625,1422.5516357421875,1422.2958984375,1422.041015625,1421.7861328125,1421.53173828125,1421.27734375,1421.022705078125,1420.7685546875,1420.5146484375,1420.2608642578125,1420.007080078125,1419.7537841796875,1419.5003662109375,1419.247314453125,1418.994384765625,1418.7423095703125,1418.4893798828125,1418.2371826171875,1417.98486328125,1417.73291015625,1417.4814453125,1417.229736328125,1416.978759765625,1416.7269287109375,1416.47607421875,1416.22509765625,1415.974853515625,1415.724365234375,1415.47412109375,1415.2242431640625,1414.9739990234375,1414.7242431640625,1414.474853515625,1414.225341796875,1413.976318359375,1413.727294921875,1413.478515625,1413.229736328125,1412.981201171875,1412.73291015625,1412.48486328125,1412.236572265625,1411.9891357421875,1411.7418212890625,1411.4940185546875,1411.246826171875,1410.999755859375,1410.753173828125,1410.5064697265625,1410.260009765625,1410.013671875,1409.767333984375,1409.521484375,1409.2760009765625,1409.02978515625,1408.7841796875,1408.5391845703125,1408.2935791015625,1408.0491943359375,1407.8043212890625,1407.5596923828125,1407.315673828125,1407.07080078125,1406.8267822265625,1406.5830078125,1406.33935546875,1406.095947265625,1405.8521728515625,1405.6090087890625,1405.3658447265625,1405.12353515625,1404.88037109375,1404.6376953125,1404.395263671875,1404.1531982421875,1403.9111328125,1403.669189453125,1403.427734375,1403.18603515625,1402.944580078125,1402.7032470703125,1402.4627685546875,1402.221435546875,1401.9808349609375,1401.7406005859375,1401.499755859375,1401.260009765625,1401.01953125,1400.7801513671875,1400.5400390625,1400.30029296875,1400.0609130859375,1399.822265625,1399.58251953125,1399.3441162109375,1399.105224609375,1398.86669921875,1398.6280517578125,1398.39013671875,1398.152099609375,1397.9140625,1397.676025390625,1397.4390869140625,1397.2012939453125,1396.964111328125,1396.72705078125,1396.489990234375,1396.2532958984375,1396.016845703125,1395.780029296875,1395.543701171875,1395.3076171875,1395.07177734375,1394.835693359375,1394.5997314453125,1394.3646240234375,1394.12939453125,1393.894287109375,1393.658935546875,1393.4241943359375,1393.1895751953125,1392.9547119140625,1392.720458984375,1392.486328125,1392.251953125,1392.01806640625,1391.784423828125,1391.55078125,1391.3173828125,1391.083984375,1390.850830078125,1390.6177978515625,1390.3848876953125,1390.1524658203125,1389.919677734375,1389.6875,1389.455078125,1389.22314453125,1388.9912109375,1388.7591552734375,1388.527587890625,1388.296142578125,1388.0643310546875,1387.833740234375,1387.602294921875,1387.37158203125,1387.1409912109375,1386.9102783203125,1386.6796875,1386.449951171875,1386.2198486328125,1385.989501953125,1385.760009765625,1385.5301513671875,1385.300537109375,1385.071044921875,1384.842041015625,1384.6129150390625,1384.3841552734375,1384.155029296875,1383.9263916015625,1383.6983642578125,1383.4697265625,1383.24169921875,1383.013671875,1382.7857666015625,1382.557861328125,1382.330322265625,1382.10302734375,1381.875732421875,1381.6484375,1381.4215087890625,1381.194580078125,1380.9674072265625,1380.740966796875,1380.514892578125,1380.28857421875,1380.0623779296875,1379.83642578125,1379.6103515625,1379.384765625,1379.159423828125,1378.93359375,1378.7083740234375,1378.4835205078125,1378.258056640625,1378.0333251953125,1377.808837890625,1377.583984375,1377.3602294921875,1377.1357421875,1376.9112548828125,1376.687255859375,1376.463134765625,1376.2396240234375,1376.0164794921875,1375.7928466796875,1375.5693359375,1375.346435546875,1375.123291015625,1374.900390625,1374.6773681640625,1374.455078125,1374.232421875,1374.009765625,1373.7880859375,1373.566162109375,1373.34423828125,1373.122314453125,1372.90087890625,1372.6795654296875,1372.4580078125,1372.23681640625,1372.0155029296875,1371.794677734375,1371.5738525390625,1371.3529052734375,1371.1324462890625,1370.9122314453125,1370.6920166015625,1370.4715576171875,1370.251708984375,1370.031982421875,1369.812255859375,1369.5924072265625,1369.3729248046875,1369.15380859375,1368.934814453125,1368.7152099609375,1368.496826171875,1368.27783203125,1368.05908203125,1367.8408203125,1367.6224365234375,1367.40380859375,1367.185791015625,1366.9677734375,1366.75,1366.532470703125,1366.314697265625,1366.097412109375,1365.879638671875,1365.6630859375,1365.4456787109375,1365.2288818359375,1365.0120849609375,1364.795166015625,1364.5787353515625,1364.3623046875,1364.145751953125,1363.929931640625,1363.7138671875,1363.4979248046875,1363.2822265625,1363.0667724609375,1362.8509521484375,1362.6357421875,1362.420166015625,1362.205322265625,1361.990234375,1361.7752685546875,1361.560546875,1361.34619140625,1361.1317138671875,1360.91748046875,1360.702880859375,1360.4891357421875,1360.27490234375,1360.061279296875,1359.84765625,1359.6337890625,1359.42041015625,1359.206787109375,1358.9940185546875,1358.78076171875,1358.56787109375,1358.355224609375,1358.1419677734375,1357.929931640625,1357.7174072265625,1357.5052490234375,1357.2928466796875,1357.080810546875,1356.868896484375,1356.65673828125,1356.445556640625,1356.2337646484375,1356.022216796875,1355.810791015625,1355.60009765625,1355.388671875,1355.177734375,1354.9669189453125,1354.75634765625,1354.54541015625,1354.335205078125,1354.124755859375,1353.9146728515625,1353.70458984375,1353.49462890625,1353.2843017578125,1353.07470703125,1352.864990234375,1352.655517578125,1352.4462890625,1352.2369384765625,1352.02783203125,1351.8184814453125,1351.60986328125,1351.4013671875,1351.192626953125,1350.9837646484375,1350.775634765625,1350.5673828125,1350.359130859375,1350.1512451171875,1349.9432373046875,1349.735595703125,1349.5274658203125,1349.3203125,1349.1126708984375,1348.9052734375,1348.697509765625,1348.490966796875,1348.283935546875,1348.0771484375,1347.8702392578125,1347.6636962890625,1347.457275390625,1347.250732421875,1347.0445556640625,1346.8385009765625,1346.632080078125,1346.426513671875,1346.2205810546875,1346.014404296875,1345.8095703125,1345.603759765625,1345.398193359375,1345.193359375,1344.9881591796875,1344.782958984375,1344.577880859375,1344.373291015625,1344.168701171875,1343.964111328125,1343.759765625,1343.555419921875,1343.3511962890625,1343.147216796875,1342.9429931640625,1342.7392578125,1342.535888671875,1342.3316650390625,1342.12841796875,1341.9251708984375,1341.721923828125,1341.518310546875,1341.3155517578125,1341.11279296875,1340.90966796875,1340.70703125,1340.50439453125,1340.3017578125,1340.099365234375,1339.897216796875,1339.69482421875,1339.4930419921875,1339.290771484375,1339.089111328125,1338.88720703125,1338.685791015625,1338.484130859375,1338.282958984375,1338.08154296875,1337.88037109375,1337.67919921875,1337.4783935546875,1337.27734375,1337.076904296875,1336.8759765625,1336.675537109375,1336.475341796875,1336.2747802734375,1336.0748291015625,1335.8746337890625,1335.6748046875,1335.4747314453125,1335.27490234375,1335.07568359375,1334.876220703125,1334.676513671875,1334.4771728515625,1334.2783203125,1334.078857421875,1333.880126953125,1333.68115234375,1333.482666015625,1333.283447265625,1333.0849609375,1332.88671875,1332.688232421875,1332.490234375,1332.2921142578125,1332.09423828125,1331.8961181640625,1331.6982421875,1331.500732421875,1331.302978515625,1331.10546875,1330.907958984375,1330.7105712890625,1330.513427734375,1330.31640625,1330.11962890625,1329.922607421875,1329.72607421875,1329.5291748046875,1329.332763671875,1329.13623046875,1328.93994140625,1328.74365234375,1328.547607421875,1328.3516845703125,1328.1553955078125,1327.9595947265625,1327.7640380859375,1327.5684814453125,1327.372802734375,1327.177490234375,1326.982177734375,1326.7872314453125,1326.5921630859375,1326.3970947265625,1326.2021484375,1326.0074462890625,1325.8125,1325.6177978515625,1325.4232177734375,1325.22900390625,1325.0347900390625,1324.840576171875,1324.646484375,1324.452880859375,1324.2591552734375,1324.064697265625,1323.87158203125,1323.677734375,1323.4842529296875,1323.291259765625,1323.097412109375,1322.904541015625,1322.711669921875,1322.518798828125,1322.326171875,1322.1328125,1321.9404296875,1321.7474365234375,1321.55517578125,1321.36279296875,1321.170654296875,1320.978515625,1320.78662109375,1320.5947265625,1320.403076171875,1320.210693359375,1320.0194091796875,1319.8275146484375,1319.63623046875,1319.4449462890625,1319.253173828125,1319.0623779296875,1318.8712158203125,1318.680419921875,1318.4892578125,1318.299072265625,1318.108154296875,1317.91748046875,1317.7269287109375,1317.53662109375,1317.3465576171875,1317.156005859375,1316.966064453125,1316.775634765625,1316.5860595703125,1316.396240234375,1316.20654296875,1316.017333984375,1315.82763671875,1315.637939453125,1315.4486083984375,1315.259521484375,1315.0701904296875,1314.8814697265625,1314.6923828125,1314.50341796875,1314.314697265625,1314.126220703125,1313.9375,1313.7493896484375,1313.56103515625,1313.372314453125,1313.1842041015625,1312.996337890625,1312.8082275390625,1312.6204833984375,1312.4327392578125,1312.2449951171875,1312.0572509765625,1311.869873046875,1311.682373046875,1311.494873046875,1311.307861328125,1311.12060546875,1310.9334716796875,1310.7467041015625,1310.5599365234375,1310.3734130859375,1310.186279296875,1309.999755859375,1309.813232421875,1309.627197265625,1309.440673828125,1309.2547607421875,1309.068115234375,1308.8828125,1308.6962890625,1308.5106201171875,1308.32470703125,1308.13916015625,1307.953369140625,1307.768310546875,1307.5828857421875,1307.3973388671875,1307.212158203125,1307.027099609375,1306.8424072265625,1306.6575927734375,1306.472412109375,1306.2877197265625,1306.10302734375,1305.9183349609375,1305.734130859375,1305.54931640625,1305.365478515625,1305.18115234375,1304.997314453125,1304.8133544921875,1304.629150390625,1304.445556640625,1304.261474609375,1304.078125,1303.894287109375,1303.7109375,1303.52734375,1303.3443603515625,1303.1610107421875,1302.9781494140625,1302.794921875,1302.61181640625,1302.42919921875,1302.24609375,1302.063720703125,1301.881103515625,1301.6983642578125,1301.515869140625,1301.3341064453125,1301.151611328125,1300.96923828125,1300.787353515625,1300.6055908203125,1300.423828125,1300.2420654296875,1300.060302734375,1299.878662109375,1299.69677734375,1299.515380859375,1299.334228515625,1299.15283203125,1298.971923828125,1298.79052734375,1298.6097412109375,1298.4285888671875,1298.248046875,1298.0672607421875,1297.8865966796875,1297.7060546875,1297.525634765625,1297.344970703125,1297.1646728515625,1296.984375,1296.80419921875,1296.62451171875,1296.4443359375,1296.264404296875,1296.0848388671875,1295.9049072265625,1295.7255859375,1295.54541015625,1295.3662109375,1295.1868896484375,1295.0078125,1294.82861328125,1294.649658203125,1294.470458984375,1294.2913818359375,1294.1123046875,1293.9339599609375,1293.7550048828125,1293.57666015625,1293.39794921875,1293.219482421875,1293.041015625,1292.86279296875,1292.6846923828125,1292.5064697265625,1292.3287353515625,1292.150390625,1291.97265625,1291.7947998046875,1291.6171875,1291.439697265625,1291.26220703125,1291.0845947265625,1290.9075927734375,1290.7301025390625,1290.55322265625,1290.3758544921875,1290.1988525390625,1290.0218505859375,1289.84521484375,1289.66796875,1289.49169921875,1289.31494140625,1289.1385498046875,1288.9619140625,1288.785888671875,1288.609130859375,1288.43310546875,1288.257080078125,1288.0809326171875,1287.90478515625,1287.7294921875,1287.5533447265625,1287.377685546875,1287.201904296875,1287.0264892578125,1286.8511962890625,1286.675537109375,1286.50048828125,1286.324951171875,1286.150146484375,1285.97509765625,1285.799560546875,1285.625244140625,1285.4501953125,1285.275390625,1285.100830078125,1284.926513671875,1284.751708984375,1284.577392578125,1284.40283203125,1284.228515625,1284.0546875,1283.880126953125,1283.706298828125,1283.5322265625,1283.3585205078125,1283.184814453125,1283.010986328125,1282.83740234375,1282.663818359375,1282.490478515625,1282.31689453125,1282.143798828125,1281.9705810546875,1281.79736328125,1281.6243896484375,1281.4512939453125,1281.2781982421875,1281.1055908203125,1280.9327392578125,1280.760009765625,1280.587158203125,1280.4146728515625,1280.2421875,1280.0701904296875,1279.8974609375,1279.725830078125,1279.55322265625,1279.38134765625,1279.209228515625,1279.0377197265625,1278.8656005859375,1278.694091796875,1278.522216796875,1278.3507080078125,1278.1790771484375,1278.007568359375,1277.8365478515625,1277.664794921875,1277.49365234375,1277.322509765625,1277.1513671875,1276.9805908203125,1276.809814453125,1276.638671875,1276.468017578125,1276.2974853515625,1276.126708984375,1275.9560546875,1275.785888671875,1275.615234375,1275.44482421875,1275.2747802734375,1275.1044921875,1274.934814453125,1274.7646484375,1274.594482421875,1274.4251708984375,1274.255126953125,1274.08544921875,1273.9156494140625,1273.746337890625,1273.576904296875,1273.407470703125,1273.2384033203125,1273.0689697265625,1272.8997802734375,1272.730712890625,1272.5616455078125,1272.3924560546875,1272.2236328125,1272.054931640625,1271.88623046875,1271.717529296875,1271.548583984375,1271.38037109375,1271.2120361328125,1271.0435791015625,1270.87548828125,1270.7071533203125,1270.538818359375,1270.37109375,1270.20263671875,1270.03515625,1269.8670654296875,1269.699462890625,1269.53173828125,1269.364013671875,1269.196533203125,1269.029296875,1268.8614501953125,1268.6943359375,1268.5269775390625,1268.35986328125,1268.192626953125,1268.0257568359375,1267.858642578125,1267.69189453125,1267.5247802734375,1267.3582763671875,1267.191650390625,1267.0247802734375,1266.858642578125,1266.692138671875,1266.525634765625,1266.3590087890625,1266.19287109375,1266.02685546875,1265.860595703125,1265.694091796875,1265.5289306640625,1265.362548828125,1265.197021484375,1265.031005859375,1264.8653564453125,1264.7001953125,1264.534423828125,1264.3690185546875,1264.20361328125,1264.0380859375,1263.873046875,1263.7076416015625,1263.54296875,1263.37744140625,1263.212646484375,1263.04736328125,1262.8828125,1262.71826171875,1262.553466796875,1262.388671875,1262.224365234375,1262.0595703125,1261.8953857421875,1261.73095703125,1261.566650390625,1261.40234375,1261.2384033203125,1261.0745849609375,1260.9102783203125,1260.746337890625,1260.582763671875,1260.418701171875,1260.255126953125,1260.09130859375,1259.9281005859375,1259.764404296875,1259.601318359375,1259.437744140625,1259.2744140625,1259.11083984375,1258.94775390625,1258.7847900390625,1258.6220703125,1258.458984375,1258.2958984375,1258.133056640625,1257.970458984375,1257.807861328125,1257.644775390625,1257.482421875,1257.31982421875,1257.15771484375,1256.9952392578125,1256.8328857421875,1256.670654296875,1256.508544921875,1256.346435546875,1256.1845703125,1256.022705078125,1255.86083984375,1255.6986083984375,1255.537109375,1255.37548828125,1255.2138671875,1255.0523681640625,1254.890625,1254.729248046875,1254.56787109375,1254.406982421875,1254.245849609375,1254.084716796875,1253.9237060546875,1253.7626953125,1253.601806640625,1253.44091796875,1253.2802734375,1253.1192626953125,1252.95849609375,1252.798095703125,1252.637451171875,1252.47705078125,1252.3165283203125,1252.156005859375,1251.99560546875,1251.8359375,1251.6756591796875,1251.515625,1251.3558349609375,1251.1959228515625,1251.0361328125,1250.875732421875,1250.71630859375,1250.556396484375,1250.39697265625,1250.237548828125,1250.0780029296875,1249.918212890625,1249.7593994140625,1249.5999755859375,1249.4404296875,1249.28125,1249.122314453125,1248.9632568359375,1248.80419921875,1248.6453857421875,1248.48681640625,1248.327880859375,1248.1689453125,1248.0107421875,1247.85205078125,1247.6934814453125,1247.53515625,1247.376708984375,1247.21826171875,1247.06005859375,1246.90185546875,1246.744140625,1246.5858154296875,1246.427490234375,1246.2696533203125,1246.1119384765625,1245.9541015625,1245.7965087890625,1245.6387939453125,1245.481201171875,1245.323486328125,1245.166015625,1245.0087890625,1244.8511962890625,1244.694091796875,1244.536865234375,1244.3795166015625,1244.22216796875,1244.0654296875,1243.9085693359375,1243.75146484375,1243.594482421875,1243.437744140625,1243.281005859375,1243.124755859375,1242.968017578125,1242.811767578125,1242.655029296875,1242.49853515625,1242.342529296875,1242.18603515625,1242.02978515625,1241.87353515625,1241.7174072265625,1241.5614013671875,1241.405517578125,1241.24951171875,1241.09375,1240.9376220703125,1240.7821044921875,1240.626708984375,1240.470947265625,1240.3153076171875,1240.159912109375,1240.0042724609375,1239.848876953125,1239.6937255859375,1239.5384521484375,1239.3834228515625,1239.228271484375,1239.072998046875,1238.9180908203125,1238.7633056640625,1238.6083984375,1238.453857421875,1238.298828125,1238.14453125,1237.989501953125,1237.8349609375,1237.6802978515625,1237.5262451171875,1237.371337890625,1237.21728515625,1237.0631103515625,1236.9090576171875,1236.754638671875,1236.6007080078125,1236.44677734375,1236.292724609375,1236.138916015625,1235.9849853515625,1235.8310546875,1235.6773681640625,1235.523681640625,1235.370361328125,1235.216552734375,1235.063232421875,1234.9097900390625,1234.756103515625,1234.6031494140625,1234.4498291015625,1234.2965087890625,1234.143798828125,1233.9906005859375,1233.837646484375,1233.6849365234375,1233.531982421875,1233.37890625,1233.22607421875,1233.0733642578125,1232.9207763671875]};

  var data = [data0];
  var layout = {"title":"loss on time"};

  Plotly.plot('plot-2000235454', data, layout);
})();
});
      
</script>
</div>

</div>

<div class="output_area">
<div class="prompt output_prompt">Out[9]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">lossSeq</span>: <span class="ansi-green-fg">Seq</span>[<span class="ansi-green-fg">Double</span>] = <span class="ansi-yellow-fg">List</span>(
  <span class="ansi-green-fg">2304.05615234375</span>,
  <span class="ansi-green-fg">2284.93359375</span>,
  <span class="ansi-green-fg">2275.099365234375</span>,
  <span class="ansi-green-fg">2266.16552734375</span>,
  <span class="ansi-green-fg">2257.55029296875</span>,
  <span class="ansi-green-fg">2249.195068359375</span>,
  <span class="ansi-green-fg">2241.08251953125</span>,
  <span class="ansi-green-fg">2233.203125</span>,
  <span class="ansi-green-fg">2225.54736328125</span>,
  <span class="ansi-green-fg">2218.10400390625</span>,
  <span class="ansi-green-fg">2210.8662109375</span>,
<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">plot</span>: <span class="ansi-green-fg">Seq</span>[<span class="ansi-green-fg">Scatter</span>] = <span class="ansi-yellow-fg">List</span>(
  <span class="ansi-yellow-fg">Scatter</span>(
    <span class="ansi-yellow-fg">Some</span>(
      <span class="ansi-yellow-fg">Doubles</span>(
        <span class="ansi-yellow-fg">Vector</span>(
          <span class="ansi-green-fg">0.0</span>,
          <span class="ansi-green-fg">1.0</span>,
          <span class="ansi-green-fg">2.0</span>,
          <span class="ansi-green-fg">3.0</span>,
          <span class="ansi-green-fg">4.0</span>,
          <span class="ansi-green-fg">5.0</span>,
          <span class="ansi-green-fg">6.0</span>,
<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">res8_4</span>: <span class="ansi-green-fg">String</span> = <span class="ansi-green-fg">&#34;plot-2000235454&#34;</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>10.使用训练后的神经网络判断测试数据的标签</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">myNeuralNetwork</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">test_data</span><span class="o">)</span>
  <span class="n">println</span><span class="o">(</span><span class="s">s&quot;result: </span><span class="si">$result</span><span class="s">&quot;</span><span class="o">)</span> <span class="c1">//输出判断结果</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>result: [[0.03, 0.05, 0.17, 0.13, 0.01, 0.13, 0.42, 0.00, 0.04, 0.00],
 [0.03, 0.17, 0.00, 0.05, 0.00, 0.01, 0.00, 0.00, 0.18, 0.55],
 [0.08, 0.09, 0.01, 0.03, 0.01, 0.00, 0.00, 0.01, 0.71, 0.05],
 [0.50, 0.04, 0.07, 0.04, 0.01, 0.02, 0.00, 0.03, 0.21, 0.08],
 [0.02, 0.03, 0.09, 0.05, 0.36, 0.10, 0.25, 0.08, 0.01, 0.00],
 [0.01, 0.07, 0.02, 0.62, 0.02, 0.04, 0.14, 0.05, 0.00, 0.03],
 [0.00, 0.00, 0.00, 0.69, 0.02, 0.05, 0.23, 0.00, 0.00, 0.00],
 [0.04, 0.01, 0.24, 0.04, 0.26, 0.10, 0.17, 0.11, 0.01, 0.01],
 [0.05, 0.17, 0.22, 0.09, 0.17, 0.16, 0.09, 0.03, 0.01, 0.01],
 [0.09, 0.44, 0.02, 0.03, 0.00, 0.06, 0.01, 0.01, 0.11, 0.24],
 [0.33, 0.09, 0.04, 0.09, 0.02, 0.07, 0.04, 0.02, 0.28, 0.03],
 [0.02, 0.49, 0.02, 0.05, 0.01, 0.01, 0.01, 0.03, 0.04, 0.33],
 [0.02, 0.32, 0.02, 0.24, 0.12, 0.07, 0.15, 0.07, 0.01, 0.01],
 [0.04, 0.21, 0.03, 0.07, 0.05, 0.01, 0.15, 0.01, 0.29, 0.15],
 [0.24, 0.18, 0.05, 0.17, 0.02, 0.04, 0.03, 0.13, 0.07, 0.06],
 [0.19, 0.03, 0.19, 0.02, 0.09, 0.06, 0.03, 0.03, 0.31, 0.04],
 [0.12, 0.04, 0.00, 0.07, 0.01, 0.21, 0.44, 0.06, 0.04, 0.01],
 [0.19, 0.03, 0.15, 0.21, 0.06, 0.02, 0.11, 0.19, 0.02, 0.02],
 [0.05, 0.12, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.46, 0.34],
 [0.01, 0.11, 0.06, 0.22, 0.06, 0.03, 0.35, 0.16, 0.00, 0.01],
 [0.18, 0.07, 0.08, 0.03, 0.19, 0.11, 0.06, 0.10, 0.13, 0.05],
 [0.18, 0.00, 0.47, 0.01, 0.21, 0.09, 0.00, 0.03, 0.01, 0.00],
 [0.48, 0.03, 0.04, 0.02, 0.01, 0.01, 0.00, 0.03, 0.35, 0.03],
 [0.01, 0.36, 0.02, 0.02, 0.06, 0.15, 0.06, 0.06, 0.01, 0.25],
 [0.02, 0.01, 0.23, 0.01, 0.42, 0.03, 0.01, 0.25, 0.00, 0.02],
 [0.05, 0.08, 0.23, 0.01, 0.06, 0.04, 0.30, 0.05, 0.12, 0.07],
 [0.10, 0.06, 0.07, 0.05, 0.14, 0.06, 0.40, 0.05, 0.04, 0.03],
 [0.31, 0.07, 0.14, 0.04, 0.09, 0.01, 0.02, 0.14, 0.09, 0.10],
 [0.10, 0.25, 0.22, 0.04, 0.15, 0.02, 0.06, 0.05, 0.02, 0.09],
 [0.00, 0.03, 0.09, 0.06, 0.18, 0.09, 0.47, 0.06, 0.00, 0.02],
 [0.02, 0.23, 0.13, 0.11, 0.10, 0.16, 0.16, 0.05, 0.01, 0.04],
 [0.03, 0.01, 0.17, 0.11, 0.12, 0.39, 0.09, 0.06, 0.01, 0.01],
 [0.08, 0.03, 0.10, 0.25, 0.10, 0.17, 0.16, 0.07, 0.04, 0.01],
 [0.07, 0.08, 0.13, 0.10, 0.03, 0.15, 0.21, 0.13, 0.02, 0.07],
 [0.06, 0.03, 0.00, 0.01, 0.01, 0.00, 0.01, 0.02, 0.69, 0.17],
 [0.03, 0.13, 0.11, 0.06, 0.11, 0.12, 0.17, 0.19, 0.03, 0.04],
 [0.02, 0.07, 0.11, 0.09, 0.39, 0.08, 0.14, 0.05, 0.02, 0.04],
 [0.02, 0.12, 0.00, 0.06, 0.00, 0.01, 0.02, 0.01, 0.05, 0.71],
 [0.05, 0.27, 0.01, 0.13, 0.02, 0.09, 0.03, 0.02, 0.06, 0.33],
 [0.19, 0.01, 0.10, 0.13, 0.05, 0.26, 0.05, 0.18, 0.03, 0.00],
 [0.69, 0.00, 0.03, 0.03, 0.04, 0.01, 0.02, 0.12, 0.03, 0.04],
 [0.05, 0.01, 0.16, 0.03, 0.30, 0.04, 0.22, 0.15, 0.03, 0.01],
 [0.07, 0.04, 0.01, 0.44, 0.05, 0.11, 0.02, 0.19, 0.01, 0.05],
 [0.03, 0.04, 0.19, 0.07, 0.19, 0.04, 0.32, 0.08, 0.02, 0.01],
 [0.56, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.08, 0.27, 0.08],
 [0.06, 0.16, 0.00, 0.10, 0.02, 0.01, 0.01, 0.14, 0.10, 0.39],
 [0.00, 0.09, 0.04, 0.17, 0.13, 0.13, 0.41, 0.01, 0.01, 0.01],
 [0.17, 0.04, 0.00, 0.24, 0.02, 0.07, 0.02, 0.12, 0.28, 0.04],
 [0.01, 0.01, 0.37, 0.02, 0.22, 0.04, 0.22, 0.09, 0.00, 0.01],
 [0.01, 0.02, 0.17, 0.02, 0.35, 0.03, 0.33, 0.06, 0.00, 0.01],
 [0.14, 0.04, 0.01, 0.00, 0.00, 0.00, 0.00, 0.01, 0.40, 0.39],
 [0.14, 0.08, 0.03, 0.01, 0.08, 0.01, 0.01, 0.29, 0.15, 0.20],
 [0.01, 0.03, 0.02, 0.26, 0.05, 0.07, 0.48, 0.04, 0.03, 0.03],
 [0.02, 0.05, 0.02, 0.23, 0.05, 0.06, 0.31, 0.04, 0.08, 0.14],
 [0.02, 0.03, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.93, 0.01],
 [0.13, 0.17, 0.05, 0.06, 0.03, 0.02, 0.03, 0.03, 0.32, 0.17],
 [0.12, 0.00, 0.07, 0.06, 0.20, 0.04, 0.08, 0.39, 0.04, 0.00],
 [0.05, 0.06, 0.14, 0.26, 0.02, 0.05, 0.11, 0.02, 0.23, 0.05],
 [0.06, 0.22, 0.08, 0.16, 0.10, 0.04, 0.11, 0.06, 0.07, 0.11],
 [0.08, 0.01, 0.42, 0.04, 0.22, 0.04, 0.02, 0.11, 0.04, 0.02],
 [0.01, 0.00, 0.16, 0.01, 0.34, 0.14, 0.22, 0.11, 0.00, 0.00],
 [0.00, 0.01, 0.09, 0.13, 0.20, 0.35, 0.21, 0.02, 0.00, 0.00],
 [0.02, 0.15, 0.01, 0.14, 0.09, 0.02, 0.38, 0.16, 0.02, 0.02],
 [0.03, 0.18, 0.37, 0.01, 0.13, 0.02, 0.02, 0.02, 0.07, 0.16],
 [0.03, 0.16, 0.09, 0.26, 0.06, 0.03, 0.22, 0.09, 0.01, 0.06],
 [0.05, 0.00, 0.20, 0.06, 0.18, 0.13, 0.22, 0.13, 0.01, 0.01],
 [0.04, 0.66, 0.03, 0.01, 0.02, 0.00, 0.02, 0.03, 0.01, 0.17],
 [0.16, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.01, 0.82, 0.00],
 [0.03, 0.02, 0.07, 0.37, 0.05, 0.06, 0.12, 0.06, 0.09, 0.12],
 [0.05, 0.12, 0.01, 0.02, 0.01, 0.00, 0.02, 0.08, 0.17, 0.52],
 [0.09, 0.05, 0.25, 0.12, 0.06, 0.05, 0.10, 0.22, 0.01, 0.05],
 [0.04, 0.05, 0.10, 0.16, 0.17, 0.12, 0.13, 0.22, 0.01, 0.01],
 [0.14, 0.18, 0.05, 0.16, 0.07, 0.05, 0.05, 0.05, 0.12, 0.14],
 [0.27, 0.01, 0.02, 0.02, 0.02, 0.02, 0.00, 0.02, 0.60, 0.02],
 [0.17, 0.02, 0.05, 0.03, 0.01, 0.01, 0.02, 0.07, 0.05, 0.55],
 [0.01, 0.01, 0.28, 0.01, 0.28, 0.01, 0.30, 0.08, 0.01, 0.01],
 [0.09, 0.24, 0.04, 0.11, 0.04, 0.06, 0.03, 0.11, 0.08, 0.20],
 [0.04, 0.13, 0.02, 0.28, 0.06, 0.22, 0.04, 0.04, 0.14, 0.04],
 [0.01, 0.16, 0.04, 0.34, 0.12, 0.09, 0.17, 0.01, 0.02, 0.04],
 [0.06, 0.37, 0.03, 0.06, 0.01, 0.01, 0.04, 0.03, 0.02, 0.37],
 [0.18, 0.05, 0.09, 0.07, 0.03, 0.03, 0.03, 0.03, 0.39, 0.10],
 [0.01, 0.24, 0.01, 0.39, 0.07, 0.08, 0.02, 0.03, 0.11, 0.04],
 [0.01, 0.04, 0.45, 0.09, 0.12, 0.05, 0.22, 0.01, 0.01, 0.00],
 [0.14, 0.02, 0.29, 0.08, 0.07, 0.05, 0.04, 0.05, 0.19, 0.07],
 [0.37, 0.11, 0.15, 0.15, 0.03, 0.01, 0.01, 0.04, 0.11, 0.03],
 [0.43, 0.00, 0.02, 0.00, 0.08, 0.00, 0.00, 0.25, 0.15, 0.07],
 [0.37, 0.02, 0.25, 0.04, 0.01, 0.15, 0.01, 0.07, 0.08, 0.01],
 [0.04, 0.13, 0.06, 0.06, 0.05, 0.04, 0.01, 0.03, 0.32, 0.27],
 [0.20, 0.02, 0.14, 0.26, 0.02, 0.06, 0.03, 0.03, 0.21, 0.04],
 [0.06, 0.16, 0.03, 0.01, 0.02, 0.02, 0.01, 0.01, 0.15, 0.54],
 [0.13, 0.16, 0.01, 0.02, 0.09, 0.01, 0.00, 0.04, 0.33, 0.20],
 [0.01, 0.02, 0.38, 0.04, 0.18, 0.09, 0.19, 0.08, 0.00, 0.01],
 [0.09, 0.15, 0.01, 0.04, 0.01, 0.01, 0.00, 0.01, 0.63, 0.07],
 [0.15, 0.01, 0.29, 0.03, 0.09, 0.03, 0.10, 0.07, 0.17, 0.05],
 [0.12, 0.05, 0.11, 0.04, 0.20, 0.05, 0.11, 0.17, 0.10, 0.06],
 [0.07, 0.02, 0.13, 0.20, 0.07, 0.14, 0.18, 0.07, 0.03, 0.09],
 [0.01, 0.04, 0.07, 0.12, 0.10, 0.21, 0.39, 0.06, 0.00, 0.01],
 [0.31, 0.02, 0.07, 0.16, 0.03, 0.11, 0.02, 0.05, 0.20, 0.02],
 [0.11, 0.00, 0.55, 0.01, 0.02, 0.09, 0.00, 0.21, 0.00, 0.00],
 [0.03, 0.02, 0.03, 0.34, 0.03, 0.01, 0.03, 0.26, 0.01, 0.25]]
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[10]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">result</span>: <span class="ansi-green-fg">FromTo</span>.<span class="ansi-green-fg">&lt;refinement&gt;</span>.this.type.<span class="ansi-green-fg">OutputData</span> = [[0.03, 0.05, 0.17, 0.13, 0.01, 0.13, 0.42, 0.00, 0.04, 0.00],
 [0.03, 0.17, 0.00, 0.05, 0.00, 0.01, 0.00, 0.00, 0.18, 0.55],
 [0.08, 0.09, 0.01, 0.03, 0.01, 0.00, 0.00, 0.01, 0.71, 0.05],
 [0.50, 0.04, 0.07, 0.04, 0.01, 0.02, 0.00, 0.03, 0.21, 0.08],
 [0.02, 0.03, 0.09, 0.05, 0.36, 0.10, 0.25, 0.08, 0.01, 0.00],
 [0.01, 0.07, 0.02, 0.62, 0.02, 0.04, 0.14, 0.05, 0.00, 0.03],
 [0.00, 0.00, 0.00, 0.69, 0.02, 0.05, 0.23, 0.00, 0.00, 0.00],
 [0.04, 0.01, 0.24, 0.04, 0.26, 0.10, 0.17, 0.11, 0.01, 0.01],
 [0.05, 0.17, 0.22, 0.09, 0.17, 0.16, 0.09, 0.03, 0.01, 0.01],
 [0.09, 0.44, 0.02, 0.03, 0.00, 0.06, 0.01, 0.01, 0.11, 0.24],
 [0.33, 0.09, 0.04, 0.09, 0.02, 0.07, 0.04, 0.02, 0.28, 0.03],
 [0.02, 0.49, 0.02, 0.05, 0.01, 0.01, 0.01, 0.03, 0.04, 0.33],
<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>11.编写工具方法，从一行INDArray中获得值最大的元素所在的列，目的是获得神经网络判断的结果，方便和原始标签比较以得出正确率。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="cm">/**</span>
<span class="cm">    * 从一行INDArray中获得值最大的元素所在的列</span>
<span class="cm">    * @param iNDArray</span>
<span class="cm">    * @return</span>
<span class="cm">    */</span>
  <span class="k">def</span> <span class="n">findMaxItemIndex</span><span class="o">(</span><span class="n">iNDArray</span><span class="k">:</span> <span class="kt">INDArray</span><span class="o">)</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">shape</span> <span class="k">=</span> <span class="n">iNDArray</span><span class="o">.</span><span class="n">shape</span><span class="o">()</span>
    <span class="k">val</span> <span class="n">col</span> <span class="k">=</span> <span class="n">shape</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
    <span class="k">var</span> <span class="n">maxValue</span> <span class="k">=</span> <span class="mf">0.0</span>
    <span class="k">var</span> <span class="n">maxIndex</span> <span class="k">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">index</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="n">col</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">itemValue</span> <span class="k">=</span> <span class="n">iNDArray</span><span class="o">.</span><span class="n">getDouble</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">index</span><span class="o">)</span>
      <span class="k">if</span> <span class="o">(</span><span class="n">itemValue</span> <span class="o">&gt;</span> <span class="n">maxValue</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">maxValue</span> <span class="k">=</span> <span class="n">itemValue</span>
        <span class="n">maxIndex</span> <span class="k">=</span> <span class="n">index</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="n">maxIndex</span>
  <span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[11]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">findMaxItemIndex</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>12.判断神经网络对测试数据分类判断的正确率，正确率应该在32%左右。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">var</span> <span class="n">right</span> <span class="k">=</span> <span class="mi">0</span>

  <span class="k">val</span> <span class="n">shape</span> <span class="k">=</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="o">()</span>
  <span class="k">for</span> <span class="o">(</span><span class="n">row</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="n">shape</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">rowItem</span> <span class="k">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getRow</span><span class="o">(</span><span class="n">row</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">index</span> <span class="k">=</span> <span class="n">findMaxItemIndex</span><span class="o">(</span><span class="n">rowItem</span><span class="o">)</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">index</span> <span class="o">==</span> <span class="n">test_expect_result</span><span class="o">.</span><span class="n">getDouble</span><span class="o">(</span><span class="n">row</span><span class="o">,</span> <span class="mi">0</span><span class="o">))</span> <span class="o">{</span>
      <span class="n">right</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="o">}</span>
  <span class="o">}</span>
  <span class="n">println</span><span class="o">(</span><span class="s">s&quot;the result is </span><span class="si">$right</span><span class="s"> %&quot;</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>the result is 32 %
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[12]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">right</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">32</span>
<span class="ansi-cyan-fg">shape</span>: <span class="ansi-green-fg">Array</span>[<span class="ansi-green-fg">Int</span>] = <span class="ansi-yellow-fg">Array</span>(<span class="ansi-green-fg">100</span>, <span class="ansi-green-fg">10</span>)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>13.<a href="https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/SoftmaxLinearClassifier.scala">完整代码</a></p>

</div>
</div>
</div>
 

