---
layout: demo
title: MiniBatchGradientDescent
download_path: demo_download/.
filename: MiniBatchGradientDescent.ipynb
---
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#32972;&#26223;">&#32972;&#26223;<a class="anchor-link" href="#&#32972;&#26223;">&#182;</a></h2><p>在大规模数据训练时，数据可以达到百万级量级。如果计算整个训练集，来获得仅仅一个参数的更新速度就太慢了。一个常用的方法是<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Mini-Batch Gradient Descent</a>，它计算训练集中的小批量（mini-batche）数据从而实现神经网络参数快速更新。</p>
<p>这节我们会先定义一个softmax分类器，然后使用<a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a>的训练集来训练这个神经网络。最后使用测试集来验证神经网络的准确率，不同的是我们将通过使用Mini-Batch Gradient Descent来实现网络参数快速更新，这样神经网络的准确率可以达到40%。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#36319;&#19978;&#19968;&#33410;&#30456;&#21516;&#65292;&#24341;&#20837;&#20381;&#36182;&#24182;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;">&#36319;&#19978;&#19968;&#33410;&#30456;&#21516;&#65292;&#24341;&#20837;&#20381;&#36182;&#24182;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;<a class="anchor-link" href="#&#36319;&#19978;&#19968;&#33410;&#30456;&#21516;&#65292;&#24341;&#20837;&#20381;&#36182;&#24182;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">import</span> <span class="nn">$plugin.$ivy.</span><span class="n">`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC7`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC7`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC7`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC7`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC7`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC7`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC7`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC7`</span>
<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.rauschig:jarchivelib:0.5.0`</span>

<span class="k">import</span> <span class="nn">$ivy.</span><span class="n">`org.plotly-scala::plotly-jupyter-scala:0.3.0`</span>

<span class="k">import</span> <span class="nn">java.io.</span><span class="o">{</span><span class="nc">FileInputStream</span><span class="o">,</span> <span class="nc">InputStream</span><span class="o">}</span>


<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableHList._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableDouble._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableINDArray._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableAny._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Layer.Batch</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Symbolic.Layers.Identity</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Symbolic._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.</span><span class="o">{</span>
  <span class="nc">DifferentiableHList</span><span class="o">,</span>
  <span class="nc">DifferentiableINDArray</span><span class="o">,</span>
  <span class="nc">Layer</span><span class="o">,</span>
  <span class="nc">Symbolic</span>
<span class="o">}</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathFunctions._</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathMethods.</span><span class="o">/</span>
<span class="k">import</span> <span class="nn">com.thoughtworks.deeplearning.Poly.MathOps</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.api.ndarray.INDArray</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.factory.Nd4j</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.indexing.</span><span class="o">{</span><span class="nc">INDArrayIndex</span><span class="o">,</span> <span class="nc">NDArrayIndex</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.nd4j.linalg.ops.transforms.Transforms</span>
<span class="k">import</span> <span class="nn">org.nd4s.Implicits._</span>
<span class="k">import</span> <span class="nn">shapeless._</span>

<span class="k">import</span> <span class="nn">plotly._</span>
<span class="k">import</span> <span class="nn">plotly.element._</span>
<span class="k">import</span> <span class="nn">plotly.layout._</span>
<span class="k">import</span> <span class="nn">plotly.JupyterScala._</span>

<span class="k">import</span> <span class="nn">scala.collection.immutable.IndexedSeq</span>
<span class="k">import</span> <span class="nn">scala.util.Random</span>

<span class="n">pprintConfig</span><span class="o">()</span> <span class="k">=</span> <span class="n">pprintConfig</span><span class="o">().</span><span class="n">copy</span><span class="o">(</span><span class="n">height</span> <span class="k">=</span> <span class="mi">2</span><span class="o">)</span>

<span class="k">import</span> <span class="nn">$file.ReadCIFAR10ToNDArray</span>
<span class="k">import</span> <span class="nn">$file.Utils</span>

<span class="k">def</span> <span class="n">softmax</span><span class="o">(</span><span class="k">implicit</span> <span class="n">scores</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">expScores</span> <span class="k">=</span> <span class="n">exp</span><span class="o">(</span><span class="n">scores</span><span class="o">)</span>
  <span class="n">expScores</span> <span class="o">/</span> <span class="n">expScores</span><span class="o">.</span><span class="n">sum</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">implicit</span> <span class="k">def</span> <span class="n">optimizer</span><span class="k">:</span> <span class="kt">Optimizer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">LearningRate</span> <span class="o">{</span>
  <span class="k">def</span> <span class="n">currentLearningRate</span><span class="o">()</span> <span class="k">=</span> <span class="mf">0.00001</span>
<span class="o">}</span>

<span class="c1">//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)</span>
<span class="k">val</span> <span class="nc">NumberOfClasses</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">def</span> <span class="n">createMyNeuralNetwork</span><span class="o">(</span><span class="k">implicit</span> <span class="n">input</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">initialValueOfWeight</span> <span class="k">=</span> <span class="nc">Nd4j</span><span class="o">.</span><span class="n">randn</span><span class="o">(</span><span class="nc">NumberOfPixels</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span> <span class="o">*</span> <span class="mf">0.001</span>
  <span class="k">val</span> <span class="n">weight</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="n">initialValueOfWeight</span><span class="o">.</span><span class="n">toWeight</span>
  <span class="k">val</span> <span class="n">result</span><span class="k">:</span> <span class="kt">INDArray</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="n">input</span> <span class="n">dot</span> <span class="n">weight</span>
  <span class="n">softmax</span><span class="o">.</span><span class="n">compose</span><span class="o">(</span><span class="n">result</span><span class="o">)</span>
<span class="o">}</span>
<span class="k">val</span> <span class="n">myNeuralNetwork</span> <span class="k">=</span> <span class="n">createMyNeuralNetwork</span>

<span class="k">def</span> <span class="n">lossFunction</span><span class="o">(</span><span class="k">implicit</span> <span class="n">pair</span><span class="k">:</span> <span class="o">(</span><span class="kt">INDArray</span> <span class="kt">::</span> <span class="kt">INDArray</span> <span class="kt">::</span> <span class="kt">HNil</span><span class="o">)</span> <span class="kt">@Symbolic</span><span class="o">)</span><span class="k">:</span> <span class="kt">Double</span> <span class="kt">@Symbolic</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">input</span> <span class="k">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">head</span>
  <span class="k">val</span> <span class="n">expectedOutput</span> <span class="k">=</span> <span class="n">pair</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>
  <span class="k">val</span> <span class="n">probabilities</span> <span class="k">=</span> <span class="n">myNeuralNetwork</span><span class="o">.</span><span class="n">compose</span><span class="o">(</span><span class="n">input</span><span class="o">)</span>

  <span class="o">-(</span><span class="n">expectedOutput</span> <span class="o">*</span> <span class="n">log</span><span class="o">(</span><span class="n">probabilities</span><span class="o">)).</span><span class="n">mean</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stderr output_text">
<pre>SLF4J: Failed to load class &#34;org.slf4j.impl.StaticLoggerBinder&#34;.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[1]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$plugin.$                                                                             

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                           
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                               
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                           
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                              
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                             
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                             
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                                 
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                                                
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                               

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$ivy.$                                             

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">java.io.{FileInputStream, InputStream}


</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableHList._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableDouble._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableINDArray._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableAny._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Layer.Batch
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Symbolic.Layers.Identity
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Symbolic._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.{
  DifferentiableHList,
  DifferentiableINDArray,
  Layer,
  Symbolic
}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathFunctions._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathMethods./
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">com.thoughtworks.deeplearning.Poly.MathOps
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.api.ndarray.INDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.cpu.nativecpu.NDArray
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.factory.Nd4j
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4j.linalg.ops.transforms.Transforms
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">org.nd4s.Implicits._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">shapeless._

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.element._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.layout._
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">plotly.JupyterScala._

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.collection.immutable.IndexedSeq
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">scala.util.Random

</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$file.$                   
</span>
<span class="ansi-green-fg">import </span><span class="ansi-cyan-fg">$file.$    

</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">softmax</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">optimizer</span>
<span class="ansi-cyan-fg">NumberOfClasses</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">10</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">createMyNeuralNetwork</span>
<span class="ansi-cyan-fg">myNeuralNetwork</span>: (<span class="ansi-green-fg">com</span>.<span class="ansi-green-fg">thoughtworks</span>.<span class="ansi-green-fg">deeplearning</span>.<span class="ansi-green-fg">Symbolic</span>.<span class="ansi-green-fg">To</span>[<span class="ansi-green-fg">INDArray</span>]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#<span class="ansi-green-fg">@</span> = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[-0.00, -0.00, 0.00, -0.00, 0.00, -0.00, -0.00<span class="ansi-yellow-fg">...</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">lossFunction</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#36816;&#29992;Mini-Batch-Gradient-Descent">&#36816;&#29992;Mini-Batch Gradient Descent<a class="anchor-link" href="#&#36816;&#29992;Mini-Batch-Gradient-Descent">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>类似前一节，我们需要训练神经网络，但是跟上一节不同的是，这次我们的训练数据是随机读取的，上一节是反复训练同一批数据集。训练神经网络并观察每次训练<code>loss</code>的变化，<code>loss</code>的变化趋势是降低，但是不是每次都降低(前途是光明的，道路是曲折的)。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="&#26681;&#25454;&#25968;&#32452;&#35835;&#21462;&#21644;&#22788;&#29702;&#25968;&#25454;&#65292;&#28982;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;">&#26681;&#25454;&#25968;&#32452;&#35835;&#21462;&#21644;&#22788;&#29702;&#25968;&#25454;&#65292;&#28982;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;<a class="anchor-link" href="#&#26681;&#25454;&#25968;&#32452;&#35835;&#21462;&#21644;&#22788;&#29702;&#25968;&#25454;&#65292;&#28982;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="nc">MiniBatchSize</span> <span class="k">=</span> <span class="mi">256</span>
<span class="k">val</span> <span class="nc">NumberOfPixels</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">3072</span>

<span class="k">def</span> <span class="n">trainData</span><span class="o">(</span><span class="n">randomIndexArray</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Int</span><span class="o">])</span><span class="k">:</span> <span class="kt">Double</span> <span class="o">=</span> <span class="o">{</span>
  <span class="k">val</span> <span class="n">trainNDArray</span> <span class="o">::</span> <span class="n">expectLabel</span> <span class="o">::</span> <span class="n">shapeless</span><span class="o">.</span><span class="nc">HNil</span> <span class="k">=</span>
    <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="n">getSGDTrainNDArray</span><span class="o">(</span><span class="n">randomIndexArray</span><span class="o">)</span>

  <span class="k">val</span> <span class="n">input</span> <span class="k">=</span>
    <span class="n">trainNDArray</span><span class="o">.</span><span class="n">reshape</span><span class="o">(</span><span class="nc">MiniBatchSize</span><span class="o">,</span> <span class="nc">NumberOfPixels</span><span class="o">)</span>

  <span class="k">val</span> <span class="n">expectLabelVectorized</span> <span class="k">=</span>
    <span class="nc">Utils</span><span class="o">.</span><span class="n">makeVectorized</span><span class="o">(</span><span class="n">expectLabel</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span>

  <span class="n">lossFunction</span><span class="o">.</span><span class="n">train</span><span class="o">(</span><span class="n">input</span> <span class="o">::</span> <span class="n">expectLabelVectorized</span> <span class="o">::</span> <span class="nc">HNil</span><span class="o">)</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[2]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">MiniBatchSize</span>: <span class="ansi-green-fg">Int</span> = <span class="ansi-green-fg">256</span>
defined <span class="ansi-green-fg">function</span> <span class="ansi-cyan-fg">trainData</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="&#27599;&#20010;epoch&#25171;&#20081;&#19968;&#27425;&#24207;&#21015;,&#29983;&#25104;&#38543;&#26426;&#25968;&#32452;">&#27599;&#20010;<a href="http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks">epoch</a>&#25171;&#20081;&#19968;&#27425;&#24207;&#21015;,&#29983;&#25104;&#38543;&#26426;&#25968;&#32452;<a class="anchor-link" href="#&#27599;&#20010;epoch&#25171;&#20081;&#19968;&#27425;&#24207;&#21015;,&#29983;&#25104;&#38543;&#26426;&#25968;&#32452;">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">random</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Random</span>

<span class="k">val</span> <span class="n">lossSeq</span> <span class="k">=</span>
  <span class="o">(</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">iteration</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">to</span> <span class="mi">50</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">randomIndex</span> <span class="k">=</span> <span class="n">random</span>
        <span class="o">.</span><span class="n">shuffle</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">IndexedSeq</span><span class="o">](</span><span class="mi">0</span> <span class="n">until</span> <span class="mi">10000</span><span class="o">)</span> <span class="c1">//https://issues.scala-lang.org/browse/SI-6948</span>
        <span class="o">.</span><span class="n">toArray</span>
      <span class="k">for</span> <span class="o">(</span><span class="n">times</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">until</span> <span class="mi">10000</span> <span class="o">/</span> <span class="nc">MiniBatchSize</span><span class="o">)</span> <span class="k">yield</span> <span class="o">{</span>
        <span class="k">val</span> <span class="n">randomIndexArray</span> <span class="k">=</span>
          <span class="n">randomIndex</span><span class="o">.</span><span class="n">slice</span><span class="o">(</span><span class="n">times</span> <span class="o">*</span> <span class="nc">MiniBatchSize</span><span class="o">,</span>
                            <span class="o">(</span><span class="n">times</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="o">*</span> <span class="nc">MiniBatchSize</span><span class="o">)</span>
          <span class="k">val</span> <span class="n">loss</span> <span class="k">=</span> <span class="n">trainData</span><span class="o">(</span><span class="n">randomIndexArray</span><span class="o">)</span>
          <span class="k">if</span><span class="o">(</span><span class="n">times</span> <span class="o">==</span> <span class="mi">3</span> <span class="o">&amp;</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">4</span><span class="o">){</span>
            <span class="n">println</span><span class="o">(</span><span class="s">&quot;at epoch &quot;</span> <span class="o">+</span> <span class="o">(</span><span class="n">iteration</span> <span class="o">/</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">1</span><span class="o">)</span> <span class="o">+</span> <span class="s">&quot; loss is :&quot;</span> <span class="o">+</span> <span class="n">loss</span><span class="o">)</span>
          <span class="o">}</span>
          <span class="n">loss</span>
      <span class="o">}</span>
    <span class="o">}</span>
  <span class="o">).</span><span class="n">flatten</span>

<span class="k">val</span> <span class="n">plot</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
  <span class="nc">Scatter</span><span class="o">(</span><span class="n">lossSeq</span><span class="o">.</span><span class="n">indices</span><span class="o">,</span> <span class="n">lossSeq</span><span class="o">)</span>
<span class="o">)</span>

<span class="n">plot</span><span class="o">.</span><span class="n">plot</span><span class="o">(</span>
  <span class="n">title</span> <span class="k">=</span> <span class="s">&quot;loss by time&quot;</span>
<span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>at epoch 1 loss is :0.21049327850341798
at epoch 2 loss is :0.20709555149078368
at epoch 3 loss is :0.1957709789276123
at epoch 4 loss is :0.1974302649497986
at epoch 5 loss is :0.19418622255325318
at epoch 6 loss is :0.19290707111358643
at epoch 7 loss is :0.19335532188415527
at epoch 8 loss is :0.18547534942626953
at epoch 9 loss is :0.18905540704727172
at epoch 10 loss is :0.1805393099784851
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>


<div class="output_html rendered_html output_subarea ">
<div class="chart" id="plot-1041885403"></div>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>




<div id="35ba8428-5376-4f8f-ba05-4b7d033eea20"></div>
<div class="output_subarea output_javascript ">
<script type="text/javascript">
var element = $('#35ba8428-5376-4f8f-ba05-4b7d033eea20');
requirejs(["plotly"], function(Plotly) {
  (function () {
  var data0 = {"type":"scatter","x":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0],"y":[0.229979944229126,0.2298872470855713,0.2301811933517456,0.22962307929992676,0.22876534461975098,0.22858400344848634,0.2291093349456787,0.2291335105895996,0.2290665626525879,0.2291789770126343,0.22875142097473145,0.22671036720275878,0.22817618846893312,0.22783207893371582,0.22527141571044923,0.2286287307739258,0.22641401290893554,0.22631077766418456,0.22573411464691162,0.2266371726989746,0.22658281326293944,0.2269871711730957,0.22616610527038575,0.2247004270553589,0.22661888599395752,0.22511911392211914,0.2251232624053955,0.22440495491027831,0.22511396408081055,0.22334685325622558,0.22383575439453124,0.22551064491271972,0.22267942428588866,0.2247917413711548,0.2236358642578125,0.22418746948242188,0.22360501289367676,0.22306442260742188,0.22250151634216309,0.2236351490020752,0.22437593936920167,0.22263383865356445,0.2239358901977539,0.22287375926971437,0.22377474308013917,0.2221384048461914,0.22232489585876464,0.22261033058166504,0.22237296104431153,0.22238612174987793,0.22195353507995605,0.22321171760559083,0.22156045436859131,0.22196345329284667,0.22123920917510986,0.2204425096511841,0.22117938995361328,0.2187314510345459,0.22018895149230958,0.21984212398529052,0.2200425386428833,0.2204051971435547,0.22010035514831544,0.2166419506072998,0.21691060066223145,0.2199603796005249,0.2188166618347168,0.21877951622009278,0.22124700546264647,0.21973824501037598,0.2185138463973999,0.21849727630615234,0.22025892734527588,0.2198551654815674,0.21859724521636964,0.2184004545211792,0.21710538864135742,0.2190922737121582,0.2179635524749756,0.21283400058746338,0.21953535079956055,0.21984283924102782,0.21718363761901854,0.21851558685302735,0.21942152976989746,0.21774146556854249,0.21621768474578856,0.21589555740356445,0.21681504249572753,0.21700356006622315,0.21897473335266113,0.21680214405059814,0.21766927242279052,0.21770906448364258,0.21659698486328124,0.21605966091156006,0.213960599899292,0.21588482856750488,0.21510026454925538,0.21733441352844238,0.2153858184814453,0.2155980110168457,0.21649370193481446,0.21533727645874023,0.2146090030670166,0.21488723754882813,0.21500520706176757,0.21347684860229493,0.21517844200134278,0.212046480178833,0.21496081352233887,0.21517467498779297,0.21442112922668458,0.21563496589660644,0.215248703956604,0.21496713161468506,0.21389083862304686,0.21455442905426025,0.21580057144165038,0.21548285484313964,0.2115936756134033,0.21505229473114013,0.2161381721496582,0.21276748180389404,0.21232903003692627,0.2134547710418701,0.2155362606048584,0.21234617233276368,0.2141054630279541,0.21424520015716553,0.21467480659484864,0.21299989223480226,0.21405529975891113,0.21194167137145997,0.21425385475158693,0.21403536796569825,0.21429777145385742,0.21253304481506347,0.21453514099121093,0.212158203125,0.21104607582092286,0.2134420394897461,0.21298642158508302,0.2107442855834961,0.21258058547973632,0.21615300178527833,0.21555933952331544,0.21315364837646483,0.2145179271697998,0.20900411605834962,0.21131386756896972,0.21393089294433593,0.2081592082977295,0.21412277221679688,0.21188297271728515,0.21395225524902345,0.20899381637573242,0.21084060668945312,0.21003961563110352,0.21049327850341798,0.21671602725982667,0.2065195083618164,0.20823361873626708,0.20627162456512452,0.20921871662139893,0.20845959186553956,0.20981171131134033,0.21172399520874025,0.21269350051879882,0.2087714195251465,0.20795760154724122,0.20859229564666748,0.20811061859130858,0.2113356113433838,0.20964412689208983,0.2118248462677002,0.20795130729675293,0.2084449052810669,0.21009092330932616,0.21298775672912598,0.2094493865966797,0.21197566986083985,0.20724940299987793,0.20685977935791017,0.2091081142425537,0.20889110565185548,0.20663189888000488,0.2139336347579956,0.2124102830886841,0.21282947063446045,0.20763201713562013,0.21301705837249757,0.210601806640625,0.21095190048217774,0.20819902420043945,0.20705785751342773,0.20768065452575685,0.2062162399291992,0.2102682113647461,0.2070824146270752,0.2072171688079834,0.20742361545562743,0.2081451416015625,0.21105444431304932,0.20880441665649413,0.20876164436340333,0.20958845615386962,0.20436592102050782,0.2068354606628418,0.20597620010375978,0.20711889266967773,0.21134343147277831,0.20990514755249023,0.20876896381378174,0.20834407806396485,0.20595188140869142,0.21294176578521729,0.20750730037689208,0.20794196128845216,0.2102628231048584,0.21008355617523194,0.20203311443328859,0.20849127769470216,0.20910840034484862,0.20868351459503173,0.20861988067626952,0.20792717933654786,0.2087658405303955,0.2072920322418213,0.20743927955627442,0.20563766956329346,0.21222105026245117,0.20620255470275878,0.2078315258026123,0.20806784629821778,0.20737624168395996,0.20618691444396972,0.20761680603027344,0.20604233741760253,0.20495288372039794,0.20545201301574706,0.2078453779220581,0.20559196472167968,0.2033411979675293,0.20564765930175782,0.20308361053466797,0.2031315803527832,0.2102527141571045,0.20616397857666016,0.2014554500579834,0.20170247554779053,0.2036041498184204,0.2065797805786133,0.20349540710449218,0.2056410789489746,0.20563035011291503,0.20854859352111815,0.20803227424621581,0.20550999641418458,0.20900850296020507,0.2061877965927124,0.20544469356536865,0.20359411239624023,0.20564067363739014,0.1995159149169922,0.21212394237518312,0.20272517204284668,0.20360872745513917,0.20598146915435792,0.2043389081954956,0.20566864013671876,0.20452561378479003,0.20210504531860352,0.2057875871658325,0.2045530319213867,0.20601954460144042,0.20805094242095948,0.20095646381378174,0.20694527626037598,0.19980292320251464,0.2036672353744507,0.20741603374481202,0.20823776721954346,0.20938234329223632,0.20238971710205078,0.20565719604492189,0.20342116355895995,0.20531091690063477,0.2025895595550537,0.2025092601776123,0.19979182481765748,0.20789012908935547,0.21002182960510254,0.2004624843597412,0.20186896324157716,0.20956921577453613,0.2047029972076416,0.20887889862060546,0.2076502799987793,0.2056746006011963,0.2032395124435425,0.2076502799987793,0.201688551902771,0.20442600250244142,0.19953888654708862,0.20651907920837403,0.20055270195007324,0.2031341552734375,0.2046356201171875,0.20442452430725097,0.2030400276184082,0.20158112049102783,0.20322179794311523,0.20104854106903075,0.20119149684906007,0.20075781345367433,0.2039947986602783,0.2074800968170166,0.20409550666809081,0.20308642387390136,0.20191686153411864,0.2059248924255371,0.20859243869781494,0.19970741271972656,0.20431718826293946,0.20498759746551515,0.20401945114135742,0.2097996711730957,0.20689895153045654,0.20596609115600586,0.2045203447341919,0.20816705226898194,0.20160038471221925,0.20450329780578613,0.20701942443847657,0.20474567413330078,0.2013022184371948,0.20180883407592773,0.19687820672988893,0.1985069751739502,0.2060450553894043,0.20342390537261962,0.19756672382354737,0.20500035285949708,0.19638960361480712,0.20367894172668458,0.2022247314453125,0.19686628580093385,0.200848388671875,0.20671381950378417,0.20379738807678222,0.20382263660430908,0.20544099807739258,0.20312278270721434,0.20709555149078368,0.1979124665260315,0.20123753547668458,0.20356526374816894,0.2047487497329712,0.2002551794052124,0.20461716651916503,0.20246472358703613,0.20370104312896728,0.20177383422851564,0.20295798778533936,0.2009854793548584,0.20216937065124513,0.19720282554626464,0.2002439022064209,0.2014139175415039,0.20032033920288086,0.20159268379211426,0.20341861248016357,0.20205371379852294,0.20235795974731446,0.20209097862243652,0.20576894283294678,0.20018770694732665,0.19694297313690184,0.19798762798309327,0.20518386363983154,0.19999947547912597,0.1956695556640625,0.2065866470336914,0.1979374408721924,0.20252866744995118,0.20311639308929444,0.2027843952178955,0.19591639041900635,0.20060195922851562,0.1990990400314331,0.19745291471481324,0.20098493099212647,0.20066466331481933,0.20446617603302003,0.19625881910324097,0.20148611068725586,0.19892330169677735,0.20226900577545165,0.2034686326980591,0.19769513607025146,0.20486700534820557,0.2029890537261963,0.20602402687072754,0.1995102047920227,0.20900659561157225,0.20364947319030763,0.1987454414367676,0.20036811828613282,0.19698951244354249,0.2009552001953125,0.19937503337860107,0.1997736692428589,0.20563030242919922,0.2016007900238037,0.197953200340271,0.1998818635940552,0.20119903087615967,0.1901148796081543,0.19908772706985473,0.19477813243865966,0.19511631727218628,0.20235040187835693,0.20205867290496826,0.19490716457366944,0.19620881080627442,0.20347495079040528,0.1995214343070984,0.19293558597564697,0.20007040500640869,0.20546457767486573,0.2030869722366333,0.20545084476470948,0.1968387246131897,0.19878768920898438,0.19508317708969117,0.1964269161224365,0.1963890552520752,0.19864176511764525,0.19835108518600464,0.19711906909942628,0.19597187042236328,0.2014225482940674,0.20055780410766602,0.20095152854919435,0.1971835732460022,0.19710705280303956,0.1957440972328186,0.20284607410430908,0.20214664936065674,0.19529755115509034,0.19508594274520874,0.19443398714065552,0.200577449798584,0.19996252059936523,0.1970336079597473,0.19701300859451293,0.20558459758758546,0.20426852703094484,0.20269854068756105,0.20329771041870118,0.19913502931594848,0.19813911914825438,0.19990012645721436,0.2028499126434326,0.20195305347442627,0.19910223484039308,0.19904597997665405,0.2049264669418335,0.19950828552246094,0.1950472354888916,0.19811969995498657,0.19453752040863037,0.19697771072387696,0.19716812372207643,0.20135347843170165,0.1966090202331543,0.19672439098358155,0.19332715272903442,0.19935938119888305,0.1982179641723633,0.19912643432617189,0.20233044624328614,0.19584180116653443,0.19414019584655762,0.20070462226867675,0.19655996561050415,0.1988086700439453,0.19694747924804687,0.19675449132919312,0.20427992343902587,0.20497984886169435,0.1949916362762451,0.19450037479400634,0.2001025438308716,0.1976919174194336,0.19742119312286377,0.19986881017684938,0.1972268581390381,0.19528173208236693,0.19922120571136476,0.19673285484313965,0.1929926872253418,0.19724973440170288,0.19901089668273925,0.19810550212860106,0.19658880233764647,0.19611388444900513,0.19979835748672486,0.196083402633667,0.20245506763458251,0.20472097396850586,0.1967474937438965,0.19710080623626708,0.19587254524230957,0.19639278650283815,0.19874539375305175,0.19727866649627684,0.19488449096679689,0.1988261103630066,0.1969344735145569,0.20146422386169432,0.19705708026885987,0.2027324914932251,0.20351021289825438,0.19704194068908693,0.20618777275085448,0.19393646717071533,0.19671108722686767,0.19767229557037352,0.19608856439590455,0.19558236598968506,0.20175037384033204,0.2021388292312622,0.20413284301757811,0.1885623574256897,0.1970511794090271,0.20055241584777833,0.19694849252700805,0.203429651260376,0.2023153781890869,0.20097088813781738,0.1963167667388916,0.19849448204040526,0.19853919744491577,0.19508661031723024,0.1960650086402893,0.19752705097198486,0.1950312614440918,0.1957709789276123,0.19613879919052124,0.19826116561889648,0.1965312123298645,0.19737138748168945,0.19417136907577515,0.19348887205123902,0.19603493213653564,0.19518810510635376,0.20440101623535156,0.2006753444671631,0.20095448493957518,0.18993844985961914,0.20129799842834473,0.1989579677581787,0.20208320617675782,0.19478135108947753,0.20030834674835205,0.2022617816925049,0.19728068113327027,0.1933994174003601,0.1989520311355591,0.19504911899566652,0.19741809368133545,0.1957857131958008,0.19863213300704957,0.19305721521377564,0.20334944725036622,0.19655734300613403,0.19935226440429688,0.1911347508430481,0.19692896604537963,0.19077081680297853,0.19483870267868042,0.19693002700805665,0.2014143466949463,0.19548617601394652,0.2067101001739502,0.19940688610076904,0.197823429107666,0.1911163091659546,0.19810118675231933,0.19143620729446412,0.2011345863342285,0.19364490509033203,0.19517996311187744,0.19890421628952026,0.19552745819091796,0.1984001636505127,0.19601836204528808,0.19455742835998535,0.20173492431640624,0.20254158973693848,0.20032134056091308,0.1984656810760498,0.19596196413040162,0.1986401081085205,0.19582451581954957,0.19482073783874512,0.19595053195953369,0.18815288543701172,0.19694099426269532,0.19263720512390137,0.20027828216552734,0.19993956089019777,0.1969839096069336,0.19806234836578368,0.19415384531021118,0.19543577432632447,0.19975234270095826,0.19920668601989747,0.19509676694869996,0.20321192741394042,0.195670747756958,0.18851035833358765,0.19914506673812865,0.19650779962539672,0.19756146669387817,0.1997016191482544,0.19869987964630126,0.19495593309402465,0.19562488794326782,0.19470341205596925,0.19116445779800414,0.20080490112304689,0.19274283647537233,0.19771307706832886,0.19754552841186523,0.19322750568389893,0.20118417739868164,0.19007278680801393,0.20252456665039062,0.20153675079345704,0.1896212577819824,0.1907447099685669,0.1909192681312561,0.19523270130157472,0.19393885135650635,0.1919994831085205,0.19459335803985595,0.1980479121208191,0.1950678586959839,0.19196684360504152,0.19892786741256713,0.1989871859550476,0.19900813102722167,0.1972464680671692,0.19120854139328003,0.19291963577270507,0.19578462839126587,0.19689834117889404,0.19804712533950805,0.1974094867706299,0.19005693197250367,0.19070942401885987,0.19634387493133545,0.19766989946365357,0.19263265132904053,0.1975954294204712,0.1945314884185791,0.1955769419670105,0.19639619588851928,0.19309515953063966,0.19739683866500854,0.19706180095672607,0.19568400382995604,0.19333142042160034,0.19367010593414308,0.19439655542373657,0.1959328055381775,0.19734246730804444,0.20187180042266845,0.1985137104988098,0.19999499320983888,0.19292356967926025,0.1959326148033142,0.19552278518676758,0.19792643785476685,0.19150400161743164,0.19469144344329833,0.19168812036514282,0.19631656408309936,0.19631749391555786,0.1963569164276123,0.19313830137252808,0.19354329109191895,0.19261529445648193,0.1941794991493225,0.19701744318008424,0.1969621181488037,0.20065135955810548,0.19975883960723878,0.19209365844726561,0.195302414894104,0.19160035848617554,0.19842817783355712,0.19038846492767333,0.1935969114303589,0.19473423957824706,0.19843060970306398,0.19418452978134154,0.19299089908599854,0.19693946838378906,0.1908273458480835,0.20094032287597657,0.2008451461791992,0.19382078647613527,0.1985912322998047,0.19958240985870362,0.19641064405441283,0.20251708030700682,0.19702563285827637,0.19463486671447755,0.19252543449401854,0.1917806386947632,0.19375965595245362,0.19579293727874755,0.19823808670043946,0.19118280410766603,0.1888988971710205,0.19644198417663575,0.19636090993881225,0.1949856996536255,0.1980951428413391,0.1893179178237915,0.19828826189041138,0.1887400984764099,0.20175299644470215,0.18612947463989257,0.19306825399398803,0.1914364814758301,0.19786486625671387,0.19413223266601562,0.1972214937210083,0.18893721103668212,0.1974302649497986,0.1969731092453003,0.19352281093597412,0.19990952014923097,0.19281535148620604,0.19450464248657226,0.19580456018447875,0.19996170997619628,0.20048985481262208,0.19097659587860108,0.18931291103363038,0.19300899505615235,0.1959538221359253,0.1917346239089966,0.1875687599182129,0.19519898891448975,0.19628419876098632,0.189388906955719,0.1925349473953247,0.18813422918319703,0.19731359481811522,0.19619696140289306,0.19291136264801026,0.19649341106414794,0.19443801641464234,0.1938559293746948,0.19516239166259766,0.19185622930526733,0.1939464330673218,0.19009842872619628,0.19285378456115723,0.19649717807769776,0.19712419509887696,0.19871761798858642,0.19135167598724365,0.19766174554824828,0.19791719913482667,0.1936103105545044,0.18691544532775878,0.19443522691726683,0.19376121759414672,0.19142441749572753,0.1902433753013611,0.1895064353942871,0.19508593082427977,0.2005160093307495,0.1918581485748291,0.19521687030792237,0.19819488525390624,0.18835268020629883,0.19011082649230956,0.197281551361084,0.19755929708480835,0.18367453813552856,0.19184612035751342,0.19645594358444213,0.18429911136627197,0.20569231510162353,0.19379080533981324,0.1992751717567444,0.1900471568107605,0.19264686107635498,0.192313814163208,0.193034565448761,0.20253725051879884,0.192567777633667,0.19073694944381714,0.19353830814361572,0.18669785261154176,0.19281740188598634,0.19926916360855101,0.1877529263496399,0.18787541389465331,0.1938507080078125,0.19466865062713623,0.18500914573669433,0.19872405529022216,0.19220099449157715,0.18875380754470825,0.1946113348007202,0.19616971015930176,0.19811418056488037,0.19679727554321289,0.19534456729888916,0.1976770877838135,0.1894003629684448,0.1839156150817871,0.1960347056388855,0.19444491863250732,0.19541341066360474,0.1939324140548706,0.1878788709640503,0.19116150140762328,0.18985252380371093,0.18767359256744384,0.1953221321105957,0.18874396085739137,0.1925682544708252,0.1983506441116333,0.19038794040679932,0.19701257944107056,0.19270049333572387,0.19078000783920288,0.19203267097473145,0.19884066581726073,0.19156336784362793,0.18715617656707764,0.1987043499946594,0.19267635345458983,0.18803321123123168,0.19349417686462403,0.19086847305297852,0.19091975688934326,0.18696563243865966,0.19825972318649293,0.18787652254104614,0.1935955762863159,0.1897125482559204,0.19412070512771606,0.19676764011383058,0.18784010410308838,0.19836251735687255,0.1900552272796631,0.19113590717315673,0.1927962303161621,0.2013559103012085,0.18926185369491577,0.19695830345153809,0.19140353202819824,0.195166015625,0.19874265193939208,0.19103784561157228,0.19347054958343507,0.19328277111053466,0.19202488660812378,0.18649345636367798,0.18778185844421386,0.19134700298309326,0.19706904888153076,0.19533066749572753,0.19203522205352783,0.19417663812637329,0.18624403476715087,0.19087660312652588,0.19174410104751588,0.19359151124954224,0.1854190468788147,0.1896250605583191,0.19388419389724731,0.19153902530670167,0.19251635074615478,0.1909087896347046,0.1952688455581665,0.18729848861694337,0.19306671619415283,0.19260098934173583,0.185209321975708,0.20534348487854004,0.1936187982559204,0.1953922390937805,0.1896649956703186,0.19657231569290162,0.19503989219665527,0.188435161113739,0.19123828411102295,0.18679540157318114,0.19394230842590332,0.18975089788436889,0.19638781547546386,0.19454745054244996,0.19258673191070558,0.19329771995544434,0.19507064819335937,0.1966537594795227,0.1923254609107971,0.18889025449752808,0.18726863861083984,0.19083287715911865,0.1998935341835022,0.1939730763435364,0.1934824228286743,0.19298092126846314,0.19276440143585205,0.19607584476470946,0.1912153482437134,0.19425760507583617,0.19803366661071778,0.19260075092315673,0.19180209636688234,0.19709250926971436,0.19744298458099366,0.19340595006942748,0.1933734893798828,0.1941157102584839,0.19415946006774903,0.19418622255325318,0.19623035192489624,0.18947131633758546,0.19216114282608032,0.19470237493515014,0.19633759260177613,0.199260413646698,0.19402575492858887,0.19503308534622193,0.1942009449005127,0.18755615949630738,0.19649677276611327,0.19267511367797852,0.19257543087005616,0.19367901086807252,0.1941903591156006,0.1858455777168274,0.18646771907806398,0.18645172119140624,0.19189977645874023,0.19349731206893922,0.19141281843185426,0.1891929864883423,0.1906261920928955,0.18784056901931762,0.1967989206314087,0.18922624588012696,0.19811125993728637,0.19516451358795167,0.19484890699386598,0.1904323697090149,0.1849175810813904,0.19584195613861083,0.2008394718170166,0.18525713682174683,0.19257465600967408,0.19187500476837158,0.18922709226608275,0.19280880689620972,0.19641177654266356,0.19300305843353271,0.19261409044265748,0.1932101607322693,0.19299681186676027,0.19456112384796143,0.1945044994354248,0.1880864381790161,0.19396369457244872,0.19106156826019288,0.18752501010894776,0.18538763523101806,0.19448356628417968,0.19537776708602905,0.1892552375793457,0.1905372381210327,0.19682101011276246,0.18850128650665282,0.1962870717048645,0.186257803440094,0.1951999545097351,0.1920241594314575,0.19219608306884767,0.1916319489479065,0.1949934482574463,0.19387720823287963,0.1902077913284302,0.18572965860366822,0.19456838369369506,0.18949652910232545,0.1933433771133423,0.19645909070968628,0.19192622900009154,0.19034063816070557,0.19032752513885498,0.1918928623199463,0.1950024724006653,0.1900830626487732,0.19657163619995116,0.18757667541503906,0.18789381980895997,0.18989275693893432,0.19350613355636598,0.1982025146484375,0.19556446075439454,0.18501746654510498,0.18588221073150635,0.19171313047409058,0.19439936876296998,0.18097035884857177,0.18267085552215576,0.19537633657455444,0.19380289316177368,0.1887808322906494,0.19102354049682618,0.18569207191467285,0.1878350257873535,0.1879357099533081,0.1901085376739502,0.19430942535400392,0.19570012092590333,0.1873132109642029,0.19380303621292114,0.19016728401184083,0.19462695121765136,0.18566601276397704,0.19186742305755616,0.18468127250671387,0.19662903547286986,0.19490946531295777,0.1933063268661499,0.19594507217407225,0.18946305513381959,0.19647985696792603,0.19555431604385376,0.1822703719139099,0.19461843967437745,0.18676722049713135,0.19667993783950805,0.19428378343582153,0.18556654453277588,0.1881054162979126,0.18604748249053954,0.19111852645874022,0.18839478492736816,0.19063200950622558,0.18431581258773805,0.1869129419326782,0.19188190698623658,0.1911540746688843,0.19035518169403076,0.19183975458145142,0.18775769472122192,0.1894744634628296,0.195923376083374,0.19026618003845214,0.19124560356140136,0.18880267143249513,0.18549399375915526,0.19166882038116456,0.1922250747680664,0.1863524079322815,0.18948802947998047,0.187613582611084,0.18535153865814208,0.19988663196563722,0.19109694957733153,0.18920670747756957,0.19042835235595704,0.19262319803237915,0.19506292343139647,0.18855714797973633,0.19376862049102783,0.19762455224990844,0.19017837047576905,0.19211505651473998,0.19626859426498414,0.1869548201560974,0.1900955319404602,0.18016493320465088,0.1883096218109131,0.19938628673553466,0.19162689447402953,0.18822416067123413,0.19327543973922728,0.1919896721839905,0.19582748413085938,0.18640881776809692,0.1890074133872986,0.19189016819000243,0.19715856313705443,0.18119020462036134,0.1951765775680542,0.18834584951400757,0.19373526573181152,0.18551558256149292,0.1945028305053711,0.1927290678024292,0.1858583688735962,0.18992829322814941,0.19911178350448608,0.18951929807662965,0.18783037662506102,0.18856784105300903,0.19056427478790283,0.18885128498077391,0.19543852806091308,0.1939213514328003,0.18674147129058838,0.18997008800506593,0.18177002668380737,0.18712867498397828,0.1844249963760376,0.18600742816925048,0.1898319959640503,0.19465284347534179,0.19290707111358643,0.18802783489227295,0.1823352575302124,0.1889258623123169,0.18807395696640014,0.1853972315788269,0.18511743545532228,0.19642200469970703,0.19467706680297853,0.1879386782646179,0.1848247766494751,0.18306362628936768,0.18912190198898315,0.19459747076034545,0.19522745609283448,0.18864786624908447,0.19711730480194092,0.19669299125671386,0.18658673763275146,0.18605201244354247,0.1883922576904297,0.19201257228851318,0.18977034091949463,0.18831181526184082,0.18995760679244994,0.18558281660079956,0.19724268913269044,0.19083187580108643,0.18900939226150512,0.1898067593574524,0.1841257929801941,0.1901320219039917,0.19002933502197267,0.1908186912536621,0.1953806161880493,0.1960922360420227,0.1891263723373413,0.18934584856033326,0.19338538646697997,0.19313108921051025,0.19392542839050292,0.18691089153289794,0.18618974685668946,0.18546617031097412,0.19385114908218384,0.19046782255172728,0.19509880542755126,0.18334693908691407,0.18974220752716064,0.18681814670562744,0.19049317836761476,0.19873428344726562,0.1887181043624878,0.1936764121055603,0.18793516159057616,0.1896732807159424,0.19215927124023438,0.19235469102859498,0.19205446243286134,0.18360490798950196,0.19655947685241698,0.18722739219665527,0.1810332417488098,0.19093611240386962,0.19541618824005128,0.18924243450164796,0.19377604722976685,0.18869467973709106,0.18738970756530762,0.19911952018737794,0.1890944480895996,0.18330938816070558,0.18638195991516113,0.1800960898399353,0.19179089069366456,0.18972322940826417,0.19308366775512695,0.1958134412765503,0.20387439727783202,0.1995232105255127,0.1803128242492676,0.1910325765609741,0.19345595836639404,0.1875304937362671,0.18758976459503174,0.18088366985321044,0.1871650218963623,0.18950161933898926,0.19204435348510743,0.18847464323043822,0.19427926540374757,0.18789048194885255,0.1963437795639038,0.1875964879989624,0.18665480613708496,0.18651108741760253,0.19114325046539307,0.18616489171981812,0.19111645221710205,0.18577038049697875,0.19202991724014282,0.1866440773010254,0.1901364207267761,0.1950518846511841,0.19115619659423827,0.18801815509796144,0.18722944259643554,0.18734500408172608,0.18959864377975463,0.18855700492858887,0.19423965215682984,0.1887177348136902,0.18260395526885986,0.18866510391235353,0.18652651309967042,0.18742915391921997,0.18679345846176149,0.18944361209869384,0.1887444257736206,0.1848444938659668,0.1893124461174011,0.18511464595794677,0.19540803432464598,0.18545559644699097,0.18781232833862305,0.18945270776748657,0.1852816343307495,0.19201908111572266,0.18973021507263182,0.19131991863250733,0.1848628282546997,0.19125089645385743,0.19325015544891358,0.19966373443603516,0.19127845764160156,0.18737071752548218,0.18887016773223878,0.1838758945465088,0.18879542350769044,0.19495404958724977,0.18781980276107788,0.19301095008850097,0.18942753076553345,0.19918723106384278,0.1886582612991333,0.19076281785964966,0.18877140283584595,0.19525978565216065,0.19137425422668458,0.18688660860061646,0.18927202224731446,0.18963794708251952,0.18530105352401732,0.19281163215637206,0.1851938009262085,0.1926976203918457,0.1900638222694397,0.1814033269882202,0.18945351839065552,0.1845652937889099,0.1900569200515747,0.18722832202911377,0.18402220010757447,0.19457459449768066,0.18771600723266602,0.17940540313720704,0.18304282426834106,0.18471033573150636,0.1898671269416809,0.18982585668563842,0.18655694723129274,0.19279062747955322,0.1891352891921997,0.18884103298187255,0.1892750859260559,0.18936216831207275,0.17750973701477052,0.19025237560272218,0.1954482078552246,0.18413177728652955,0.187296462059021,0.18505918979644775,0.190980863571167,0.19009249210357665,0.19043158292770385,0.1884765148162842,0.18706223964691163,0.1903097152709961,0.19307448863983154,0.18949730396270753,0.2000936508178711,0.18445560932159424,0.18745343685150145,0.1877300500869751,0.1846709132194519,0.19335532188415527,0.1871092677116394,0.19499386548995973,0.18134095668792724,0.1915389895439148,0.19238628149032594,0.19184374809265137,0.19141430854797364,0.18822427988052368,0.1909374475479126,0.18867926597595214,0.19365005493164061,0.18820345401763916,0.1916224479675293,0.18942649364471437,0.1868178129196167,0.18638508319854735,0.18274900913238526,0.17841761112213134,0.18651692867279052,0.18515830039978026,0.18666505813598633,0.18086313009262084,0.18648717403411866,0.1910146951675415,0.19327353239059447,0.19362083673477173,0.18878505229949952,0.18848068714141847,0.17812511920928956,0.19035861492156983,0.18303000926971436,0.18683854341506959,0.19005885124206542,0.19171979427337646,0.18331596851348878,0.18779449462890624,0.18838164806365967,0.1864981770515442,0.1878281831741333,0.19221887588500977,0.19275979995727538,0.18652617931365967,0.18894920349121094,0.18846629858016967,0.18432412147521973,0.18869482278823851,0.18283137083053588,0.1838098168373108,0.1863243341445923,0.18533973693847655,0.1898120641708374,0.1834975004196167,0.1861619234085083,0.1904292106628418,0.18475580215454102,0.1907030463218689,0.1806693196296692,0.18300986289978027,0.19023575782775878,0.18513755798339843,0.18929522037506102,0.1903982162475586,0.19266220331192016,0.18080902099609375,0.18984174728393555,0.18818836212158202,0.18985084295272828,0.19066572189331055,0.18324241638183594,0.18524315357208251,0.18812329769134523,0.1860938549041748,0.19180212020874024,0.19037307500839235,0.186143159866333,0.18070499897003173,0.1939840316772461,0.18557673692703247,0.18407080173492432,0.18713078498840333,0.19501131772994995,0.18837090730667114,0.18122565746307373,0.1903451442718506,0.1911332130432129,0.185987389087677,0.18248018026351928,0.1878364324569702,0.18734452724456788,0.1856498599052429,0.1851550817489624,0.17801597118377685,0.19448863267898558,0.1835625410079956,0.19189157485961914,0.17961333990097045,0.18269059658050538,0.19308264255523683,0.19386836290359497,0.18889962434768676,0.19209858179092407,0.19614617824554442,0.18634250164031982,0.1845250368118286,0.19966835975646974,0.18990569114685057,0.18776116371154786,0.18550127744674683,0.1883945345878601,0.1889894723892212,0.19250636100769042,0.18060585260391235,0.18227124214172363,0.19570474624633788,0.1854738712310791,0.1833473563194275,0.18664684295654296,0.1902423620223999,0.18611385822296142,0.1897071361541748,0.19120264053344727,0.18705826997756958,0.18942369222640992,0.19014947414398192,0.1854664206504822,0.18179848194122314,0.1884936809539795,0.18163843154907228,0.1807102680206299,0.17861415147781373,0.18701496124267578,0.18435046672821045,0.18477689027786254,0.18500020503997802,0.19109804630279542,0.1938761591911316,0.18944374322891236,0.18918998241424562,0.18744988441467286,0.18487343788146973,0.19022600650787352,0.18091503381729127,0.1978222131729126,0.1867525339126587,0.18841071128845216,0.1914914608001709,0.17888787984848023,0.18764879703521728,0.18503165245056152,0.18644070625305176,0.18564720153808595,0.17946398258209229,0.1866663932800293,0.1924762725830078,0.1939132571220398,0.1876217842102051,0.18881564140319823,0.1900844931602478,0.19261322021484376,0.186098051071167,0.18891893625259398,0.18649730682373047,0.1840706706047058,0.19239554405212403,0.1915970802307129,0.19742848873138427,0.1918813943862915,0.1842413902282715,0.1864657998085022,0.1866161823272705,0.19358296394348146,0.1807722806930542,0.18457479476928712,0.18444035053253174,0.18492892980575562,0.18493275642395018,0.19583505392074585,0.19785062074661255,0.18690149784088134,0.18498026132583617,0.18367445468902588,0.1844712257385254,0.18362953662872314,0.1893777370452881,0.18632419109344484,0.1830775260925293,0.18447867631912232,0.1923115372657776,0.1891208529472351,0.18555693626403807,0.19018998146057128,0.18332092761993407,0.1788919448852539,0.19140872955322266,0.18547534942626953,0.18386261463165282,0.18864554166793823,0.18434324264526367,0.1887431263923645,0.1920170545578003,0.19226745367050171,0.1881878614425659,0.19149123430252074,0.1851755976676941,0.1859274387359619,0.1887765645980835,0.18677020072937012,0.17996699810028077,0.18680863380432128,0.18071664571762086,0.1948598027229309,0.18214356899261475,0.19201955795288086,0.1788005828857422,0.18872580528259278,0.18486229181289673,0.18726567029953003,0.17856054306030272,0.1869734764099121,0.18473949432373046,0.18439126014709473,0.18389265537261962,0.18224146366119384,0.17973954677581788,0.18607628345489502,0.1906421422958374,0.1892866849899292,0.18438619375228882,0.19138798713684083,0.17794495820999146,0.1998492956161499,0.18943386077880858,0.18602679967880248,0.19259226322174072,0.182456374168396,0.18546065092086791,0.18894942998886108,0.18581774234771728,0.1827868938446045,0.1810703158378601,0.19326716661453247,0.19207725524902344,0.19592690467834473,0.19182250499725342,0.1902367353439331,0.19266180992126464,0.189490008354187,0.18207608461380004,0.18666311502456664,0.18599425554275512,0.17705010175704955,0.18410637378692626,0.18886384963989258,0.18674532175064087,0.19088419675827026,0.18390212059020997,0.1839396834373474,0.18817390203475953,0.18764518499374389,0.18000006675720215,0.19022774696350098,0.19092386960983276,0.1852625846862793,0.18568022251129152,0.17368857860565184,0.19319939613342285,0.1916041374206543,0.1875057816505432,0.1866351842880249,0.18409732580184937,0.18358991146087647,0.19216047525405883,0.18964438438415526,0.19281874895095824,0.19010616540908815,0.19419881105422973,0.1889105796813965,0.18916523456573486,0.18820348978042603,0.1864137291908264,0.19143712520599365,0.19179859161376953,0.1856865406036377,0.1897475838661194,0.18943082094192504,0.18985533714294434,0.1912845253944397,0.18544232845306396,0.19125003814697267,0.1893960118293762,0.19299964904785155,0.18620432615280152,0.1831951141357422,0.18380677700042725,0.18741487264633178,0.18415976762771608,0.19003347158432007,0.18945211172103882,0.17995543479919435,0.1880974531173706,0.17645589113235474,0.19433943033218384,0.18648135662078857,0.1869441270828247,0.18216896057128906,0.18765538930892944,0.18857256174087525,0.1845981001853943,0.18946945667266846,0.19029746055603028,0.19284632205963134,0.19333659410476683,0.18239924907684327,0.19821428060531615,0.18565069437026976,0.18896666765213013,0.19275572299957275,0.18691401481628417,0.1834597706794739,0.18667817115783691,0.18127208948135376,0.17781546115875244,0.1884504437446594,0.1858288288116455,0.17834677696228027,0.1863195538520813,0.18688702583312988,0.19266624450683595,0.19486006498336791,0.19090983867645264,0.1911938190460205,0.1889432907104492,0.18845392465591432,0.19199061393737793,0.18333122730255128,0.18696967363357545,0.1901167631149292,0.19162157773971558,0.18730540275573732,0.18244768381118776,0.19386211633682252,0.18738486766815185,0.1912910223007202,0.18153157234191894,0.18156497478485106,0.18746918439865112,0.19083046913146973,0.19407002925872802,0.1866316556930542,0.1872451663017273,0.17675806283950807,0.18555320501327516,0.18749537467956542,0.19137871265411377,0.18674975633621216,0.1758137345314026,0.1844213843345642,0.18432066440582276,0.18601303100585936,0.19074854850769044,0.18919918537139893,0.1871131420135498,0.18399360179901122,0.18689703941345215,0.18007025718688965,0.18929773569107056,0.18102216720581055,0.20417416095733643,0.18906599283218384,0.18797993659973145,0.18836548328399658,0.19410243034362792,0.18481917381286622,0.18455382585525512,0.1863040566444397,0.1821122646331787,0.19813408851623535,0.19110989570617676,0.18042107820510864,0.18704442977905272,0.19251936674118042,0.1900458574295044,0.18802064657211304,0.18938090801239013,0.18251781463623046,0.18697463274002074,0.18683642148971558,0.1884266972541809,0.18904387950897217,0.18905540704727172,0.1871690034866333,0.18699265718460084,0.17950575351715087,0.18961206674575806,0.18844859600067138,0.18334881067276002,0.18590588569641114,0.18800804615020753,0.18943096399307252,0.19792850017547609,0.19145753383636474,0.19133011102676392,0.1872830867767334,0.1928640604019165,0.18246865272521973,0.17940517663955688,0.19275550842285155,0.1792732834815979,0.1807868003845215,0.18884849548339844,0.18324732780456543,0.18886325359344483,0.19176068305969238,0.18158766031265258,0.18666379451751708,0.17766139507293702,0.19264228343963624,0.1871337890625,0.18499614000320436,0.18432737588882447,0.18364377021789552,0.18596410751342773,0.18188791275024413,0.1852739453315735,0.18295000791549682,0.18489267826080322,0.18811590671539308,0.18608739376068115,0.18874286413192748,0.18784879446029662,0.1913691997528076,0.18298803567886351,0.18074113130569458,0.18929402828216552,0.18833303451538086,0.1898871421813965,0.18729356527328492,0.18284229040145875,0.1857276201248169,0.18908338546752929,0.18439340591430664,0.18496861457824706,0.19416093826293945,0.18556804656982423,0.1813117504119873,0.18716598749160768,0.184336256980896,0.1809545636177063,0.19830751419067383,0.1801047682762146,0.18084607124328614,0.18208694458007812,0.18054571151733398,0.18693220615386963,0.19402973651885985,0.18382203578948975,0.17483408451080323,0.18050680160522461,0.18560447692871093,0.18940526247024536,0.1819911479949951,0.1902914524078369,0.18549028635025025,0.18116223812103271,0.18695391416549684,0.19475226402282714,0.1916893243789673,0.19128941297531127,0.18316277265548705,0.19003759622573851,0.18283058404922486,0.19028928279876708,0.18523809909820557,0.18509161472320557,0.17881170511245728,0.1863420605659485,0.18117960691452026,0.1851200580596924,0.1813579559326172,0.19548238515853883,0.18618093729019164,0.1986164093017578,0.17928872108459473,0.18966388702392578,0.18515616655349731,0.18052597045898439,0.18828643560409547,0.18781068325042724,0.19200501441955567,0.1886134386062622,0.18214993476867675,0.1917526125907898,0.18944358825683594,0.19116766452789308,0.1825456738471985,0.18697423934936525,0.18418416976928711,0.18261237144470216,0.18340582847595216,0.18452661037445067,0.18683340549468994,0.190615975856781,0.18313837051391602,0.18293344974517822,0.19122769832611083,0.1798500418663025,0.189025616645813,0.18642239570617675,0.18991401195526122,0.18880467414855956,0.18864511251449584,0.18439791202545167,0.18296236991882325,0.18377920389175414,0.18919991254806517,0.18947008848190308,0.18525843620300292,0.1850661277770996,0.18321599960327148,0.1838850498199463,0.18366739749908448,0.1905036211013794,0.18560152053833007,0.18907110691070556,0.1887068271636963,0.18677184581756592,0.19108833074569703,0.1819506525993347,0.18371350765228273,0.18833292722702027,0.19198659658432007,0.18569388389587402,0.1710299849510193,0.18666532039642333,0.1883985161781311,0.1863186240196228,0.19105679988861085,0.1811152458190918,0.1826404094696045,0.18907469511032104,0.1784372091293335,0.18271543979644775,0.1854134202003479,0.18651467561721802,0.1817234992980957,0.1905163288116455,0.1852453112602234,0.18269505500793456,0.1822510004043579,0.18822340965270995,0.1863544225692749,0.19235762357711791,0.188348650932312,0.1907106399536133,0.18673095703125,0.1818983793258667,0.17937690019607544,0.18677736520767213,0.1861558198928833,0.17291005849838256,0.18464888334274293,0.18024811744689942,0.18138972520828248,0.19469285011291504,0.18954429626464844,0.1973952054977417,0.18767002820968628,0.18677207231521606,0.1841683268547058,0.18675631284713745,0.17864770889282228,0.1829824686050415,0.18330297470092774,0.18399275541305543,0.188447904586792,0.18467105627059938,0.1831337332725525,0.18565502166748046,0.18757855892181396,0.18551646471023558,0.1827056288719177,0.18200316429138183,0.18189475536346436,0.193574059009552,0.1805393099784851,0.1841123342514038,0.18485357761383056,0.18737475872039794,0.1878100037574768,0.19111125469207763,0.1828853964805603,0.18374407291412354,0.18710800409317016,0.18491463661193847,0.1822072982788086,0.18500657081604005,0.17712656259536744,0.176790189743042,0.191773521900177,0.18461617231369018,0.19424173831939698,0.1790217638015747,0.18260520696640015,0.18285768032073973,0.18425203561782838,0.1914445161819458,0.18134087324142456,0.18725981712341308,0.18772152662277222,0.19596879482269286,0.18459864854812622,0.1914369583129883,0.1934353828430176,0.17652685642242433,0.1858777642250061,0.17504405975341797,0.1856329321861267,0.1871817708015442,0.19159258604049684,0.18544981479644776,0.19163001775741578,0.18082830905914307,0.187263822555542,0.1838848352432251,0.1787474274635315,0.18520565032958985,0.18297908306121827,0.18569521903991698,0.1794174313545227,0.18602923154830933,0.18743937015533446,0.19078032970428466,0.18507111072540283,0.18852102756500244,0.17824630737304686,0.19164352416992186,0.1857609272003174,0.18026995658874512,0.18320879936218262,0.189917254447937,0.1962412714958191,0.18284327983856202,0.18311309814453125,0.18627113103866577,0.18661465644836425,0.19086873531341553,0.1899329900741577,0.18711516857147217,0.18624579906463623,0.18566460609436036,0.1850048542022705,0.18464293479919433,0.1867103934288025,0.18927242755889892,0.1861477017402649,0.1773014783859253,0.1859882116317749,0.18447643518447876,0.18382140398025512]};

  var data = [data0];
  var layout = {"title":"loss by time"};

  Plotly.plot('plot-1041885403', data, layout);
})();
});
      
</script>
</div>

</div>

<div class="output_area">
<div class="prompt output_prompt">Out[3]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">random</span>: <span class="ansi-green-fg">Random</span> = scala.util.Random@62f3ad76
<span class="ansi-cyan-fg">lossSeq</span>: <span class="ansi-green-fg">IndexedSeq</span>[<span class="ansi-green-fg">Double</span>] = <span class="ansi-yellow-fg">Vector</span>(
  <span class="ansi-green-fg">0.229979944229126</span>,
<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">plot</span>: <span class="ansi-green-fg">Seq</span>[<span class="ansi-green-fg">Scatter</span>] = <span class="ansi-yellow-fg">List</span>(
  <span class="ansi-yellow-fg">Scatter</span>(
<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">res2_3</span>: <span class="ansi-green-fg">String</span> = <span class="ansi-green-fg">&#34;plot-1041885403&#34;</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#20934;&#22791;&#21644;&#22788;&#29702;&#27979;&#35797;&#38598;">&#20934;&#22791;&#21644;&#22788;&#29702;&#27979;&#35797;&#38598;<a class="anchor-link" href="#&#20934;&#22791;&#21644;&#22788;&#29702;&#27979;&#35797;&#38598;">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>类似<a href="https://thoughtworksinc.github.io/DeepLearning.scala/demo/SoftmaxLinearClassifier.html">前一节</a>，我们从CIFAR10 database中读取和处理测试数据的图片和对应的标签信息。但是这次我们在这里只读取测试集即可，训练集已在训练时随机读取。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">testNDArray</span> <span class="k">=</span>
   <span class="nc">ReadCIFAR10ToNDArray</span><span class="o">.</span><span class="n">readFromResource</span><span class="o">(</span><span class="s">&quot;/cifar-10-batches-bin/test_batch.bin&quot;</span><span class="o">,</span> <span class="mi">100</span><span class="o">)</span>

<span class="k">val</span> <span class="n">testData</span> <span class="k">=</span> <span class="n">testNDArray</span><span class="o">.</span><span class="n">head</span>

<span class="k">val</span> <span class="n">testExpectResult</span> <span class="k">=</span> <span class="n">testNDArray</span><span class="o">.</span><span class="n">tail</span><span class="o">.</span><span class="n">head</span>

<span class="k">val</span> <span class="n">vectorizedTestExpectResult</span> <span class="k">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">makeVectorized</span><span class="o">(</span><span class="n">testExpectResult</span><span class="o">,</span> <span class="nc">NumberOfClasses</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[4]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">testNDArray</span>: <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">INDArray</span> <span class="ansi-green-fg">::</span> <span class="ansi-green-fg">HNil</span> = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">testData</span>: <span class="ansi-green-fg">INDArray</span> = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">testExpectResult</span>: <span class="ansi-green-fg">INDArray</span> = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0<span class="ansi-yellow-fg">...</span>
<span class="ansi-cyan-fg">vectorizedTestExpectResult</span>: <span class="ansi-green-fg">INDArray</span> = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],
 [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],
<span class="ansi-yellow-fg">...</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20934;&#30830;&#29575;">&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20934;&#30830;&#29575;<a class="anchor-link" href="#&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20934;&#30830;&#29575;">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>跟上一节相同，我们使用测试数据来验证神经网络的预测结果并计算准确率。这次准确率应该会有所上升，最终结果在40%左右。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">right</span> <span class="k">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">getAccuracy</span><span class="o">(</span><span class="n">myNeuralNetwork</span><span class="o">.</span><span class="n">predict</span><span class="o">(</span><span class="n">testData</span><span class="o">),</span> <span class="n">testExpectResult</span><span class="o">)</span>
<span class="n">println</span><span class="o">(</span><span class="s">s&quot;the result is </span><span class="si">$right</span><span class="s"> %&quot;</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>the result is 40.0 %
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt output_prompt">Out[5]:</div>



<div class="output_text output_subarea output_execute_result">
<pre><span class="ansi-cyan-fg">right</span>: <span class="ansi-green-fg">Double</span> = <span class="ansi-green-fg">40.0</span></pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="&#24635;&#32467;">&#24635;&#32467;<a class="anchor-link" href="#&#24635;&#32467;">&#182;</a></h2><p>在这节中我们学到了：</p>
<ul>
<li>Mini-Batch Gradient Descent</li>
<li>epoch</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/MiniBatchGradientDescent.scala">完整代码</a></p>

</div>
</div>
</div>
 

