{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contributing to DeepLearning.scala\n",
    "\n",
    "So, you like deep learning and functional programming, and decide that DeepLearning.scala might be your thing. Great! There are a lot of ways you can help, and here is a non-comprehensive guide on how you can make DeepLearning.scala even more awesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helping fellow users\n",
    "\n",
    "DeepLearning.scala is a new framework with relatively fewer users. Besides the scaladoc documentation, our website includes several tutorials which demostrates using DeepLearning.scala for machine learning, but third-party tutorials are also welcome! As the developers of this framework, we'd love to hear from our users and get some idea on how they adapt the framework for their own specific tasks.\n",
    "\n",
    "If you've evaluated DeepLearning.scala and authored a blog post about it, let it be an introductory tutorial or a real-world deep learning challenge, feel free to send us a link! Your feedback will be of great help to our fellow users.\n",
    "\n",
    "You can also help by participating in discussions in our [Gitter chatroom](https://gitter.im/ThoughtWorksInc/DeepLearning.scala), or answering relevant questions on Stack Overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting issues\n",
    "\n",
    "If you encounter problems when using DeepLearning.scala and believe something is wrong on our end, please file a bug report on the [GitHub issue tracker](https://github.com/ThoughtWorksInc/DeepLearning.scala/issues). For us to track down the problem and deliver a solution, do include a minimum example to reproduce the bug in the description.\n",
    "\n",
    "You may also file a feature request if you're missing some functionality from other deep learning frameworks. Note that there's a high chance a feature request can be fulfilled by simply implementing a new plugin, which does not require code revision in the main repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacking DeepLearning.scala\n",
    "\n",
    "By its core, DeepLearning.scala is simply an interface which states support of automatic differentiation for any type. Most of DeepLearning.scala's functionality lies in its plugins. The project provides a hierarchy of built-in plugins, which provides support for common types like floating-point numbers and multi-dimensional arrays.\n",
    "\n",
    "Users of DeepLearning.scala are encouraged to craft new plugins for their own specific tasks. Custom plugins can extend built-in ones and provide extra functionalities like logging, custom optimizers, or even custom neural network architectures. Here we give a brief introduction on the architecture of DeepLearning.scala and demonstrate the development of a simple plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "DeepLearning.scala is based on several other Java/Scala libraries:\n",
    "\n",
    "* [nd4j](https://github.com/deeplearning4j/nd4j)/[nd4s](https://github.com/deeplearning4j/nd4s) for multi-dimensional arrays\n",
    "* [scalaz](https://github.com/scalaz/scalaz) for common abstractions of functional programming\n",
    "* [feature.scala](https://github.com/ThoughtWorksInc/feature.scala) for creating dynamic mixins\n",
    "* [RAII.scala](https://github.com/ThoughtWorksInc/raii.scala) for resource management\n",
    "* [each](https://github.com/ThoughtWorksInc/each) for syntactic sugar to write imperative code\n",
    "\n",
    "To implement a new plugin, one does not need to dive into implementation details of the above libraries, but reading their documentation will surely be useful.\n",
    "\n",
    "Knowledge on deep learning is not strictly a prerequisite. However you should know some basics about linear algebra and vector calculus, and how [Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up development environment\n",
    "\n",
    "Code of a DeepLearning.scala plugin need not be integrated into an sbt project. It can be put into an [Ammonite](https://github.com/lihaoyi/Ammonite) script. It is very handy to use [jupyter-scala](https://github.com/alexarchambault/jupyter-scala) to develop the plugin interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the core interface\n",
    "\n",
    "The core interface of DeepLearning.scala is [`DeepLearning`](https://static.javadoc.io/com.thoughtworks.deeplearning/deeplearning_2.11/2.0.0-RC4/index.html#com.thoughtworks.deeplearning.DeepLearning). This trait defines a type class for data types which support automatic differentiation. To implement an instance, one needs to inherit the instance and override the definitions of `Data`, `Delta` and `forward`. The `train` method can be invoked to perform a single round of training (which is the basis of gradient descent). The `predict` method can be invoked to perform a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the hierarchy of built-in plugins\n",
    "\n",
    "The core interface can be extended by various plugins. Plugins provide various functionality, like supporting new differentiable data types, optimizing, logging, etc. Typically, end-users do not need to extend the core interface themselves; they need only to select plugins which provides required functionality, combine them and initiate an instance like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`com.thoughtworks.deeplearning::plugins-doublelayers:2.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::plugins-indarraylayers:2.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::plugins-indarrayweights:2.0.0`\n",
    "\n",
    "import scala.concurrent.ExecutionContext.Implicits.global\n",
    "import com.thoughtworks.feature.Factory\n",
    "import com.thoughtworks.deeplearning.plugins.DoubleLayers\n",
    "import com.thoughtworks.deeplearning.plugins.INDArrayLayers\n",
    "import com.thoughtworks.deeplearning.plugins.INDArrayWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp.load(\"val hyperparameters = Factory[DoubleLayers with INDArrayLayers with INDArrayWeights].newInstance()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hyperparameters` is now in scope and provides functionality from `DoubleLayers`, `INDArrayLayers` and `INDArrayWeights`. One can use `hyperparameters.INDArrayWeight` to initiate a weight matrix, `hyperparameters.DoubleLayer`/`hyperparameters.INDArrayLayer` to initiate a hidden layer/output layer of the neural network. (Note that we're using [`Factory`](https://javadoc.io/doc/com.thoughtworks.feature/factory_2.11/2.0.1) instead of Scala's built-in anonymous trait instantiation syntax, because `Factory` provides some extra niceties)\n",
    "\n",
    "There exist a hierarchy of built-in plugins. They are roughly structured as follows:\n",
    "\n",
    "* For end users: [`Builtins`](https://static.javadoc.io/com.thoughtworks.deeplearning/deeplearning_2.11/2.0.0-RC4/index.html#com.thoughtworks.deeplearning.plugins.Builtins) is available for use. It simply combines all other traits and enables all built-in plugins.\n",
    "* The elementary plugins include [`Layers`](https://static.javadoc.io/com.thoughtworks.deeplearning/deeplearning_2.11/2.0.0-RC4/index.html#com.thoughtworks.deeplearning.plugins.Layers), [`Training`](https://static.javadoc.io/com.thoughtworks.deeplearning/deeplearning_2.11/2.0.0-RC4/index.html#com.thoughtworks.deeplearning.plugins.Training), [`Weights`](https://static.javadoc.io/com.thoughtworks.deeplearning/deeplearning_2.11/2.0.0-RC4/index.html#com.thoughtworks.deeplearning.plugins.Weights) and [`Operators`](https://static.javadoc.io/com.thoughtworks.deeplearning/deeplearning_2.11/2.0.0-RC4/index.html#com.thoughtworks.deeplearning.plugins.Operators). Each plugin is structured as a `trait` equipped with abstract types (often with constraints specifying the APIs they support) and API traits.\n",
    "* Extending the elementary plugins, we got support for `Float`/`Double`/`INDArray`.\n",
    "\n",
    "The plugin hierarchy is designed with extensibility in mind. Although complicated, end users need only to use [Factory](https://javadoc.io/doc/com.thoughtworks.feature/factory_2.11/2.0.1) to create a new instance for a plugin. The created instance often serves as the context type for the whole machine learning task, and can be used to create new differentiable variables and launch forward/backward passes.\n",
    "\n",
    "When creating a custom plugin, one typically starts by implementing a new trait which extends some built-in traits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a plugin for optimizing `INDArrayWeights`\n",
    "\n",
    "Let's create a plugin for optimizing `INDArrayWeights`. The `INDArrayWeights` trait extends `Weights`, and its inner trait `INDArrayOptimizerApi` extends the inner trait `OptimizerApi` of `Weights`. `WeightApi` declares a `delta` method, which is invoked to calculate how much is to be subtracted from the original weights after back propagation produces the current gradient. We'll implement a simple plugin which uses a fixed learning rate to calculate `delta`. The code is listed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.nd4j::nd4s:0.8.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::plugins-indarrayweights:2.0.0`\n",
    "\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4s.Implicits._\n",
    "import com.thoughtworks.deeplearning.plugins.INDArrayWeights\n",
    "\n",
    "trait INDArrayLearningRate extends INDArrayWeights {\n",
    "    val learningRate: Double\n",
    "    \n",
    "    override type INDArrayOptimizer <: INDArrayOptimizerApi with Optimizer\n",
    "    \n",
    "    trait INDArrayOptimizerApi extends super.INDArrayOptimizerApi { this: INDArrayOptimizer =>\n",
    "        override def delta: INDArray = {\n",
    "            super.delta * learningRate\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plugin can now be used to optimize an `INDArrayWeight` (don't forget to pass the `learningRate` parameter when initiating an instance). As we can see, extending a built-in plugin is not that hard:\n",
    "\n",
    "* Use a trait to extend a trait of existing plugins. This guarantees your plugin can be freely mixed and extended.\n",
    "* Add custom fields to preserve information required by this plugin.\n",
    "* Plugins typically has nested traits. To override a method of a nested trait, first extend the inner trait (you need to use \"self type\" to make it type check).\n",
    "\n",
    "Besides the example of \"learning rate\", we'll develop another plugin that provides another piece of functionality: logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a plugin for dumping `INDArrayWeights`\n",
    "\n",
    "We'll demostrate another plugin here: adding logging functionality to `INDArrayWeights`. As we all know, training a neural network is a long and arduous process, consuming many hours (even days). What if the training process is interrupted, and all intermediate result is gone? Adding a dumper which serializes `INDArrayWeights` in case of disruption sounds like a good idea.\n",
    "\n",
    "Below is a plugin which supports dumping `INDArrayWeights` each time after the weights are updated. The logic is pretty simple: in `INDArrayWeights`, the weight is updated by the `backward` method. So we need only to extend `INDArrayWeights` and override the `backward` method. We first invoke `backward` of the superclass to perform proper updating, then we use the `java.io.Serializable` interface to dump the data to a file. The user needs only to supply a `dumpingPathPrefix` when initiating an instance, and it keeps track of how many times the weight matrix has been serialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.nd4j::nd4s:0.8.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::plugins-indarrayweights:2.0.0`\n",
    "\n",
    "import java.io.{FileOutputStream, ObjectOutputStream}\n",
    "import scalaz.syntax.all._\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.feature.{Factory, ImplicitApply, PartialApply}\n",
    "import com.thoughtworks.raii.asynchronous._\n",
    "import com.thoughtworks.deeplearning.plugins.INDArrayWeights\n",
    "\n",
    "trait INDArrayDumping extends INDArrayWeights {\n",
    "    \n",
    "    val dumpingPathPrefix: String\n",
    "    \n",
    "    private var currentDumped: Int = 0\n",
    "    \n",
    "    override type INDArrayWeight <: INDArrayWeightApi with Weight\n",
    "    \n",
    "    trait INDArrayWeightApi extends super.INDArrayWeightApi { this: INDArrayWeight =>\n",
    "        override protected def backward[SubtypeOfOptimizer](\n",
    "            originalDelta: INDArray)(\n",
    "            implicit implicitApplyRest: ImplicitApply.Aux[PartiallyAppliedOptimizer, SubtypeOfOptimizer],\n",
    "            asOptimizer: SubtypeOfOptimizer <:< OptimizerApi { type Delta <: INDArray }\n",
    "        ): Do[Unit] = {\n",
    "           super.backward(originalDelta).map { _ =>\n",
    "               val os = new ObjectOutputStream(new FileOutputStream(dumpingPathPrefix + \"/\" + currentDumped.toString))\n",
    "               try {    \n",
    "                   os.writeObject(data)\n",
    "               } finally {\n",
    "                   os.close()\n",
    "               }\n",
    "               currentDumped += 1\n",
    "           }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove that our custom `INDArrayDumping` can indeed be used to initiate an `INDArrayWeight` which automatically does the serialization while preserving the original `backward` behavior, we'll test a trivial example below. We combine a random `INDArrayWeight` to form a `DoubleLayer` and invoke its `train` multiple times. We can observe the resulting value steadily decreases, and the intermediate weight matrices are indeed serialized to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.nd4j:nd4j-native-platform:0.8.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::plugins-builtins:2.0.0`\n",
    "\n",
    "import scala.concurrent.Await\n",
    "import scala.concurrent.duration.Duration\n",
    "import scala.concurrent.ExecutionContext.Implicits.global\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import com.thoughtworks.future._\n",
    "import com.thoughtworks.deeplearning.plugins.Builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interp.load(\"val hyperparameters = Factory[INDArrayDumping with Builtins].newInstance(dumpingPathPrefix=\\\"/Users/cshao/example\\\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperparameters.INDArrayWeight\n",
    "import hyperparameters.DoubleLayer\n",
    "import hyperparameters.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val w = INDArrayWeight(Nd4j.randn(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def o: DoubleLayer = w.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Await.result(o.train.toScalaFuture, Duration.Inf) // strike enter for n times, and watch the magic happen.\n",
    "// Make sure dumpingPathPrefix exists."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
